[
  {
    "objectID": "09-spatial_lag_error_regressions.html",
    "href": "09-spatial_lag_error_regressions.html",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "This session builds on the techniques taught in week 3’s tutorials focused on spatial dependence & spatial autocorrelation. Today, you will learn how to build spatially-based regression models for dealing with spatial autocorrelation whenever you want to determine relationship between an outcome and other independent variables. These are known as Spatial Lag or Spatial Error regression models.\n\n\nIn this exercise, we will be using London’s Lower Super Output Area (LSOA) data from 2015 pertained to house prices (as a dependent variable), and assessing it’s relationship with public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) as independent variables while accounting for spatial autocorrelation.\nWe will implement three models:\n\nMultivariable linear regression\nSpatial lag regression\nSpatial error regression\n\n\n\n\nBefore you begin do make sure to download all data by clicking here. Create a folder on called “Week 7” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 7” folder. Open a new R script and set the work directory to Week 7’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 7\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 7\")\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nspdep: Spatial Dependence (Weighting schemes & Spatial Statistics)\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\n\n# Load packages using library() function\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"spdep\")\nlibrary(\"sp\")\n\nThe above packages sf, tmap, spdep & sp should have been installed in the previous session(s).\nWe will need to install the following packages:\n\nspatialreg: provides functions for spatial regression modelling.\n\n\n# Install the packages: spatialreg using the install.package()\ninstall.packages(\"spatialreg\")\n\n# Load the packages with library()\nlibrary(\"spatialreg\")\n\n\n\n\nLet us first import the quantitative data i.e., London LSOA 2015 data.csv into R/RStudio.\n\n# Use read.csv() to import \ndatafile &lt;- read.csv(file = \"London LSOA 2015 data.csv\", header = TRUE, sep = \",\")\n\nNOTE: The description of the column names are as follows:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nLSOACODE\nUnique identification code for the geographic area\n\n\nAVEPRICE (Dependent variable)\nAverage house price estimated for the LSOA in 2015\n\n\nAVEINCOME\nEstimated average annual income for households within an LSOA in 2015\n\n\nIMDSCORE\nDeprivation score for an LSOA in 2015\n\n\nPTAINDEX\nMeasures levels of access/connectivity to public transport\n\n\nPTACAT\nPTAINDEX rendered into a categorical variable\n\n\n\n\n\n\n\nThe above data were sourced from the London DATASTORE. Next, we import the shape files for London (i.e., LSOA- and Borough-level):\n\nLondon LSOA shape file: London LSOA Areas.shp\nLondon Borough shape file: London Borough Areas.shp\n\n\n# Use read_sf() function to load shape file \nLSOAshp &lt;- read_sf(\"London LSOA Areas.shp\")\nBOROUGHshp &lt;- read_sf(\"London Borough Areas.shp\")\n\nThe code chunk below generates an empty map with the tmap functions. It inspects the spatial configuration of London’s LSOA with the Boroughs superimposed.\n\n# Generate an empty map to visualise the spatial configuration and hierarchy of LSOA and Boroughs\ntm_shape(LSOAshp) + \n    tm_polygons() +\ntm_shape(BOROUGHshp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\ntm_compass(position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor any statistical & spatial analysis its always useful to conduct some descriptive analysis of your dataset. You can obtain basic summary measures using the summary() function which reports the overall result i.e., minimum, maximum, median, mean, 25th and 75th percentile values. For instance, we can report these estimates for the house price variable:\n\nsummary(datafile$AVEPRICE)\n\nWe can see that the mean price across the LSOAs is £528,509 with ranges from £133,997 to £5,751,342. You can also report the standard deviation across all LSOAs using the sd() function. As you can see the result is large and estimated as ±£418,842.5.\n\nsd(datafile$AVEPRICE)\n\n\n\n\n\n\n\nNote\n\n\n\nWe have compiled some useful functions for reporting basic measures for descriptive analysis of the fly:\n\nmin(): minimum\nmax(): maximum\nmean(): mean\nmedian(): median\nsd(): standard deviation\n\n\n\n\n\n\nNow lets examine the spatial distribution for the house price. First, the LSOA data is stored in datafile which is just a data frame object. We need to merge this to the spatial object LSOAshp before using the tmap functions. Lets create a new spatial data frame and name it spatialdatafile\n\n# Merge datafile to LSOAshp uniquely by using \"LSOACODE column\nspatialdatafile &lt;- merge(LSOAshp, datafile, by.x = \"LSOACODE\", by.y = \"LSOACODE\")\n\nLets now generate our first map to inspect the distribution of house prices. We can actually store a picture as an image object called plot1\n\nplot1 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"AVEPRICE\", \n        fill.scale = tm_scale_intervals(n=7, style = \"quantile\", values = \"brewer.greens\"),\n        fill.legend = tm_legend(frame = FALSE)) +\ntm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\ntm_compass(position = c(\"right\", \"top\")) +\ntm_scalebar(position = c(\"left\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n# plot the image object\nplot1\n\n\n\n\n\n\n\n\n\n\nDescriptively, we can observe already an interesting pattern. LSOAs in parts of North, Central and the central West of Inner London tend to have high-priced properties on average exceeding £758,000.00, whereas in parts of Hillingdon, Enfield, Croydon, Bexley, Barking & Dagenham have on average cheaper properties below £273,000.00. We need to take note that the patterns look clustered; however, this descriptive reporting is by no means an evidence-based analysis for assessing clustering or dispersion (i.e., spatial autocorrelation) - a Moran’s I test will be able to diagnosis this problem for house price but when other factors are involved we will need to use the residuals in the Moran’s I test.\nLet us visualise three other variables which will subsequently be treated as independent variables in a regression to eyeball whether they are correlated with the house prices. Let us plot maps for income, deprivation and PAT (categories); and then stitch them together by invoking the tmap_arrange() function.\n\n# create 3 separate maps and store them in plot2, plot3 & plot4 objects\n\n# map for income\nplot2 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"AVEINCOME\", \n        fill.scale = tm_scale_intervals(n=7, style = \"quantile\", values = \"brewer.oranges\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nplot3 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"IMDSCORE\", \n        fill.scale = tm_scale_intervals(n=7, style = \"quantile\", values = \"brewer.reds\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nplot4 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"PTACAT\", \n        fill.scale = tm_scale_categorical(values = \"brewer.blues\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\ntmap_arrange(plot1, plot2, plot3, plot4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nVisually, the income appears to be strongly correlated with house price. The patterns for socioeconomic deprivation on the other hand appears to have a negative correlation with house price. PTAs relationship is unclear.\n\n\n\nNext, we are going to fit a linear regression where the response variable is AVEPRICE and the predictors are AVEINCOME, IMDSCORE and PTACAT, and then extract the residuals from the model in order to test for spatial autocorrelation. When fitting a linear regression - we must test that the residuals have the following properties:\n\nThe residuals should exhibit a normal distribution.\nResiduals must be homoscedasticity and exhibit a constant random when plotted against the fitted prediction\nThe residual should not deviate too much away from the index line in the QQ-plot.\n\n\n\n\n\n\n\nNote\n\n\n\nThings that can violate the above requirements are:\n\nA skewed variable. It is best to transform using the log10() function, or in any scale that’s justifiable to normalise it. if your data is skewed then do a transformation in the models since only care about the residuals. Note that there is no right or wrong when it comes to implementing the transformation, as long as the assumption for the residuals from a linear regression are not violated then all is fine.\nIf one of these characteristics are violated then its an indication that the data are not independent. This could possibly be due to some data artefact (i.e., a critical error in the data itself), or the residuals being correlated with each other.\nPresence of collinearity between residual points. Here, we should check first by mapping the residuals on a map to examine it’s spatial patterns for clustering and testing it with the Moran’s I test.\n\n\n\nLet’s demonstrate by using the built-in function lm() to create a simple linear or multivariable linear regression.\n\n# lm() function builds a regression model and stores model output into the object 'modelMLR'\nmodelMLR &lt;- lm(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile)\n# Include the 'scipen=7' argument in the summary() function remove those annoying scientific notation!\noptions(scipen = 7)\n# summary() calls report the output stored in object 'modelMLR'\nsummary(modelMLR)\n\n\nCall:\nlm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + \n    log10(PTAINDEX), data = spatialdatafile)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39249 -0.06489 -0.00572  0.06046  0.62993 \n\nCoefficients:\n                  Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)      -4.100992   0.095592 -42.901        &lt; 2e-16 ***\nlog10(IMDSCORE)   0.136713   0.007681  17.798        &lt; 2e-16 ***\nlog10(AVEINCOME)  2.036354   0.019340 105.292        &lt; 2e-16 ***\nlog10(PTAINDEX)   0.030055   0.004816   6.241 0.000000000471 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1027 on 4964 degrees of freedom\nMultiple R-squared:  0.789, Adjusted R-squared:  0.7889 \nF-statistic:  6189 on 3 and 4964 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nImportant\n\n\n\nThe above results presented in the above table indicates the relationship between the dependent and independent variable. Under the column for Estimate reports the overall intercept and coefficients for each independent variable. The p-values for the estimates are reported under the column Pr(&gt;|t|) which determines whether the relationship between the dependent and independent variables are statistically significant or not.\n\n\nLastly, the other two important pieces of information is the Adjusted R-Squared value and the model’s p-value at the bottom. These tells us the performance of the model in general. The former tells you the percentage of variation explained in the house price when including AVEINCOME, IMDSCORE, PTAINDEX in the regression, while the latter informs us whether if this is significant or not.\nHOW TO INTERPRET RESULTS: This is how we fully interpret the coefficients and model performance from the above table:\n\nOn a log transformed scale, if the AVEINCOME was to increase by 1.0%, we expect the house prices to increase by 2.04% and this average increase is statistically significant since the p-value = 0.00000000002 &lt;0.05.\nOn a log transformed scale, if the PTAINDEX was to increase by 1.0%, we expect the house prices to increase marginally by 0.03%. The marginally increase is statistically significant since the p-value = 0.000000000471 &lt; 0.05.\nOn a log transformed scale, if the IMDSCORE was to increase by 1.0%, we expect the house prices to increase marginally by 0.13% and this average increase is statistically significant since the p-value = 0.00000000002 &lt;0.05.\nIn terms of model performance: according to the Adjusted R-Squared value 0.7889 (78.89%) of the variation in the house prices across LSOAs were explained by the model after accounting for AVEINCOME, IMDSCORE, PTAINDEX. Since the Adjusted R-Squared more than 50.0% it is hence a very good model and significant (i.e., p-value = 0.0000000002 &lt; 0.05).\n\nNow that we have fitted the model, we can extract the residuals and insert them as a new column into the spatialdatafile. To perform this action, use the modelMLR object and extract the residuals output from it.\n\n# Extract residuals from \"modelLMR\" object and dump into \"spatialdatafile\" and call the column \"RESIDUALS\"\nspatialdatafile$RESIDUALS &lt;- modelMLR$residuals\n\n# Reporting basic summary measures to have an idea of its distribution before plotting them on map\nsummary(spatialdatafile$RESIDUALS)\n\nLet us generate a map to examine if these residuals show patterns of spatial autocorrelation. We have divergent values for the legends (i.e., negative and positive value) therefore it best to specify in the tm_fill() function that style = \"cont\" and the midpoint = 0, and a divergent colour scheme e.g. Reds to Blue (-RdBu).\n\n# plot the residuals\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESIDUALS\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\ntm_shape(BOROUGHshp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nNotice the spatial patterning and clusters of the LSOAs and the over-prediction (i.e., areas that have negative residuals, or blue tones) and under-prediction (i.e., areas that positive residuals, or red tones). This visual inspection of the residuals is telling you that spatial autocorrelation may be present here. This, however, would require a more formal test.\nNow, let’s use the Moran’s I test to confirm the presence of spatial autocorrelation. Recall week 3’s lectures and computer labs session on spatial autocorrleation? Here, we create the spatial adjacency matrix and apply the Moran’s I test on the modelMLR object using the lm.morantest() function.\n\n#generate unique number for each row\nspatialdatafile$ROWNUM &lt;- 1:nrow(spatialdatafile)\n# We need to coerce the sf spatialdatafile object into a new sp object\nspatialdatafile_2.0 &lt;- as(spatialdatafile, \"Spatial\")\n# Create spatial weights matrix for areas\nWeights &lt;- poly2nb(spatialdatafile_2.0, row.names = spatialdatafile_2.0$ROWNUM)\nWeightsMatrix &lt;- nb2mat(Weights, style='B')\nResidual_WeightMatrix &lt;- mat2listw(WeightsMatrix , style='W')\n# Run the test on the regression model output object \"modelMLR\" using lm.morantest()\nlm.morantest(modelMLR, Residual_WeightMatrix, alternative=\"two.sided\")\n\n\nGlobal Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile)\nweights: Residual_WeightMatrix\n\nMoran I statistic standard deviate = 56.28, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n   0.47489527088   -0.00060260241    0.00007138138 \n\nYou will notice we obtained a statistically significant value (i.e., p-value &lt;0.001) for Moran’s I of value = 0.475. The value of the Moran’s I test somewhat high. This is an indication that the errors (the residuals) are somewhat related to each other and thus not independent. A spatial regression would be much appropriate for modelling this type of data since there’s evidence of spatial autocorrelation.\n\n\n\n\n\n\nAt this point, we will introduce two kinds of spatial regressions models that can address the issues of spatial autocorrelation. These models are similar in a sense that they all require the inclusion of a spatial weight matrix to account for the spatial configuration of the areas under investigation.\n\nSpatial Lag Model lagged on the dependent variable\nSpatial Error Model\n\nPreviously, we fitted a linear regression model that’s non-spatial, which takes the mathematical formula as follows:\n\n\\[y = \\beta_{0} + x_{1}\\beta_{1} + x_{2}\\beta_{2} + ... +\\epsilon\\]\n\nHere, \\(y\\) is the response variable (i.e., AVEPRICE). The \\(x_{1}\\), \\(x_{2}\\)., etc., are the independent variables (i.e, \\(x_{1}\\), \\(x_{2}\\) & \\(x_{3}\\) are AVEINCOME, IMDSCORE and PTAINDEX respectively). \\(\\beta_{1}\\), \\(\\beta_{2}\\)., etc., are the estimated coefficients for the independent variables. The \\(\\epsilon\\) represents the uncorrelated error term.\nTo make this a Spatial Lag Model lagged on the dependent variable, we tweak the above equation by including the spatial weight matrix \\(W\\) which is multiplied by the dependent variable \\(y\\). This product will have an estimated coefficient termed \\(\\rho\\). The \\(\\rho\\) parameter tells us the degree at which our observed outcome inside the study area of interest is influenced by outcomes measured from its neighbours. It’s model formulation is below:\n\n\\[y = \\rho Wy + \\beta_{0} + x_{1}\\beta_{1} + x_{2}\\beta_{2} + ... +\\epsilon\\]\n\nThe Spatial Error Model, on the other hand, incorporates spatial weight matrix \\(W\\) into the error term, where \\(\\lambda\\) is an estimated coefficient for the product between \\(W\\) and \\(u\\), which \\(u\\) is a correlated spatial error term. It’s model formulation is below:\n\n\\[y =  + \\beta_{0} + x_{1}\\beta_{1} + x_{2}\\beta_{2} + ...+ \\lambda Wu +\\epsilon\\]\n\nHere, we show you how to implement these two commonly used models to see which one better address the issue of spatial autocorrelation. But there are some important points:\n\nWhen implementing these models, we want to ensure that the spatial autocorrelation are accounted for, and therefore, we have to perform the Moran’s I test again. Here, we want to make sure that it is lower than what we observed for the linear regression model.\nWe want to also compare the spatial model against the non-spatial model by checking the AIC values, the one with lowest is the better model.\nPreviously, we were able to interpret the coefficients for the linear regression model. For the spatial models however, the coefficients derived from the model lagged on the dependent variable are difficult to interpret. Therefore, we have to estimate a quantity called impacts which we will use in the interpretation. This step is the most crucial part which must be done\n\n\n\n\nA Spatial Lag Model on the dependent variable assumes that dependencies exist directly among the levels of the dependent variable. That is, the observed outcome in one primary location is affected by other outcomes in nearby or neighbouring locations. For instance, if we implement this model on the LSOA house price, we are assuming that the property prices in one LSOA is impacted by the property prices from nearby neighbouring LSOAs.\nWe can perform this analysis in four steps:\n\nStep 1: Fit the Spatial Lag Model with the dependent variable lagged using the lagsarlm() function.\nStep 2: Use the summary() function to report the results. Here, we are interested in the \\(\\rho\\) parameter & its p-value. Here, we also want to check if the model is appropriate than a non-spatial model - examine the AIC (for lag versus LM) and the one with the lowest AIC is the better model.\nStep 3: Extract the residual lags for the model object and carry out a Moran’s I test using moran.mc() to ensure that the statistic is less than what was obtained for the Moran’s I test for the linear model. Use the tmap to examine its spatial patterning.\nStep 4: Interpretation of the parameters using the impact() function\n\nSTEP ONE\n\n# Fit model using lagsarlm()\n# reuse spatial weight matrix created earlier as an object called \"Residual_WeighMatrix\" \nmodelSLY &lt;- lagsarlm(log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile_2.0, Residual_WeightMatrix)\n\nSTEP TWO\n\n# Report results with summary()\n# We are interested in the rho-coefficient, log-likelihood ratio test's p-value and the AIC\nsummary(modelSLY)\n\n\nCall:lagsarlm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + \n    log10(PTAINDEX), data = spatialdatafile, listw = Residual_WeightMatrix)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.3608275 -0.0540603 -0.0039772  0.0518209  0.6492007 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n                   Estimate Std. Error  z value        Pr(&gt;|z|)\n(Intercept)      -2.6649683  0.0924917 -28.8130       &lt; 2.2e-16\nlog10(IMDSCORE)   0.0435882  0.0068588   6.3551 0.0000000002083\nlog10(AVEINCOME)  1.2144821  0.0286833  42.3412       &lt; 2.2e-16\nlog10(PTAINDEX)   0.0106795  0.0041275   2.5874        0.009671\n\nRho: 0.4522, LR test value: 1354.5, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.012282\n    z-value: 36.819, p-value: &lt; 2.22e-16\nWald statistic: 1355.6, p-value: &lt; 2.22e-16\n\nLog likelihood: 4937.637 for lag model\nML residual variance (sigma squared): 0.0077091, (sigma: 0.087801)\nNumber of observations: 4968 \nNumber of parameters estimated: 6 \nAIC: -9863.3, (AIC for lm: -8510.8)\nLM test for residual autocorrelation\ntest value: 443.54, p-value: &lt; 2.22e-16\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The \\(\\rho\\) statistic informs us of how the neighbouring LSOA house price values affect the house price at \\(y\\). The \\(\\rho\\) value is a positive value of 0.4522 which means the neighbouring LSOAs affect is a positive manner, and it is statistically significant (i.e., p-value &lt; 0.05). We can see the AIC for the lag model is lower than the original linear regression model (i.e., Lag: -9863.3 vs LM: -8510.8) therefore the lag model is okay.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIMPORTANT NOTE: In a lag model, do not even try to interpret the coefficients for the independent variables - ignore them and their p-values… they are nonsense! Why? This is because there is a global feedback effect happening here - i.e., whenever we change something in our own region (i.e., LSOA) for instance, like the AVEINCOME in a LSOA, that will not only affect our own house price, but when it causes the house price to go up in its own area, this in turn will cause the house prices to increase in its neighbour’s area; and when the neighbour’s house price go up - it is again going to affect our house price… so its an infinite loop. Instead, we interpret the result churn out from the impact function which reports their direct and indirect effects.\n\n\nSTEP THREE\n\n# extract the residuals for modelSLY object and dump back to original sf spatialdatafile object\nspatialdatafile$RESID_SLY &lt;- modelSLY$residuals\n# use Moran's I test using moran.mc() function\nmoran.mc(spatialdatafile$RESID_SLY, Residual_WeightMatrix, 1000, zero.policy = T)\n\n\nMonte-Carlo simulation of Moran I\n\ndata:  spatialdatafile$RESID_SLY \nweights: Residual_WeightMatrix  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.13417, observed rank = 1001, p-value = 0.000999\nalternative hypothesis: greater\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The Moran’s I from the original model was 0.4748. Here, it is 0.1341 which is much lower thus the lag model has accounted for a lot of spatial autocorrelation although it still significantly remains. We can conclude that spatial lag model does address some of the issues of spatial autocorrelation in the model’s residuals but not all since it significantly positive. This is evidenced in the map output of the residual lags.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have to make a mental note of this and compare it with the performance of the Spatial Error Model.\n\n\n\n# generate the map\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESID_SLY\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nSTEP FOUR\n\n# Interpretation of results using impacts\n# impacts\nWeights_2.0 &lt;- as(Residual_WeightMatrix, \"CsparseMatrix\")\ntrMC &lt;- trW(Weights_2.0, type=\"MC\")\nsummary(impacts(modelSLY, tr = trMC, R=100), zstats=TRUE)\n\n\nImpact measures (lag, trace):\n                     Direct    Indirect      Total\nlog10(IMDSCORE)  0.04550352 0.034066675 0.07957020\nlog10(AVEINCOME) 1.26784902 0.949188065 2.21703709\nlog10(PTAINDEX)  0.01114883 0.008346682 0.01949551\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: Here is where we derive meaningful interpretation of the coefficients. A big table is churned out - all we care about is the first table titled: Impact Measures (lag, trace): and the last table titled: Simulated p-values. For instance, let’s interpret the log10(AVEINCOME)(on the log-scale), for the direct effects in its own LSOA, if the levels of income were to increase by 1%, this will cause an increase in the property prices by 1.267% (p &lt; 0.05) in its own LSOA. But for the indirect affects, if the log10(AVEINCOME) were to change across neighbouring LSOAs, this will affect the value of our house prices by 0.949% (p &lt; 0.05). The total column is the combined effect.\n\n\n\n\n\nIn a Spatial Error Model, we assume that the error terms are correlated across observations (i.e., the error of an observed value affects the errors of its neighbors).\nWe essentially repeat the first 3 steps highlighted in section 3.2. for this analysis:\n\nStep 1: Fit the Spatial Error Model using the errorsarlm() function.\nStep 2: Use the summary() function to report the results for the \\(\\lambda\\) parameter and it’s p-value to check if the model is appropriate than a non-spatial one. You can also check with the AIC.\nStep 3: Extract the residuals errors and carry out a Moran’s I test using moran.mc() to ensure that the statistic is less than what was obtained for the Moran’s I test for the linear model. Use the tmap to examine its spatial patterning.\n\nSTEP ONE\n\nmodelSER &lt;- errorsarlm(log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile_2.0, Residual_WeightMatrix)\n\nSTEP TWO\n\n# Report results with summary()\n# We are interested in the rho-coefficient, log-likelihood ratio test's p-value and the AIC\nsummary(modelSER)\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The \\(\\lambda\\) statistic informs us that if there’s a sudden change in the error term for house prices in neighbouring LSOAs how did it impact the error term for the house price in our LSOA at \\(y\\). The \\(\\lambda\\) value is a positive value of 0.7221 which means the affect of neighbouring LSOAs are positive and the impact is statistically significant (i.e., p-value &lt; 0.05). We can see the AIC for the error model is lower than both the original linear regression & lag model (i.e., Error: -10705.3 vs LM: -8510.8 & Lag:-9863.3) therefore the error model better than the two.\n\n\nIMPORTANT NOTE: Unlike the lag model, we can interpret the coefficients from the error model for the independent variables! We can interpret them the same way we did for the linear regression model (see section 7.3.3.).\nSTEP THREE\n\n# extract the residuals for modelSLY object and dump back to original sf spatialdatafile object\nspatialdatafile$RESID_SER &lt;- modelSER$residuals\n# use Moran's I test using moran.mc() function\nmoran.mc(spatialdatafile$RESID_SER, Residual_WeightMatrix, 1000, zero.policy = T)\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The Moran’s I from the original model was 0.4748. Here, it is -0.0579 which is negative and the lowest for the error model. On top of that there is no evidence of spatial autocorrelation since its p-value is not significant. Therefore, we can conclude that the spatial error model does address the issue of spatial autocorrelation in the residuals. Its a better model than the Linear regression and Lag model for exploring the relationship with those independent variables and accounting for spatial autocorrelation. This is also evidenced in the map output of the residual errors.\n\n\n\n# generate the map\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESID_SER\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code below computes the spatially lagged model based on the independent variables. Briefly, the results are a quite tedious to interpret directly from the regression model. Hence, you will want to use the post estimation impacts() to get the direct, indirect and total coefficients from model:\n\n\nClick here\n\nSolutions\n\nmodelSLX &lt;- lmSLX(log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile_2.0, Residual_WeightMatrix)\nsummary(modelSLX)\n\nIMPACTS_SLX &lt;- impacts(modelSLX, tr = trMC, R=100)\nIMPACTS_SLX\n\n\n\n\n\n\n\nPaper: Guangqing Chi and Jun Zhu (2008) Spatial Regression Models for Demographic Analysis, Popul Res Policy Rev, 27:17-42. DOI 10.1007/s11113-007-9051-8\nBook Chapter: David Darmofal, (2015), Chapter Six: Spatial Lag and Spatial Error Models (2015). Social Analysis for the Social Sciences. Cambridge Press Gain Access to PDF via Institutional login\nPaper: Wen-Ching Wang, Yu-Ju Chang & Hsueh-Ching Wang, (2019), An Application of the Spatial Autocorrelation Method on the Change of Real Estate Prices in Taitung City. Int Jour Geo-Information, (8)249, doi:10.3390/ijgi8060249\nOnline Tutorial: [R Programming] Reka Solymosi and Juanjo Medina (2022) Crime Mapping in R, Chapter 9: Spatial Regression Models Click link\n\n\n\n\n\nEnglish indices for Multiple Deprivation (IMD) for 2019 [Source: UK Gov] Click Here\nUK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here\nThe following indicators for averaged house prices, income and PTAL estimates were obtained from London DATASTORE\n\n\n\n\n\nSpatial Regression in R (Part One): The Four Simplest Models (Length: 00:40:36) [Watch on YouTube]",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#learning-objective",
    "href": "09-spatial_lag_error_regressions.html#learning-objective",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "In this exercise, we will be using London’s Lower Super Output Area (LSOA) data from 2015 pertained to house prices (as a dependent variable), and assessing it’s relationship with public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) as independent variables while accounting for spatial autocorrelation.\nWe will implement three models:\n\nMultivariable linear regression\nSpatial lag regression\nSpatial error regression",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#datasets-setting-up-the-work-directory",
    "href": "09-spatial_lag_error_regressions.html#datasets-setting-up-the-work-directory",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "Before you begin do make sure to download all data by clicking here. Create a folder on called “Week 7” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 7” folder. Open a new R script and set the work directory to Week 7’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 7\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 7\")\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nspdep: Spatial Dependence (Weighting schemes & Spatial Statistics)\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\n\n# Load packages using library() function\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"spdep\")\nlibrary(\"sp\")\n\nThe above packages sf, tmap, spdep & sp should have been installed in the previous session(s).\nWe will need to install the following packages:\n\nspatialreg: provides functions for spatial regression modelling.\n\n\n# Install the packages: spatialreg using the install.package()\ninstall.packages(\"spatialreg\")\n\n# Load the packages with library()\nlibrary(\"spatialreg\")\n\n\n\n\nLet us first import the quantitative data i.e., London LSOA 2015 data.csv into R/RStudio.\n\n# Use read.csv() to import \ndatafile &lt;- read.csv(file = \"London LSOA 2015 data.csv\", header = TRUE, sep = \",\")\n\nNOTE: The description of the column names are as follows:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nLSOACODE\nUnique identification code for the geographic area\n\n\nAVEPRICE (Dependent variable)\nAverage house price estimated for the LSOA in 2015\n\n\nAVEINCOME\nEstimated average annual income for households within an LSOA in 2015\n\n\nIMDSCORE\nDeprivation score for an LSOA in 2015\n\n\nPTAINDEX\nMeasures levels of access/connectivity to public transport\n\n\nPTACAT\nPTAINDEX rendered into a categorical variable\n\n\n\n\n\n\n\nThe above data were sourced from the London DATASTORE. Next, we import the shape files for London (i.e., LSOA- and Borough-level):\n\nLondon LSOA shape file: London LSOA Areas.shp\nLondon Borough shape file: London Borough Areas.shp\n\n\n# Use read_sf() function to load shape file \nLSOAshp &lt;- read_sf(\"London LSOA Areas.shp\")\nBOROUGHshp &lt;- read_sf(\"London Borough Areas.shp\")\n\nThe code chunk below generates an empty map with the tmap functions. It inspects the spatial configuration of London’s LSOA with the Boroughs superimposed.\n\n# Generate an empty map to visualise the spatial configuration and hierarchy of LSOA and Boroughs\ntm_shape(LSOAshp) + \n    tm_polygons() +\ntm_shape(BOROUGHshp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\ntm_compass(position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"left\", \"bottom\"))",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#diagnostics-of-residuals",
    "href": "09-spatial_lag_error_regressions.html#diagnostics-of-residuals",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "For any statistical & spatial analysis its always useful to conduct some descriptive analysis of your dataset. You can obtain basic summary measures using the summary() function which reports the overall result i.e., minimum, maximum, median, mean, 25th and 75th percentile values. For instance, we can report these estimates for the house price variable:\n\nsummary(datafile$AVEPRICE)\n\nWe can see that the mean price across the LSOAs is £528,509 with ranges from £133,997 to £5,751,342. You can also report the standard deviation across all LSOAs using the sd() function. As you can see the result is large and estimated as ±£418,842.5.\n\nsd(datafile$AVEPRICE)\n\n\n\n\n\n\n\nNote\n\n\n\nWe have compiled some useful functions for reporting basic measures for descriptive analysis of the fly:\n\nmin(): minimum\nmax(): maximum\nmean(): mean\nmedian(): median\nsd(): standard deviation\n\n\n\n\n\n\nNow lets examine the spatial distribution for the house price. First, the LSOA data is stored in datafile which is just a data frame object. We need to merge this to the spatial object LSOAshp before using the tmap functions. Lets create a new spatial data frame and name it spatialdatafile\n\n# Merge datafile to LSOAshp uniquely by using \"LSOACODE column\nspatialdatafile &lt;- merge(LSOAshp, datafile, by.x = \"LSOACODE\", by.y = \"LSOACODE\")\n\nLets now generate our first map to inspect the distribution of house prices. We can actually store a picture as an image object called plot1\n\nplot1 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"AVEPRICE\", \n        fill.scale = tm_scale_intervals(n=7, style = \"quantile\", values = \"brewer.greens\"),\n        fill.legend = tm_legend(frame = FALSE)) +\ntm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\ntm_compass(position = c(\"right\", \"top\")) +\ntm_scalebar(position = c(\"left\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n# plot the image object\nplot1\n\n\n\n\n\n\n\n\n\n\nDescriptively, we can observe already an interesting pattern. LSOAs in parts of North, Central and the central West of Inner London tend to have high-priced properties on average exceeding £758,000.00, whereas in parts of Hillingdon, Enfield, Croydon, Bexley, Barking & Dagenham have on average cheaper properties below £273,000.00. We need to take note that the patterns look clustered; however, this descriptive reporting is by no means an evidence-based analysis for assessing clustering or dispersion (i.e., spatial autocorrelation) - a Moran’s I test will be able to diagnosis this problem for house price but when other factors are involved we will need to use the residuals in the Moran’s I test.\nLet us visualise three other variables which will subsequently be treated as independent variables in a regression to eyeball whether they are correlated with the house prices. Let us plot maps for income, deprivation and PAT (categories); and then stitch them together by invoking the tmap_arrange() function.\n\n# create 3 separate maps and store them in plot2, plot3 & plot4 objects\n\n# map for income\nplot2 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"AVEINCOME\", \n        fill.scale = tm_scale_intervals(n=7, style = \"quantile\", values = \"brewer.oranges\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nplot3 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"IMDSCORE\", \n        fill.scale = tm_scale_intervals(n=7, style = \"quantile\", values = \"brewer.reds\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nplot4 &lt;- tm_shape(spatialdatafile) +\n    tm_polygons(fill = \"PTACAT\", \n        fill.scale = tm_scale_categorical(values = \"brewer.blues\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) +\n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\ntmap_arrange(plot1, plot2, plot3, plot4, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nVisually, the income appears to be strongly correlated with house price. The patterns for socioeconomic deprivation on the other hand appears to have a negative correlation with house price. PTAs relationship is unclear.\n\n\n\nNext, we are going to fit a linear regression where the response variable is AVEPRICE and the predictors are AVEINCOME, IMDSCORE and PTACAT, and then extract the residuals from the model in order to test for spatial autocorrelation. When fitting a linear regression - we must test that the residuals have the following properties:\n\nThe residuals should exhibit a normal distribution.\nResiduals must be homoscedasticity and exhibit a constant random when plotted against the fitted prediction\nThe residual should not deviate too much away from the index line in the QQ-plot.\n\n\n\n\n\n\n\nNote\n\n\n\nThings that can violate the above requirements are:\n\nA skewed variable. It is best to transform using the log10() function, or in any scale that’s justifiable to normalise it. if your data is skewed then do a transformation in the models since only care about the residuals. Note that there is no right or wrong when it comes to implementing the transformation, as long as the assumption for the residuals from a linear regression are not violated then all is fine.\nIf one of these characteristics are violated then its an indication that the data are not independent. This could possibly be due to some data artefact (i.e., a critical error in the data itself), or the residuals being correlated with each other.\nPresence of collinearity between residual points. Here, we should check first by mapping the residuals on a map to examine it’s spatial patterns for clustering and testing it with the Moran’s I test.\n\n\n\nLet’s demonstrate by using the built-in function lm() to create a simple linear or multivariable linear regression.\n\n# lm() function builds a regression model and stores model output into the object 'modelMLR'\nmodelMLR &lt;- lm(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile)\n# Include the 'scipen=7' argument in the summary() function remove those annoying scientific notation!\noptions(scipen = 7)\n# summary() calls report the output stored in object 'modelMLR'\nsummary(modelMLR)\n\n\nCall:\nlm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + \n    log10(PTAINDEX), data = spatialdatafile)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39249 -0.06489 -0.00572  0.06046  0.62993 \n\nCoefficients:\n                  Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)      -4.100992   0.095592 -42.901        &lt; 2e-16 ***\nlog10(IMDSCORE)   0.136713   0.007681  17.798        &lt; 2e-16 ***\nlog10(AVEINCOME)  2.036354   0.019340 105.292        &lt; 2e-16 ***\nlog10(PTAINDEX)   0.030055   0.004816   6.241 0.000000000471 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1027 on 4964 degrees of freedom\nMultiple R-squared:  0.789, Adjusted R-squared:  0.7889 \nF-statistic:  6189 on 3 and 4964 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\nImportant\n\n\n\nThe above results presented in the above table indicates the relationship between the dependent and independent variable. Under the column for Estimate reports the overall intercept and coefficients for each independent variable. The p-values for the estimates are reported under the column Pr(&gt;|t|) which determines whether the relationship between the dependent and independent variables are statistically significant or not.\n\n\nLastly, the other two important pieces of information is the Adjusted R-Squared value and the model’s p-value at the bottom. These tells us the performance of the model in general. The former tells you the percentage of variation explained in the house price when including AVEINCOME, IMDSCORE, PTAINDEX in the regression, while the latter informs us whether if this is significant or not.\nHOW TO INTERPRET RESULTS: This is how we fully interpret the coefficients and model performance from the above table:\n\nOn a log transformed scale, if the AVEINCOME was to increase by 1.0%, we expect the house prices to increase by 2.04% and this average increase is statistically significant since the p-value = 0.00000000002 &lt;0.05.\nOn a log transformed scale, if the PTAINDEX was to increase by 1.0%, we expect the house prices to increase marginally by 0.03%. The marginally increase is statistically significant since the p-value = 0.000000000471 &lt; 0.05.\nOn a log transformed scale, if the IMDSCORE was to increase by 1.0%, we expect the house prices to increase marginally by 0.13% and this average increase is statistically significant since the p-value = 0.00000000002 &lt;0.05.\nIn terms of model performance: according to the Adjusted R-Squared value 0.7889 (78.89%) of the variation in the house prices across LSOAs were explained by the model after accounting for AVEINCOME, IMDSCORE, PTAINDEX. Since the Adjusted R-Squared more than 50.0% it is hence a very good model and significant (i.e., p-value = 0.0000000002 &lt; 0.05).\n\nNow that we have fitted the model, we can extract the residuals and insert them as a new column into the spatialdatafile. To perform this action, use the modelMLR object and extract the residuals output from it.\n\n# Extract residuals from \"modelLMR\" object and dump into \"spatialdatafile\" and call the column \"RESIDUALS\"\nspatialdatafile$RESIDUALS &lt;- modelMLR$residuals\n\n# Reporting basic summary measures to have an idea of its distribution before plotting them on map\nsummary(spatialdatafile$RESIDUALS)\n\nLet us generate a map to examine if these residuals show patterns of spatial autocorrelation. We have divergent values for the legends (i.e., negative and positive value) therefore it best to specify in the tm_fill() function that style = \"cont\" and the midpoint = 0, and a divergent colour scheme e.g. Reds to Blue (-RdBu).\n\n# plot the residuals\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESIDUALS\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\ntm_shape(BOROUGHshp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nNotice the spatial patterning and clusters of the LSOAs and the over-prediction (i.e., areas that have negative residuals, or blue tones) and under-prediction (i.e., areas that positive residuals, or red tones). This visual inspection of the residuals is telling you that spatial autocorrelation may be present here. This, however, would require a more formal test.\nNow, let’s use the Moran’s I test to confirm the presence of spatial autocorrelation. Recall week 3’s lectures and computer labs session on spatial autocorrleation? Here, we create the spatial adjacency matrix and apply the Moran’s I test on the modelMLR object using the lm.morantest() function.\n\n#generate unique number for each row\nspatialdatafile$ROWNUM &lt;- 1:nrow(spatialdatafile)\n# We need to coerce the sf spatialdatafile object into a new sp object\nspatialdatafile_2.0 &lt;- as(spatialdatafile, \"Spatial\")\n# Create spatial weights matrix for areas\nWeights &lt;- poly2nb(spatialdatafile_2.0, row.names = spatialdatafile_2.0$ROWNUM)\nWeightsMatrix &lt;- nb2mat(Weights, style='B')\nResidual_WeightMatrix &lt;- mat2listw(WeightsMatrix , style='W')\n# Run the test on the regression model output object \"modelMLR\" using lm.morantest()\nlm.morantest(modelMLR, Residual_WeightMatrix, alternative=\"two.sided\")\n\n\nGlobal Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile)\nweights: Residual_WeightMatrix\n\nMoran I statistic standard deviate = 56.28, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n   0.47489527088   -0.00060260241    0.00007138138 \n\nYou will notice we obtained a statistically significant value (i.e., p-value &lt;0.001) for Moran’s I of value = 0.475. The value of the Moran’s I test somewhat high. This is an indication that the errors (the residuals) are somewhat related to each other and thus not independent. A spatial regression would be much appropriate for modelling this type of data since there’s evidence of spatial autocorrelation.",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#spatial-regression-models",
    "href": "09-spatial_lag_error_regressions.html#spatial-regression-models",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "At this point, we will introduce two kinds of spatial regressions models that can address the issues of spatial autocorrelation. These models are similar in a sense that they all require the inclusion of a spatial weight matrix to account for the spatial configuration of the areas under investigation.\n\nSpatial Lag Model lagged on the dependent variable\nSpatial Error Model\n\nPreviously, we fitted a linear regression model that’s non-spatial, which takes the mathematical formula as follows:\n\n\\[y = \\beta_{0} + x_{1}\\beta_{1} + x_{2}\\beta_{2} + ... +\\epsilon\\]\n\nHere, \\(y\\) is the response variable (i.e., AVEPRICE). The \\(x_{1}\\), \\(x_{2}\\)., etc., are the independent variables (i.e, \\(x_{1}\\), \\(x_{2}\\) & \\(x_{3}\\) are AVEINCOME, IMDSCORE and PTAINDEX respectively). \\(\\beta_{1}\\), \\(\\beta_{2}\\)., etc., are the estimated coefficients for the independent variables. The \\(\\epsilon\\) represents the uncorrelated error term.\nTo make this a Spatial Lag Model lagged on the dependent variable, we tweak the above equation by including the spatial weight matrix \\(W\\) which is multiplied by the dependent variable \\(y\\). This product will have an estimated coefficient termed \\(\\rho\\). The \\(\\rho\\) parameter tells us the degree at which our observed outcome inside the study area of interest is influenced by outcomes measured from its neighbours. It’s model formulation is below:\n\n\\[y = \\rho Wy + \\beta_{0} + x_{1}\\beta_{1} + x_{2}\\beta_{2} + ... +\\epsilon\\]\n\nThe Spatial Error Model, on the other hand, incorporates spatial weight matrix \\(W\\) into the error term, where \\(\\lambda\\) is an estimated coefficient for the product between \\(W\\) and \\(u\\), which \\(u\\) is a correlated spatial error term. It’s model formulation is below:\n\n\\[y =  + \\beta_{0} + x_{1}\\beta_{1} + x_{2}\\beta_{2} + ...+ \\lambda Wu +\\epsilon\\]\n\nHere, we show you how to implement these two commonly used models to see which one better address the issue of spatial autocorrelation. But there are some important points:\n\nWhen implementing these models, we want to ensure that the spatial autocorrelation are accounted for, and therefore, we have to perform the Moran’s I test again. Here, we want to make sure that it is lower than what we observed for the linear regression model.\nWe want to also compare the spatial model against the non-spatial model by checking the AIC values, the one with lowest is the better model.\nPreviously, we were able to interpret the coefficients for the linear regression model. For the spatial models however, the coefficients derived from the model lagged on the dependent variable are difficult to interpret. Therefore, we have to estimate a quantity called impacts which we will use in the interpretation. This step is the most crucial part which must be done\n\n\n\n\nA Spatial Lag Model on the dependent variable assumes that dependencies exist directly among the levels of the dependent variable. That is, the observed outcome in one primary location is affected by other outcomes in nearby or neighbouring locations. For instance, if we implement this model on the LSOA house price, we are assuming that the property prices in one LSOA is impacted by the property prices from nearby neighbouring LSOAs.\nWe can perform this analysis in four steps:\n\nStep 1: Fit the Spatial Lag Model with the dependent variable lagged using the lagsarlm() function.\nStep 2: Use the summary() function to report the results. Here, we are interested in the \\(\\rho\\) parameter & its p-value. Here, we also want to check if the model is appropriate than a non-spatial model - examine the AIC (for lag versus LM) and the one with the lowest AIC is the better model.\nStep 3: Extract the residual lags for the model object and carry out a Moran’s I test using moran.mc() to ensure that the statistic is less than what was obtained for the Moran’s I test for the linear model. Use the tmap to examine its spatial patterning.\nStep 4: Interpretation of the parameters using the impact() function\n\nSTEP ONE\n\n# Fit model using lagsarlm()\n# reuse spatial weight matrix created earlier as an object called \"Residual_WeighMatrix\" \nmodelSLY &lt;- lagsarlm(log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile_2.0, Residual_WeightMatrix)\n\nSTEP TWO\n\n# Report results with summary()\n# We are interested in the rho-coefficient, log-likelihood ratio test's p-value and the AIC\nsummary(modelSLY)\n\n\nCall:lagsarlm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + \n    log10(PTAINDEX), data = spatialdatafile, listw = Residual_WeightMatrix)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.3608275 -0.0540603 -0.0039772  0.0518209  0.6492007 \n\nType: lag \nCoefficients: (asymptotic standard errors) \n                   Estimate Std. Error  z value        Pr(&gt;|z|)\n(Intercept)      -2.6649683  0.0924917 -28.8130       &lt; 2.2e-16\nlog10(IMDSCORE)   0.0435882  0.0068588   6.3551 0.0000000002083\nlog10(AVEINCOME)  1.2144821  0.0286833  42.3412       &lt; 2.2e-16\nlog10(PTAINDEX)   0.0106795  0.0041275   2.5874        0.009671\n\nRho: 0.4522, LR test value: 1354.5, p-value: &lt; 2.22e-16\nAsymptotic standard error: 0.012282\n    z-value: 36.819, p-value: &lt; 2.22e-16\nWald statistic: 1355.6, p-value: &lt; 2.22e-16\n\nLog likelihood: 4937.637 for lag model\nML residual variance (sigma squared): 0.0077091, (sigma: 0.087801)\nNumber of observations: 4968 \nNumber of parameters estimated: 6 \nAIC: -9863.3, (AIC for lm: -8510.8)\nLM test for residual autocorrelation\ntest value: 443.54, p-value: &lt; 2.22e-16\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The \\(\\rho\\) statistic informs us of how the neighbouring LSOA house price values affect the house price at \\(y\\). The \\(\\rho\\) value is a positive value of 0.4522 which means the neighbouring LSOAs affect is a positive manner, and it is statistically significant (i.e., p-value &lt; 0.05). We can see the AIC for the lag model is lower than the original linear regression model (i.e., Lag: -9863.3 vs LM: -8510.8) therefore the lag model is okay.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIMPORTANT NOTE: In a lag model, do not even try to interpret the coefficients for the independent variables - ignore them and their p-values… they are nonsense! Why? This is because there is a global feedback effect happening here - i.e., whenever we change something in our own region (i.e., LSOA) for instance, like the AVEINCOME in a LSOA, that will not only affect our own house price, but when it causes the house price to go up in its own area, this in turn will cause the house prices to increase in its neighbour’s area; and when the neighbour’s house price go up - it is again going to affect our house price… so its an infinite loop. Instead, we interpret the result churn out from the impact function which reports their direct and indirect effects.\n\n\nSTEP THREE\n\n# extract the residuals for modelSLY object and dump back to original sf spatialdatafile object\nspatialdatafile$RESID_SLY &lt;- modelSLY$residuals\n# use Moran's I test using moran.mc() function\nmoran.mc(spatialdatafile$RESID_SLY, Residual_WeightMatrix, 1000, zero.policy = T)\n\n\nMonte-Carlo simulation of Moran I\n\ndata:  spatialdatafile$RESID_SLY \nweights: Residual_WeightMatrix  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.13417, observed rank = 1001, p-value = 0.000999\nalternative hypothesis: greater\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The Moran’s I from the original model was 0.4748. Here, it is 0.1341 which is much lower thus the lag model has accounted for a lot of spatial autocorrelation although it still significantly remains. We can conclude that spatial lag model does address some of the issues of spatial autocorrelation in the model’s residuals but not all since it significantly positive. This is evidenced in the map output of the residual lags.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have to make a mental note of this and compare it with the performance of the Spatial Error Model.\n\n\n\n# generate the map\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESID_SLY\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nSTEP FOUR\n\n# Interpretation of results using impacts\n# impacts\nWeights_2.0 &lt;- as(Residual_WeightMatrix, \"CsparseMatrix\")\ntrMC &lt;- trW(Weights_2.0, type=\"MC\")\nsummary(impacts(modelSLY, tr = trMC, R=100), zstats=TRUE)\n\n\nImpact measures (lag, trace):\n                     Direct    Indirect      Total\nlog10(IMDSCORE)  0.04550352 0.034066675 0.07957020\nlog10(AVEINCOME) 1.26784902 0.949188065 2.21703709\nlog10(PTAINDEX)  0.01114883 0.008346682 0.01949551\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: Here is where we derive meaningful interpretation of the coefficients. A big table is churned out - all we care about is the first table titled: Impact Measures (lag, trace): and the last table titled: Simulated p-values. For instance, let’s interpret the log10(AVEINCOME)(on the log-scale), for the direct effects in its own LSOA, if the levels of income were to increase by 1%, this will cause an increase in the property prices by 1.267% (p &lt; 0.05) in its own LSOA. But for the indirect affects, if the log10(AVEINCOME) were to change across neighbouring LSOAs, this will affect the value of our house prices by 0.949% (p &lt; 0.05). The total column is the combined effect.\n\n\n\n\n\nIn a Spatial Error Model, we assume that the error terms are correlated across observations (i.e., the error of an observed value affects the errors of its neighbors).\nWe essentially repeat the first 3 steps highlighted in section 3.2. for this analysis:\n\nStep 1: Fit the Spatial Error Model using the errorsarlm() function.\nStep 2: Use the summary() function to report the results for the \\(\\lambda\\) parameter and it’s p-value to check if the model is appropriate than a non-spatial one. You can also check with the AIC.\nStep 3: Extract the residuals errors and carry out a Moran’s I test using moran.mc() to ensure that the statistic is less than what was obtained for the Moran’s I test for the linear model. Use the tmap to examine its spatial patterning.\n\nSTEP ONE\n\nmodelSER &lt;- errorsarlm(log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile_2.0, Residual_WeightMatrix)\n\nSTEP TWO\n\n# Report results with summary()\n# We are interested in the rho-coefficient, log-likelihood ratio test's p-value and the AIC\nsummary(modelSER)\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The \\(\\lambda\\) statistic informs us that if there’s a sudden change in the error term for house prices in neighbouring LSOAs how did it impact the error term for the house price in our LSOA at \\(y\\). The \\(\\lambda\\) value is a positive value of 0.7221 which means the affect of neighbouring LSOAs are positive and the impact is statistically significant (i.e., p-value &lt; 0.05). We can see the AIC for the error model is lower than both the original linear regression & lag model (i.e., Error: -10705.3 vs LM: -8510.8 & Lag:-9863.3) therefore the error model better than the two.\n\n\nIMPORTANT NOTE: Unlike the lag model, we can interpret the coefficients from the error model for the independent variables! We can interpret them the same way we did for the linear regression model (see section 7.3.3.).\nSTEP THREE\n\n# extract the residuals for modelSLY object and dump back to original sf spatialdatafile object\nspatialdatafile$RESID_SER &lt;- modelSER$residuals\n# use Moran's I test using moran.mc() function\nmoran.mc(spatialdatafile$RESID_SER, Residual_WeightMatrix, 1000, zero.policy = T)\n\n\n\n\n\n\n\nImportant\n\n\n\nINTERPRETATION: The Moran’s I from the original model was 0.4748. Here, it is -0.0579 which is negative and the lowest for the error model. On top of that there is no evidence of spatial autocorrelation since its p-value is not significant. Therefore, we can conclude that the spatial error model does address the issue of spatial autocorrelation in the residuals. Its a better model than the Linear regression and Lag model for exploring the relationship with those independent variables and accounting for spatial autocorrelation. This is also evidenced in the map output of the residual errors.\n\n\n\n# generate the map\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESID_SER\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(BOROUGHshp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code below computes the spatially lagged model based on the independent variables. Briefly, the results are a quite tedious to interpret directly from the regression model. Hence, you will want to use the post estimation impacts() to get the direct, indirect and total coefficients from model:\n\n\nClick here\n\nSolutions\n\nmodelSLX &lt;- lmSLX(log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + log10(PTAINDEX), data = spatialdatafile_2.0, Residual_WeightMatrix)\nsummary(modelSLX)\n\nIMPACTS_SLX &lt;- impacts(modelSLX, tr = trMC, R=100)\nIMPACTS_SLX",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#references-see-reading-list",
    "href": "09-spatial_lag_error_regressions.html#references-see-reading-list",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "Paper: Guangqing Chi and Jun Zhu (2008) Spatial Regression Models for Demographic Analysis, Popul Res Policy Rev, 27:17-42. DOI 10.1007/s11113-007-9051-8\nBook Chapter: David Darmofal, (2015), Chapter Six: Spatial Lag and Spatial Error Models (2015). Social Analysis for the Social Sciences. Cambridge Press Gain Access to PDF via Institutional login\nPaper: Wen-Ching Wang, Yu-Ju Chang & Hsueh-Ching Wang, (2019), An Application of the Spatial Autocorrelation Method on the Change of Real Estate Prices in Taitung City. Int Jour Geo-Information, (8)249, doi:10.3390/ijgi8060249\nOnline Tutorial: [R Programming] Reka Solymosi and Juanjo Medina (2022) Crime Mapping in R, Chapter 9: Spatial Regression Models Click link",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#data-sources",
    "href": "09-spatial_lag_error_regressions.html#data-sources",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "English indices for Multiple Deprivation (IMD) for 2019 [Source: UK Gov] Click Here\nUK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here\nThe following indicators for averaged house prices, income and PTAL estimates were obtained from London DATASTORE",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "09-spatial_lag_error_regressions.html#useful-videos",
    "href": "09-spatial_lag_error_regressions.html#useful-videos",
    "title": "Week 7: Spatial Lag & Error Regression",
    "section": "",
    "text": "Spatial Regression in R (Part One): The Four Simplest Models (Length: 00:40:36) [Watch on YouTube]",
    "crumbs": [
      "Spatial Models",
      "Week 7: Spatial Lag & Error Regression"
    ]
  },
  {
    "objectID": "02-installation.html",
    "href": "02-installation.html",
    "title": "Installation of R and RStudio",
    "section": "",
    "text": "R, or RStudio is a statistical software programming package that allows the user to carry out different types of statistical analysis. It can also be used as a GIS software to perform various kinds of spatial analysis as you will see throughout the course of GEOG0114. In the same vein, you can use it for data managing and geo-processing (i.e., importing different types of spatial formats for manipulation beforehand for spatial analysis). There are two versions:\n\n\n\n\n\n\n\n\n\nThe famous icon on the left is the version for R, and the one on the right is the version for RStudio. Both software packages are the same. The only difference is that RStudio is attractive, intuitive, and more importantly, it is user-friendly than Base R. So, we will be using this version (i.e., RStudio) throughout the course.\nLet us talk about downloading RStudio.",
    "crumbs": [
      "Getting Started",
      "Installation of R and RStudio"
    ]
  },
  {
    "objectID": "02-installation.html#what-is-r-and-rstudio",
    "href": "02-installation.html#what-is-r-and-rstudio",
    "title": "Installation of R and RStudio",
    "section": "",
    "text": "R, or RStudio is a statistical software programming package that allows the user to carry out different types of statistical analysis. It can also be used as a GIS software to perform various kinds of spatial analysis as you will see throughout the course of GEOG0114. In the same vein, you can use it for data managing and geo-processing (i.e., importing different types of spatial formats for manipulation beforehand for spatial analysis). There are two versions:\n\n\n\n\n\n\n\n\n\nThe famous icon on the left is the version for R, and the one on the right is the version for RStudio. Both software packages are the same. The only difference is that RStudio is attractive, intuitive, and more importantly, it is user-friendly than Base R. So, we will be using this version (i.e., RStudio) throughout the course.\nLet us talk about downloading RStudio.",
    "crumbs": [
      "Getting Started",
      "Installation of R and RStudio"
    ]
  },
  {
    "objectID": "02-installation.html#downloading-and-install-r-and-rstudio-on-to-your-laptop",
    "href": "02-installation.html#downloading-and-install-r-and-rstudio-on-to-your-laptop",
    "title": "Installation of R and RStudio",
    "section": "Downloading and install R and RStudio on to your laptop",
    "text": "Downloading and install R and RStudio on to your laptop\nRStudio is an open source software, and today its the go-to software for many researchers - its highly recommended for anyone in the domains of data science, scientific research, and technical communication.\nIt is easy to access, and easy to download and install. In order for RStudio to work you must first install R. You can follow the steps to download and install the correct version of R and RStudio for your operating system (i.e., Windows or MAC).\n\nInstallation for MacOS users\nYou will need to have the following software installed for R and RStudio to work properly on MAC.\n\nR (version 4.5.1) and RStudio (version 2025.09.0-387)\nXQuartz (version 2.8.5)\nXCode (version 26.0.1)\nGNU Fortran (version 12.2)\n\nInstallation of R (4.5.1) and RStudio (2025.09.0-387) on MAC:\n\n\n\nOS User type\nR (Base)\nRStudio Desktop\n\n\n\n\nMAC (Intel)\nR-4.5.1-x86_64.pkg\nRStudio-2025.09.0-387.dmg\n\n\nMAC (M1, M2, M3, or M4)\nR-4.5.1-arm64.pkg\nRStudio-2025.09.0-387.dmg\n\n\n\n\nFirst download the appropriate R (Base) file for the appropriate MacOS.\nDouble-click the downloaded file (i.e., R-4.5.1-x86_64.pkg or R-4.5.1-arm64.pkg) and follow the steps to complete the installation.\nNext, download the RStudio-2025.09.0-387.dmg to install RStudio.\nDouble-click the downloaded file (i.e., RStudio-2023.06.2-561.dmg) and then drag and drop the RStudio icon into the Applications folder to complete the installation.\n\nInstallation of XQuartz (2.8.5):\nSome functions in R require some of the libraries from XQuartz to function properly on your MAC.\n\nYou can download the latest version of XQuartz (XQuartz-2.8.5.pkg) by clicking on this LINK and complete the installation process by following its steps.\n\nInstallation of XCode (26.0.1):\nSome functions in R require some of the external developer tools from XCode application to function properly on your MAC.\n\nGo to the App Store application and get the XCode app downloaded by clicking on this LINK.\nOnce it is downloaded, you can click on the “OPEN” button to verify it’s been downloaded. A window will prompt you to complete installation.\n\nGNU Fortran (version 12.2):\nR (Base) and some packages require the GNU Fortran 12.2 compiler in order to function smoothly on your MacOS.\n\nDownload this specific version of GNU Fortran 12.2 (gfortran-12.2-universal.pkg) by clicking on this LINK and simply complete the installation process by following the steps on your system.\n\n\n\n\n\n\n\nNote\n\n\n\nThe above four steps should complete the installation process for R and RStudio on MAC.\n\n\n\n\nInstallation for Windows users\nYou will need to have the following software installed for the rstan package to work on Windows.\n\nR (version 4.5.1)\nRtools45 (version 4.5)\nRStudio (version 2025.09.0-387)\n\nInstallation of R (4.5.1) and RStudio RStudio (2025.09.0-387) on Windows:\n\n\n\nOS User type\nR (Base)\nRStudio Desktop\n\n\n\n\nWindows\nR-4.5.1-win.exe\nRStudio-2025.09.0-387.exe\n\n\n\n\nDownload the file for R-4.5.1-win.exe attached in the table above.\nDouble-click the downloaded file (i.e., R-4.5.1-win.exe) and follow the steps to complete the installation.\nNow, we can download the file (i.e., RStudio-2025.09.0-387.exe) for RStudio from the link provided in the above table.\nDouble-click the downloaded file (i.e., RStudio-2025.09.0-387.exe) and follow the steps from the installer to complete the installation.\n\nInstallation of Rtools 4.5\nFor Windows users, after you have completed the installation for R and RStudio, you are required to install the Rtools45 package as it contains some libraries and developer tools for the smooth functioning of R.\n\nDownload the latest version of Rtools45 by clicking on this LINK to initiate the download of the Rtools45 installer.\nDouble-click the downloaded rtools45-6608-6492.exe file and follow the steps to complete the installation.\n\n\n\n\n\n\n\nNote\n\n\n\nThe above two steps should complete the installation process for R and RStudio on Windows.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe installation of all programs on Windows and MacOS are specifically for R (version 4.5.1) and RStudio (version 2025.09.0-387), which are the most up-to-date versions of R and RStudio, respectively, as of writing these instructions today (02/10/2025). If an update get pushed for these software - make sure to check that supporting programs (i.e., Rtools for Windows, and XQuartz & XCode for Mac) are updated.\n\n\nThis concludes the installation section and sets your computer up for the course.",
    "crumbs": [
      "Getting Started",
      "Installation of R and RStudio"
    ]
  },
  {
    "objectID": "04-graphical_representation.html",
    "href": "04-graphical_representation.html",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "",
    "text": "The goal for this week’s session is to introduce you to the various and most common geoprocessing (i.e., spatial operations) and data managing techniques for handling geospatial vector datasets. Geoprocessing is a framework and set of tools for processing geographic and related data - having this under your sleeve will help you to efficiently analyse and represent geospatial data.\n\n\nUnderstanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This tutorial takes you through a simple approach to measuring greenspace access for schools in London, using geometric operations as the main methods for processing and analysing your data. You will construct a buffer data set around our greenspace and determine whether nearby schools intersect with this buffer. We will first visualise our data as points to see if we can identify areas of high versus low access - and then aggregate the data to the ward level for potential further use within analysis with statistical data, such as census information.\n\n\n\nTo enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script\nLet’s go ahead and save our script now, so we know it’s stored in our system - and in the future, we only need to remind ourselves to complete a quick save (e.g. cmd + s (mac) / ctrl + s (windows)).\n\n\n\nBefore you begin, if you have not done so already, please make sure to download all data by clicking here. Create a sub folder called “Week 2” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the downloaded zip folder and store it into “Week 2” folder. Use your newly open R script and set the work directory to Week 2’s folder.\nFor Windows, the code for setting the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 2\")\n\nFor MAC, the code for setting the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 2\")\n\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\n\nThe above packages sf and tmap should have been installed previously in the last session. Therefore, we do not have to install them again, we only need to activate them using the library() function. There will be some new packages we need to install:\n\ntidyverse: this library is a efficient coding style in RStudio. It allows the use of a pipe function (%&gt;%), among other things, within our work and enables more efficient programming.\nunits: this library provide functions that support the conversion of measurement units in R vectors, matrices and arrays, and among other thing, the simplification of units.\ndplyr: this library is an R package whose set of functions are designed to enable dataframe (a spreadsheet-like data structure) manipulation in an intuitive, user-friendly way.\n\n\n# install the packages using the install.package()\ninstall.packages(\"tidyverse\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"units\")\n\n# Load the packages with library()\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"units\")\nlibrary(\"sf\")\nlibrary(\"tmap\")",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#introduction",
    "href": "04-graphical_representation.html#introduction",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "",
    "text": "The goal for this week’s session is to introduce you to the various and most common geoprocessing (i.e., spatial operations) and data managing techniques for handling geospatial vector datasets. Geoprocessing is a framework and set of tools for processing geographic and related data - having this under your sleeve will help you to efficiently analyse and represent geospatial data.\n\n\nUnderstanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This tutorial takes you through a simple approach to measuring greenspace access for schools in London, using geometric operations as the main methods for processing and analysing your data. You will construct a buffer data set around our greenspace and determine whether nearby schools intersect with this buffer. We will first visualise our data as points to see if we can identify areas of high versus low access - and then aggregate the data to the ward level for potential further use within analysis with statistical data, such as census information.\n\n\n\nTo enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script\nLet’s go ahead and save our script now, so we know it’s stored in our system - and in the future, we only need to remind ourselves to complete a quick save (e.g. cmd + s (mac) / ctrl + s (windows)).\n\n\n\nBefore you begin, if you have not done so already, please make sure to download all data by clicking here. Create a sub folder called “Week 2” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the downloaded zip folder and store it into “Week 2” folder. Use your newly open R script and set the work directory to Week 2’s folder.\nFor Windows, the code for setting the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 2\")\n\nFor MAC, the code for setting the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 2\")\n\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\n\nThe above packages sf and tmap should have been installed previously in the last session. Therefore, we do not have to install them again, we only need to activate them using the library() function. There will be some new packages we need to install:\n\ntidyverse: this library is a efficient coding style in RStudio. It allows the use of a pipe function (%&gt;%), among other things, within our work and enables more efficient programming.\nunits: this library provide functions that support the conversion of measurement units in R vectors, matrices and arrays, and among other thing, the simplification of units.\ndplyr: this library is an R package whose set of functions are designed to enable dataframe (a spreadsheet-like data structure) manipulation in an intuitive, user-friendly way.\n\n\n# install the packages using the install.package()\ninstall.packages(\"tidyverse\")\ninstall.packages(\"dplyr\")\ninstall.packages(\"units\")\n\n# Load the packages with library()\nlibrary(\"tidyverse\")\nlibrary(\"dplyr\")\nlibrary(\"units\")\nlibrary(\"sf\")\nlibrary(\"tmap\")",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#case_study_w03",
    "href": "04-graphical_representation.html#case_study_w03",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "2 Case study",
    "text": "2 Case study\nRecent research (Bijnens et al. 2020) has shown that children brought up in proximity to greenspace have a higher IQ and fewer behavioral problems, irrespective of socio-economic background. In our analysis today, we will look to understand whether there are geographical patterns to schools that have high versus low access of greenspace and where a lack of greenspace needs to be addressed in London. In this practical we will try to quantify these visual patterns we may observe and find out which schools are within 400 metres of greenspace that is larger than 50,000 square meters. We then calculate for each ward the percentage of schools that have access to a large greenspace.",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#loading_data_w03",
    "href": "04-graphical_representation.html#loading_data_w03",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "3 Loading our data sets for case study",
    "text": "3 Loading our data sets for case study\nFor this analysis we have three different data sets available: schools in London, greenspace in London (split into two separate data sets), and wards (an administrative geography) in London. All three of our data sets are provided as shapefiles which will make working with the data relatively straight-forward (e.g. even for our point data, the schools, we do not need to convert them from a csv as we often find with this type of data). But we’ll need to do quite a few steps of processing to get our final data set.\nLet’s go ahead and load our three variables - we will use the sf library st_read() command to load our data sets into variables for use within our code:\n\n# load london schools shapefile\nlondon_schools &lt;- st_read('school_data_london_2016.shp')\n\n# load london wards shapefile\nlondon_wards &lt;- st_read('london_wards.shp')\n\n# load the two london greenspace shapefiles\nTL_greenspace &lt;- st_read('TL_GreenspaceSite.shp')\nTQ_greenspace &lt;- st_read('TQ_GreenspaceSite.shp')\n\nTo see what each variable looks like, you can type in plot(name_of_variable) into the R console. This is a quick command to understand both the spatial coverage and attributes of your data - as it will display the data by each of its attribute fields as a plot.",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#data-processing",
    "href": "04-graphical_representation.html#data-processing",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "4 Data Processing",
    "text": "4 Data Processing\nNow we have our data loaded as variables, we’re ready to start processing! In spatial data processing, the question always is: where do I start first? And the easiest answer to that is: make sure all of your data is in the same Projected (or Coordinate) Reference System as each other. Checking - and changing projections - should always be the first step of any workflow as this will ensure you do not carry through any potential mistakes or errors that using the wrong system can cause.\n\n4.1 Reprojecting\nWhen you loaded your data sets in the above step, you may have notice that in the console additional information about the data set is printed - this includes the metadata on the data set’s Coordinate Reference System! As a result, it is quite easy to simply scroll the terminal to check the CRS for each data set - which as you’ll see, all the data sets bar the school are using EPSG 27700, which is the code for British National Grid, whereas our schools data set shows 3857, the code for Web Mercator. That means we need to start with our london_schools variable - as we know that this is the only data set currently in the wrong projection instead of using British National Grid.\nTo reproject our data set, we can use a function within the sf library, known as st_transform(). It is very simple to use - you only need to provide the function with the data set and the code for the new CRS you wish to use with the data. For now, we will simply store the result of this transformation as a new variable - but you could in the future, rewrite this code to use pipes to pipe this transformation when loading the data set.\n\n# reproject london schools from Web Mercator to BNG \nlondon_schools_prj &lt;- st_transform(london_schools, 27700)\n\nWe can now double-check our new variable is in the correct CRS by typing the following into the console and checking the result:\n\n# check CRS \nst_crs(london_schools_prj)\n\nAs you can see from the output above, our data set has been reprojected into EPSG 27700 or British National Grid!\nThe next step to process our london_schools_prj data set is to reduce the schools to only our chosen London extent. As you may have seen from the map above, our schools cover an area larger than our usual London extent. We can even make a quick map of this to check this properly:\n\n# inspect\ntm_shape(london_wards) + \n  tm_polygons() + \ntm_shape(london_schools_prj) + \n  tm_dots()\n\n\n\n\n\n\n\n\n\n\nAs we can see, we indeed have schools outside of our London wards - as a result, we want to remove those schools outside of this boundary. We will do this by first dissolving our ward file to create a more simplified shapefile for use as a “cookie-cutter”.\n\n\n4.2 Dissolving\nTo dissolve a polygon shapefile using R code, we will use the summarise() function that comes from the dplyr library (part of the tidyverse) and summarise our London wards data set by summing its total area (supplied in the HECTARES attribute field/column) across all records. This will reduce our data frame to a single row, which will only contain one attribute - our total area of London, which we can then map/use as our clip (cookie-cutter) feature!\n\n# dissolve\nlondon_outline &lt;- london_wards %&gt;% summarise(area = sum(HECTARES))\n\n# inspect\ntm_shape(london_outline) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n\n4.3 Subsetting\nNow we have our London outline, we can go ahead and clip our schools data set by our London outline. Whilst there is a clip function within the sf library, what we will do here is use a techinque known as spatial subsetting, which is more similar to selecting by location: we will subset our london schools data set by filtering out those that are not within the London Outline. This approach in R is much quicker than using the clip function - although deciding which approach to use is not only a question of speed but also how each function will affect the filtered data. When using a clip function, the function acts exactly like a cookie-cutter and will trim off any data that overlaps with the boundaries used. Conversely, when using a subsetting approach, if a data point or polygon overlaps on the boundary, it will still be included (depending on the topological relationship used) but in its entirety (i.e. no trimming!).\nAs we’re using point data, it is generally easier to use a subset approach. There are multiple ways and notations to conduct spatial subsetting within R:\nFirst, we can either use the [] notation just like you would use for selecting and slicing a normal (table-based) dataframe from R’s base package. Second, sf has its own named functions for geometric operations, including: intersection, difference, symmetrical difference and snap. What actually happens is that when you use the [] notation on the background one of those sf named functions get called. More details on manipulating simple features can be found in this vignette.\nTo keep things simple, we will use the base subsetting approach - which also works similarly when programming in Python, for instance.\n\n# subset London schools\nlondon_schools_prj_ss &lt;- london_schools_prj[london_outline,]\n\n\n\n\n\n\n\nNote\n\n\n\nIn a case like above, you can just overwrite the current london_schools_prj variable as you know it is the data set you want to use. Much of this code could be condensed into several lines using pipes to make our code shorter and more efficient - but then it would be harder to explain! As you progress with R and programming, you are welcome to bring pipes and restructuring into own your code - but even if you don’t, as long as your code does what you need it to do, then that’s our main aim with this course!\n\n\nOnce you have run the above code, you should notice that your london_schools_prj_ss variable now only contains 3,372 records, instead of the original 3,889. We can also plot our variable using the same code as above, to double-check that it worked:\n\n# inspect\ntm_shape(london_wards) + \n  tm_polygons() + \ntm_shape(london_schools_prj_ss) + \n  tm_dots()\n\n\n\n\n\n\n\n\n\n\nWe should now see that our schools are all contained within our ward data set, so we know this data set is ready to be used for analysis. We will now explore which schools are within 400m of greenspace and which are not. But first, we need to get our greenspace data ready so we can create the 400m buffers needed for this analysis.\n\n\n4.4 Unioning\nWe’ve done a lot of processing so far to do with our schools and ward data, but now it’s time for the greenspace data sets. If you look back at your code, you should remember that we have two data sets for our greenspace in London, which we now need to join together. This type of join is typically known as a union - and this is the type of tool you would want to look for across any GUI system.\nWhen it comes to programming, however, in either R or python, there is a much simpler way of joining data sets - and that’s simply copying over the records or observations from one variable into another - and the base library has a ready-to-go function for us to use, known as rbind(). This function allows you to ‘bind’ rows from one or more data sets together. This also works for sf objects.\n\n# join greenspace data sets together\ngreenspace = rbind(TQ_greenspace, TL_greenspace)\n\n\n\n4.5 Clipping\nThe next step is to clip our reduced greenspace data to our London outline. Within sf, the clip function is known as the st_intersection() function - not to be confused with st_intersects() from above! A clip will change the geometry of some of our greenspaces on the outskirts of London, i.e. cookie-cut them precisely to the London outline. If we used the subset approach approach as we did earlier with our point data, we would simply extract all greenspaces that intersect with the London outline - but not change their geometry.\nWhat we can do however if reduce the processing required by our computer by using a mixture of these two methods - if we first subset our all_greenspace data set by our London outline and then run the clip, our processing will be much faster:\n\n# subset and clip\nlondon_greenspace &lt;- greenspace[london_outline,] %&gt;% st_intersection(london_outline)\n\n# inspect\ntm_shape(london_outline) + \n  tm_polygons() + \ntm_shape(london_greenspace) + \n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n\n4.6 Attribute selection\nNow we have only London greenspaces in our data set, the next step, is to reduce the number of greenspaces to only those bigger than 50,000 square meters. To do this, we will use another type of subsetting you’ve probably come across, which is attribute subsetting - by using a simple query to subset only records that have an area larger than 50,000 square metres. To do this, we’ll use the filter() function from the dplyr library we mentioned earlier as well as another function called set_units() which is from the unit library that you’ve loaded - but we haven’t yet discussed. The set_units() function allows us to assign units to numerical values we are using within our query, i.e. here, for our query to run, our value must be in square metres to match the unit of the area_m column.\nTo be able to query on our area, we must first calculate the area of each of our greenspaces. To do so in R, we can use the st_area() function within sf, which will calculate the area of each of our records/observations in our greenspace data set. To store the output of this function as a new column in our london_greenspace data set, we use a simple notation at the end of our london_greenspace variable: $area_m. The $ in R means for this data frame, access the column that proceeds this sign. In our case, we do not as yet have a column called area_m, therefore R will automatically create this column and then store the outputs of the function in this column:\n\n# calculate area\nlondon_greenspace$area_m &lt;- st_area(london_greenspace)\n\nOnce we have our area column, we can now filter our data set based on that column and filter out all greenspace with an area that is smaller than 50,000 square meters.\n\n# filter large greenspaces\nlarge_london_greenspace &lt;- london_greenspace %&gt;% filter(area_m &gt; set_units(50000.0, m^2))\n\nWe now can look at our final greenspace data set against our london outline to see its final coverage:\n\n# inspect\ntm_shape(london_outline) + \n  tm_polygons() + \ntm_shape(large_london_greenspace) + \n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\n\n\n4.7 Buffering\nWe now have our London greenspace data set - we are ready for the last step of processing with this data set - generating our buffers that we can use to find all schools within 400 meters of the large greenspace areas. Once again, the sf library has a function for generating buffers - we just need to know how to deploy it successfully on our London greenspace data set - and this involves understanding how to denote our distance correctly - as well as understanding if and how we can dissolve our buffer into a single record.\nTo do this, we would investigate the documentation of the function st_buffer() to find out what additional parameters it takes - and how. What we can find out is that we need to (of course!) provide a distance for our buffer - but whatever figure we supply, this will be interpreted within the units of the CRS we are using. In our case, we are using British National Grid and, luckily for us, the units of the CRS is metres - which makes are life significantly easier when calculating these buffers. For other CRS, many use a base unit of an Arc Degree, e.g. WGS84. In this case, you technically have two options: 1) reproject your data into a CRS that uses metres as its base unit OR 2) convert your distance into an Arc Degree measurement. Always choose Option 1.\nFortunately none of this is our concern - we know we can simply input the figure of 400 into our buffer and this will generate a buffer of 400m.\n\n# greenspace buffer\ngs_buffer_400m &lt;- st_buffer(large_london_greenspace, dist=400)\n\nAs our final bit of processing with our greenspace buffer, we want to dissolve the whole buffer into a single record. To do this, we’ll replicate the code used for our London ward dissolve, creating a an area value for our buffer records in the process to be used within the summarisation - and then result in a new gs_buffer_400m_single variable:\n\n# dissolve greenspace buffer\ngs_buffer_400m_single &lt;- gs_buffer_400m %&gt;% summarise(area = sum(st_area(gs_buffer_400m)))\n\n# inspect\ntm_shape(london_outline) + \n  tm_polygons() + \ntm_shape(gs_buffer_400m_single) + \n  tm_polygons()",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#greenspace-in-london",
    "href": "04-graphical_representation.html#greenspace-in-london",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "5 Greenspace in London",
    "text": "5 Greenspace in London\nGreat, we are now ready to bring our two data sets together ready for analysis - and to do so, we’ll use subsetting as well as the st_intersects() function, although with this one, we’ll use it in two different ways!\nOur first task is to identify those schools that have access to greenspace - and extract them to create a new variable for use within our final point-in-polygon count (i.e. how many schools within each ward has access to greenspace). As we know, we can subset our london_schools data set by our greenspace buffer quite easily using the subset approach:\n\n# schools within 400m of greenspace\nlondon_schools_gs &lt;- london_schools_prj_ss[gs_buffer_400m_single,]\n\nOur london_schools_gs variable has been subsetted correctly if we end up with 1,770 records, instead of the 3,372 records we had previously. We can now use this data set and our previous london_schools_prj_ss data set to create counts at the ward level. But before we do that, we will create a binary attribute of greenspace access within our london_schools_prj_ss variable to visualise our school ‘points’. To do this, we’ll use the st_intersects() function mentioned above and add a new column, gs_access (i.e. greenspace access), which will tell us which schools have access to greenspace or not.\nThe st_intersects() function is really useful as its output is a simple TRUE or FALSE statement - does this record intersect with the greenspace buffer? This result is what will be stored in our new column as a TRUE or FALSE response and what we can use to map our schools and their greenspace access:\n\n# greenspace access\nlondon_schools_prj_ss$gs_access &lt;- st_intersects(london_schools_prj_ss, gs_buffer_400m_single, sparse=FALSE)\n\nWe could go ahead and recode this to create a 1 or 0, or YES or NO after processing, but for now we’ll leave it as TRUE (green) or FALSE (red). We can go head and now visualise our schools based on this column, to see if they have access (TRUE) or do not have access (FALSE) to greenspace. To do this, we’ll use the tmap library again:\n\n# inspect\ntm_shape(london_schools_prj_ss) + \n    tm_dots(fill = \"gs_access\", \n        fill.scale = tm_scale_categorical(values = c(\"red\", \"darkgreen\")), \n        fill.legend = tm_legend(title = \"Greenspace Access\", frame = FALSE)\n        )\n\n\n\n\n\n\n\n\n\n\nYou’ll be pleased to read that we are finally here - we are at the last stage of our processing and can finally create the ward-level percentage of schools that have greenspace access, versus those that do not! To do this, we’ll be counting the number of points in each of our polygons, i.e. the number of schools in each ward.\nTo do so in R and with sf, it is one line of code - which at first look does not sound at all like it is completing a point-in-polygon calculation - but it does! To create a PIP count within sf, we use the st_intersects() function again - but instead of using the output of TRUE or FALSE, what we actually extract from our function is its lengths recording. The lengths part of this function records how many times a join feature (i.e. our schools) intersects with our main features (i.e. our wards). (Note here, we do not set the sparse function to FALSE but leave it as TRUE/its default by not entering the parameter). As a result, the length of this list is equal to the count of how many schools are within the polygon - i.e. a PIP calculation.\nThis is a really simple way of doing a PIP calculation - and makes it easy for us to store the output of the function and its lengths (and thus the count) directly as a column within our london_wards data set, as so:\n\n# total number of schools in each ward \nlondon_wards$total_schools &lt;- lengths(st_intersects(london_wards, london_schools_prj_ss))\n\n# total number of schools with greenspace access in each ward\nlondon_wards$gs_schools &lt;- lengths(st_intersects(london_wards, london_schools_gs))\n\nAs you can see from the code above, we’ve now calculated this for our total schools data set and the schools that have access to greenspace. The final step in our processing therefore is to create our rate. To do so, we’ll use the same approach of generating a new column within our london_wards data set - and then use a mathematical formula to calculate our rates:\n\n# percentage of schools with greenspace access\nlondon_wards$gs_rate &lt;- (london_wards$gs_schools/london_wards$total_schools)*100\n\nAnd that’s it! We now have our greenspace rate for our wards, which we can now again map:\n\n# inspect\ntm_shape(london_wards) + \n    tm_polygons(fill = \"gs_rate\",\n        fill.scale = tm_scale_intervals(n = 5, style = \"pretty\", values = \"brewer.greens\"),\n        fill.legend = tm_legend(title = \"Greenspace Access [%]\", frame = FALSE)\n        )\n\n\n\n\n\n\n\n\n\n\nWe now have our final data set ready for analysis. Right now, we haven’t introduced you to any statistical or spatial analysis techniques to fully analyse our data set - but instead, we can focus on what are data shows visually!\nThe last step of any programming is to extract our variables into permanent data sets for use at a later time. You can at any point in this practical, extract a permanent data file for each of our variables. For now, we’ll extract our new london_wards data set as we might want to use this in some additional analysis that we could look at next week or for our assessments at a later stage. The great thing about coding this up now, is that it will be easy to re-run all of this analysis and export any of the variables, again, at a later time!\n\n# write\nst_write(obj=london_wards, dsn='london_ward_gs.shp', delete_dsn=TRUE)\n\nYou should now see the data set appear in your files!",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#attributions_w03",
    "href": "04-graphical_representation.html#attributions_w03",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "6 Attributions",
    "text": "6 Attributions\nThis week’s practical uses content and inspiration from:\n\nWilkin, J. 2020. Analysing school access to greenspace in London. [Source]\nDijk, J.V. 2021. Spatial Operations. [Source]",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#reading_w03",
    "href": "04-graphical_representation.html#reading_w03",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "7 References (see reading list)",
    "text": "7 References (see reading list)\n\nGitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 8: Spatial operations and vector overlays Click link\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & systems, Chapter 13: Spatial Analysis Click link\nGitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. (2021) Geocomputation with R, Chapter 4: Spatial data operations Click link\nGitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations Click link\nPaper: [Research] Bijnens, E. et al (2020). Residential green space and child intelligence and behavior across urban, suburban, and rural areas in Belgium: A longitudinal birth cohort study of twins. PLOS Medicine 17(8), e1003213. Click link",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#data-sources",
    "href": "04-graphical_representation.html#data-sources",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "8 Data Sources",
    "text": "8 Data Sources\n\nUK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here\nUK OS Greenspace open data [Source: Ordinance Survey] Click Here\nEarthquake Catalog data [Source: USGS - Science for a changing world] Click Here\nThe national & administrative areas for Turkiye [Source: Database of Global Administrative Areas] Click Here\nBuilding footprints in Türkiye [Source: The Humanitarian Data Exchange] Click Here",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "04-graphical_representation.html#exercise",
    "href": "04-graphical_representation.html#exercise",
    "title": "Week 2: Graphical representation of spatial data",
    "section": "9 Exercise",
    "text": "9 Exercise\nGeoprocessing of spatial-referenced natural hazards: Earthquakes\nThere are severe economic and social consequence of natural hazards such as earthquakes which result innumerable disasters that are instant (e.g., deaths, injury, homes destroyed, transport and communication links disrupted, water pipes burst and water supplies contaminated and many more) and long-term (e.g., disease may spread, re-housing, and most often refugee camps). You can download freely available open-source data on earthquake occurrence worldwide here: https://earthquake.usgs.gov/earthquakes/search/\nTürkiye (or Turkey) is a nation that often experience earthquakes on a frequent basis with nearly over 20 earth tremors in 2022 according to the Turkish Disaster & Emergency Management Authority.\nYou can download the dataset for this homework exercise: [CLICK HERE]\nYou are given the following spatial datasets:\n\nAll_earthquake_points_in_Turkey.shp: 984-point locations of where earthquakes/tremors occurred in Greece and Türkiye in 2019 to 2023 as a shapefile;\ngadm41_TUR_1.shp: The administrative areas of Türkiye as a shapefile; and\nKonya_buildings_at_risk.shp: 25,000+ building footprints from small administrative area in Türkiye called Konya, as a shapefile.\nEarthquake_point_in_Konya.shp: Single point location of an earthquake that is a focal point in Konya.\n\nYour goal for this homework practical is to use the appropriate geoprocessing task to achieve the following tasks:\nTask 1: Use the dataset All_earthquake_points_in_Turkey.shp and gadm41_TUR_1.shp to visualise the distribution of ALL point locations of where earthquakes had occurred ONLY in Türkiye?\nHints: You may want to consider using the following operations: using the st_area() before dissolving to create an outline, and then sub-setting.\nTask 2: How would you go about calculating the overall numbers of reporte earthquakes for each administrative area in Türkiye? Visualise this output as a map.\nHints: You may want to consider creating a new variable total_earthquakes using the st_interects() and lengths() functions.\nTask 3: Use the dataset Konya_buildings_at_risk.shp and Earthquake_point_in_Konya.shp to create a map showing three ringed buffer zones around the focal point of the earthquake. Within each buffer region estimate the number of buildings that are:\n\nAt risk of being destroyed if within distance 0-2000m (1st buffer zone);\nThose that are at risk of being severely damaged if within a distance of 2000-5000m (2nd buffer zone);\nThose that are at risk of receiving partial damages if within a distance of 5000-9000m (last buffer zone);\n\nHints: You may want to consider using the st_buffer(), st_centroids(), st_interects() and st_difference() functions for capturing the buildings within the 0-2000m (1st ring), 2000-5000m (2nd ring), and 5000-9000m (3rd ring) zones. The expected output should look as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHave a go at these questions before revealing the solution codes and output below\n\n\n\n\nClick here to see solution code:\n\nLoad the required packages:\n\nlibrary(\"units\")\nlibrary(\"dplyr\")\nlibrary(\"tidyverse\")\nlibrary(\"sf\")\nlibrary(\"tmap\")\n\nTASK ONE\nLoad datasets for Task One and Two:\n\nall_earthquakes &lt;- st_read(\"All_earthquake_points_in_Turkey.shp\")\n\nReading layer `All_earthquake_points_in_Turkey' from data source \n  `/Users/anwarmusah/Documents/Websites/GEOG0114/all_datasets/Week 2 - Homework Dataset/All_earthquake_points_in_Turkey.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 984 features and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 25.1435 ymin: 35.3918 xmax: 45.3486 ymax: 42.7588\nGeodetic CRS:  WGS 84\n\nturkey_adm_areas &lt;- st_read(\"gadm41_TUR_1.shp\")\n\nReading layer `gadm41_TUR_1' from data source \n  `/Users/anwarmusah/Documents/Websites/GEOG0114/all_datasets/Week 2 - Homework Dataset/gadm41_TUR_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 81 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25.66514 ymin: 35.81543 xmax: 44.83499 ymax: 42.10666\nGeodetic CRS:  WGS 84\n\n\nFirstly, visualise to see what the data looks like:\n\ntm_shape(all_earthquakes) + \n    tm_dots() +\ntm_shape(turkey_adm_areas) + \n    tm_polygons(fill_alpha = 0)\n\n\n\n\n\n\n\n\nAs you can see, there are points outside the study area. Let us clip/subset them to create an outline of the region. To do this, we will need to first calculate the areas before dissolving it into one big outline:\n\nturkey_adm_areas$area_est &lt;- st_area(turkey_adm_areas)\n\nNow, dissolve the areas to form the outline:\n\nturkey_outline &lt;- turkey_adm_areas %&gt;% summarise(area = sum(area_est))\n\nNow, subset or cookie-cut the earthquake points that are only within Turkey:\n\nturkey_earthquakes &lt;- all_earthquakes[turkey_outline,]\n\nNow, visualise output:\n\ntm_shape(turkey_outline) + \n    tm_polygons(fill_alpha = 0) +\ntm_shape(turkey_earthquakes) + \n    tm_dots()\n\n\n\n\n\n\n\n\nTASK TWO Using the derived dataset i.e., turkey_earthquakes, we are aggregating the points in those administrative areas i.e., `turkey_adm_areas:\n\nturkey_adm_areas$total_earthquakes &lt;- lengths(st_intersects(turkey_adm_areas, turkey_earthquakes))\n\nIf you check the data frame, you can see a new column has been created with the total counts of earthquakes per region. You can see that the area K. Maras has the highest burden of 140 earthquake occurrences.\nLet us visualise this result as a map:\n\ntm_shape(turkey_adm_areas) +\n    tm_polygons(fill = \"total_earthquakes\", \n        fill.scale = tm_scale_intervals(n=6, style = \"pretty\", values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(title = \"Total Earthquakes\", frame = FALSE)) +\ntm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\nTASK THREE\nLoad the datasets for Task Three:\n\nfocal_earthquake_point &lt;- st_read(\"Earthquake_point_in_Konya.shp\")\n\nReading layer `Earthquake_point_in_Konya' from data source \n  `/Users/anwarmusah/Documents/Websites/GEOG0114/all_datasets/Week 2 - Homework Dataset/Earthquake_point_in_Konya.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 22 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 32.5587 ymin: 37.9064 xmax: 32.5587 ymax: 37.9064\nGeodetic CRS:  WGS 84\n\nkonya_buildings &lt;- st_read(\"Konya_buildings_at_risk.shp\")\n\nReading layer `Konya_buildings_at_risk' from data source \n  `/Users/anwarmusah/Documents/Websites/GEOG0114/all_datasets/Week 2 - Homework Dataset/Konya_buildings_at_risk.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 28765 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 32.39742 ymin: 37.81919 xmax: 32.66019 ymax: 38.04143\nGeodetic CRS:  WGS 84\n\n\nAs usual, it is always good practice to visual the dataset at face-value to have an understanding of it:\n\ntm_shape(konya_buildings) + \n    tm_polygons(fill_alpha = 0) +\ntm_shape(focal_earthquake_point) + \n    # increased the size of the single focal point to 0.5 and coloured it red to standout more\n    tm_dots(size = 0.5, fill = \"red\", col = \"red\") +\ntm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\nBoth shapefiles have CRS of 4326 (WGS 84) and so distance is measured in decimal degrees for this CRS. This is not a good system for distance as its unintuitive especially if you need to calculate one for creating a buffer zone! It is always best to use one that is in metres, kilometres or miles etc.\n\nLet us use the World Mecartor 3857 CRS as its in metres. So let’s change the CRS from 4326 to 3857 for the shapefiles to be in metres. This makes specifying the distance a lot easier:\n\nkonya_buildings_3857 &lt;- st_transform(konya_buildings, 3857)\nfocal_earthquake_point_3857 &lt;- st_transform(focal_earthquake_point, 3857)\n\nNow, let’s create the three buffer zones around the focal earthquake point:\n\n# create buffer for 2000m\ndestroyed_buffer_2000m &lt;- st_buffer(focal_earthquake_point_3857, dist = 2000)\n# second and third buffers are created for 0-5000 and 0-9000\nsec_buffer_5000m &lt;- st_buffer(focal_earthquake_point_3857, dist = 5000)\nthr_buffer_9000m &lt;- st_buffer(focal_earthquake_point_3857, dist = 9000)\n\nNow, we need to calculate the number of building that exist within 2000m, within 2000-5000m and from 5000-9000m. However, the trickier ones are the zones for 2000-5000m and 5000-9000m as they are rings. Therefore, we need to create ‘ringed’ zones from the buffers through taking the differences between the different buffer zones.\nSo, we can use st_difference() function to create the rings for buffer zones 2000-5000 and 5000-9000 through differencing:\n\n# take the second buffer zone 5000m and substract the inner 2000m buffer from it to retain 2000-5000m\nsevere_damaged_buffer_2000_5000m &lt;- st_difference(sec_buffer_5000m, destroyed_buffer_2000m)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n# take the third buffer zone 9000m and substract the inner 5000m buffer from it to retain 5000-9000m\npartial_damaged_buffer_5000_9000m &lt;- st_difference(thr_buffer_9000m, sec_buffer_5000m)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\n\n\n\n\n\n\nNote\n\n\n\nDo not worry about the warning message - that is just R/RStudio behaving badly!\n\n\nNow, visual the current output:\n\n# add buildings\ntm_shape(konya_buildings_3857) + \n    tm_polygons(fill_alpha = 0) +\n# add the single focal point of the earthquake\ntm_shape(focal_earthquake_point_3857) + \n    tm_dots(size = 0.5, fill = \"red\", col = \"red\") +\n# add the thirdmost outer ring (5000-9000m) and make fill colour pink, and line as dashes in pink colour\ntm_shape(partial_damaged_buffer_5000_9000m) + \n    tm_polygons(fill = \"pink\", fill_alpha = 0.1, col = \"pink\", lwd = 1.1, lty=\"dashed\") +\n# add the second middle ring (2000-5000m)\ntm_shape(severe_damaged_buffer_2000_5000m) + \n    tm_polygons(fill = \"red\", fill_alpha = 0.2, col = \"red\", lwd = 1.1, lty=\"dashed\") +\n# add the 1st ring as solid red polygon with transparency of 0.4\ntm_shape(destroyed_buffer_2000m) + \n    tm_polygons(fill = \"red\", fill_alpha = 0.5, col = \"red\", lwd = 1.5) +\ntm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\nTo get the total number of buildings within each ringed region, we will need to first get the centroids of each building, and then count the number of points that intersect with each zone.\nYou can do that by using the st_centroid() function:\n\nkonya_buildings_points &lt;- konya_buildings_3857 %&gt;% st_centroid()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n\nNow, calculate the number of buildings for the following:\n\n# calculate the number of buildings within 2000m from the focal point\ndestroyed_buffer_2000m$total_buildings &lt;- lengths(st_intersects(destroyed_buffer_2000m, konya_buildings_points))\n# calculate the number of buildings within 2000-5000m from the focal point\nsevere_damaged_buffer_2000_5000m$total_buildings &lt;- lengths(st_intersects(severe_damaged_buffer_2000_5000m, konya_buildings_points))\n# calculate the number of buildings within 5000-9000m from the focal point\npartial_damaged_buffer_5000_9000m$total_buildings &lt;- lengths(st_intersects(partial_damaged_buffer_5000_9000m, konya_buildings_points))\n\n\n\n\n\n\n\nNote\n\n\n\nWhen viewing inside these data frame - you should see the total number of buildings for each buffer. The total number of buildings in that regions is 28,765. We can calculatethe prevalence (or proportion) of buildings by disaster outcomes i.e., destroyed, severely damage, or partially damaged by the earthquake:\n\nPrevalence of buildings that sustain partial damage (5000-9000m from focal point) = 10734/28765 = 37.31%\nPrevalence of buildings that were severely damaged (2000-5000m from focal point) = 3180/28765 = 11.05%\nPrevalence of buildings that were completely destroyed (&lt;2000m from focal point) = 294/28765 = 1.02%\nPrevalence of buildings that survival (&gt;9000m from focal point) = 50.62%\n\n\n\nLet’s put all the information together:\n\n# add buildings\ntm_shape(konya_buildings_3857) + \n    tm_polygons(fill_alpha = 0) +\n# add the single focal point of the earthquake\ntm_shape(focal_earthquake_point_3857) + \n    tm_dots(size = 0.5, fill = \"red\", col = \"red\") +\n# add the thirdmost outer ring (5000-9000m) and make fill colour pink, and line as dashes in pink colour\ntm_shape(partial_damaged_buffer_5000_9000m) + \n    tm_polygons(fill = \"pink\", fill_alpha = 0.1, col = \"pink\", lwd = 1.1, lty=\"dashed\") +\n    # add the second middle ring (2000-5000m)\ntm_shape(severe_damaged_buffer_2000_5000m) + \n    tm_polygons(fill = \"red\", fill_alpha = 0.2, col = \"red\", lwd = 1.1, lty=\"dashed\") +\n# add the 1st ring as solid red polygon with transparency of 0.4\ntm_shape(destroyed_buffer_2000m) + \n    tm_polygons(fill = \"red\", fill_alpha = 0.5, col = \"red\", lwd = 1.5) +\ntm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE) +\ntm_credits(\"Prevalence [%]\\nPartial: 37.31%\\nSevere: 11.05%\\nDestroyed: 1.02%\\nSurvived: 50.62%\", position = c(\"left\", \"top\"), size = 0.8)",
    "crumbs": [
      "Foundation & Theory",
      "Week 2: Graphical representation of spatial data"
    ]
  },
  {
    "objectID": "01-reading_list.html",
    "href": "01-reading_list.html",
    "title": "Reading List for GEOG0114",
    "section": "",
    "text": "If you are having problems securing one or any of these recommended books from the UCL library or elsewhere. You can access PDF versions HERE",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#spatial-analysis-for-data-science",
    "href": "01-reading_list.html#spatial-analysis-for-data-science",
    "title": "Reading List for GEOG0114",
    "section": "Spatial analysis for data science",
    "text": "Spatial analysis for data science\n\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & Systems (4th Edition); Chapters 2: The Nature of Geographic Data Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book)\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & Systems (4th Edition); Chapters 3: Representing Geography Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 3: Basics of Handling Spatial Data in R Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 1: The R Environment Click link (Note: Digital book)\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with tables Click link (Note: Digital book)\nPaper: [R Programming] Tennekes, M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39. https://doi.org/10.18637/jss.v084.i06 (Download)",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#graphical-representation-of-spatial-data",
    "href": "01-reading_list.html#graphical-representation-of-spatial-data",
    "title": "Reading List for GEOG0114",
    "section": "Graphical representation of spatial data",
    "text": "Graphical representation of spatial data\n\nGitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 8: Spatial operations and vector overlays Click link\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & systems, Chapter 13: Spatial Analysis Click link\nGitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. (2021) Geocomputation with R, Chapter 4: Spatial data operations Click link\nGitbook: [R Programming] Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, Chapter 5: Geometry operations Click link\nPaper: [Research] Bijnens, E. et al (2020). Residential green space and child intelligence and behavior across urban, suburban, and rural areas in Belgium: A longitudinal birth cohort study of twins. PLOS Medicine 17(8), e1003213. Click link",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#spatial-autocorrelation",
    "href": "01-reading_list.html#spatial-autocorrelation",
    "title": "Reading List for GEOG0114",
    "section": "Spatial autocorrelation",
    "text": "Spatial autocorrelation\n\nGitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation Click link\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & systems (4th Edition); Chapter 2: The Nature of Geographic Data Click link\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & systems (4th Edition); Chapter 13: Spatial Analysis Click link\nPaper: [Research] Radil, S. 2016. Spatial analysis of crime. In: Huebner, B. and Bynum, T. The Handbook of Measurement Issues in Criminology and Criminal Justice, Chapter 24, pp.536-554. Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital paper)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 7: Spatial Attribute Analysis with R, (See pages 257 to 262) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 8: Localised Spatial Analysis, (See pages 281 to 289) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [Theory] Lloyd, C.D., et al (2010) Spatial Data Analysis: An Introduction for GIS Users; Chapter 4: Key concepts 3 - Spatial Data Analysis, (See pages 43 to 64)\nBook: [Theory] Lloyd, C.D., et al (2010) Spatial Data Analysis: An Introduction for GIS Users; Chapter 8: Local Univariate Measures, (See section 8.4.1. [Local Spatial Autocorrelation] on pages 110 to 113)",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#suitability-mapping-i",
    "href": "01-reading_list.html#suitability-mapping-i",
    "title": "Reading List for GEOG0114",
    "section": "Suitability Mapping I",
    "text": "Suitability Mapping I\n\nPaper: [Theory] R.W. Saaty (1987). The Analytical Hierarchy Process - What it is and how it is used, Mathl Modelling, Volume 9(3-5), pages 161 to 176 Click link\nTechnical Document: [Theory] IDRISI Kilimanjaro: Guide to GIS and Image Processing. Chapter 12: Decision Support - Decision Strategy Analysis, Section on ‘A Typology of Decisions’, pages 151 to 154. Click link\nPaper: [Application] A.A. Ali, A. Ahmad (2019). Spatial susceptibility analysis of vector-borne diseases in KMC using geospatial technique and MCDM approach, Modeling Earth Systems and Environment, Issue 5, pages 1135–1159 Click link\nPaper: [Application] X. Zhao et al., (2020). Malaria Risk Map Using Spatial Multi-Criteria Decision Analysis along Yunnan Border During the Pre-elimination Period, Am J Trop Med Hyg, Volume 103 (2), pages 793–809 Click link\nPaper: [Application] B. Ahmed (2015). Landslide susceptibility mapping using multi-criteria evaluation techniques in Chittagong Metropolitan Area, Bangladesh. Landslides, Issue 12, pages 1077–1095 Click link\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Digital book)",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#suitability-mapping-ii",
    "href": "01-reading_list.html#suitability-mapping-ii",
    "title": "Reading List for GEOG0114",
    "section": "Suitability Mapping II",
    "text": "Suitability Mapping II\n\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Digital book)\nBook: [Theory] Stockwell, D. (2019) Niche Modeling: Predictions from Statistical Distributions; Chapter 4: Topology; CRC Press; pages: 45-63.\nOnline: [Tutorials] Hijmans, R.J., & Elith, J. (2021) Species distribution modelling Click link\nOnline: [Tutorials] Kerkhoff, D. (2016) Ecological Responses to Climate Change: Species Distribution Modelling using Maxent Click link\nPaper: [Application] Escobar, L.E., (2020). Ecological Niche Modeling: An Introduction for Veterinarians and Epidemiologists, Frontiers in Veterinary Science Click link\nPaper: [Application] Banks, W.E., (2017). The application of ecological niche modeling methods to archaeological data in order to examine culture-environment relationships and cultural trajectories; Quarternaire Click link\nPaper: [Application] Liao, Y., Lei, Y., Ren, Z., Chen, H., & Li., D., (2017). Predicting the potential risk area of illegal vaccine trade in China; Scientific Reports, Issue 7, 3883. Click link",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#geostatistical-modelling",
    "href": "01-reading_list.html#geostatistical-modelling",
    "title": "Reading List for GEOG0114",
    "section": "Geostatistical Modelling",
    "text": "Geostatistical Modelling\n\nTechnical Document: [R Programming] Gurpreet Singh and Biju Soman, (2020), Spatial Interpolation using Kriging in R. Download here\nTechnical Document: [R Programming] Fernando Antonanzas-Torres, (2014), Geostatistics examples in R: Ordinary Kriging, Universal Kriging and Inverse Distance Weighted. Download here\nTechnical Document: [R Programming] Adela Volfova and Martin Smejkal, (2012), Geostatistical Methods in R. Download here\nBook: [R Programming] Roger S. Bivand, Edzer J. Pebesma and Virgilio Gomez-Rubio, (2008), Applied Spatial Data Analysis with R, Chapter 8: Interpolation and Geostatistics, pages 191 to 235.\nBook: [R Programming] Michael Dorman, (2014), Learning R for Geospatial Analysis, Chapter 8: Spatial Interpolation of Point Data, pages 241 to 279. Click link (Note: Digital book)\nBook: [Theory] Christopher D. Lloyd, (2010), Spatial Data Analysis: An Introduction for GIS Users, Chapter 9: Spatial Interpolation (Section 9.7. Ordinary Kriging), pages 140 to 150.",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#spatial-models-i",
    "href": "01-reading_list.html#spatial-models-i",
    "title": "Reading List for GEOG0114",
    "section": "Spatial Models I",
    "text": "Spatial Models I\n\nPaper: [Application] Guangqing Chi and Jun Zhu (2008) Spatial Regression Models for Demographic Analysis, Popul Res Policy Rev, 27:17-42. DOI 10.1007/s11113-007-9051-8\nBook Chapter: [Theory] David Darmofal, (2015), Chapter Six: Spatial Lag and Spatial Error Models (2015). Social Analysis for the Social Sciences. Cambridge Press Gain Access to PDF via Institutional login\nPaper: [Application] Wen-Ching Wang, Yu-Ju Chang & Hsueh-Ching Wang, (2019), An Application of the Spatial Autocorrelation Method on the Change of Real Estate Prices in Taitung City. Int Jour Geo-Information, (8)249, doi:10.3390/ijgi8060249\nOnline Tutorial: [R Programming] Reka Solymosi and Juanjo Medina (2022) Crime Mapping in R, Chapter 9: Spatial Regression Models Click link",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "01-reading_list.html#spatial-models-ii",
    "href": "01-reading_list.html#spatial-models-ii",
    "title": "Reading List for GEOG0114",
    "section": "Spatial Models II",
    "text": "Spatial Models II\n\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 7: Spatial Attribute Analysis with R, (See pages 257 to 262) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 8: Localised Spatial Analysis, (See pages 281 to 289) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [Theory] Lloyd, C.D., et al (2010) Spatial Data Analysis: An Introduction for GIS Users; Chapter 8: Exploring spatial patterning in data values, (See section 8.5.3. [Geographically Weighted Regression] on pages 115 to 123)\nBook: [R Programming] Roger S. Bivand, Edzer J. Pebesma and Virgilio Gomez-Rubio, (2008), Applied Spatial Data Analysis with R; Chapter 10: Modelling of Areal Data, (See section 10.5.3. [Geographically Weighted Regression] on pages 305 to 309)\nPaper [Theory] Comber, A. et al (2022) A Route Map for Successful Application of Geographically Weighted Regression; Geographical Analysis; https://doi.org/10.1111/gean.12316 Click link",
    "crumbs": [
      "Getting Started",
      "Reading List for GEOG0114"
    ]
  },
  {
    "objectID": "00-module_overview.html",
    "href": "00-module_overview.html",
    "title": "Principles of Spatial Analysis (2025/26)",
    "section": "",
    "text": "Welcome to GEOG0114: Principles of Spatial Analysis, one of the core 1st term modules for this MSc programme (Social and Geographic Data Science). This module has been designed as an introduction to the core tenets and principles of spatial analysis. Here, you will learn the key concepts and spatial analytical methods, which are applicable to the domains of social science and geography.\nIn the first three weeks, we will cover Spatial analysis for data science, Graphical representation of spatial data and Spatial autocorrelation.\nAfterwards, in week 4, 5 and 6, we will cover a variety of basic geostatistical techniques that require analysis of point and raster data - these include suitability mapping such as Analytical Hierarchical Process (AHP) and Ecological Niche Modelling; and then Geostatistical analysis using Kriging.\nIn week 7, 8, and 9, we will learn specialised tools for performing spatial analysis on vector data (e.g., point, line and areal structures) - these include the various spatial regression model families e.g., Spatial Lag & Error models, Geographical Weighted Regression (GWR) and Spatial Risk Models. All these tutorials will be covered in RStudio.\n\n\nAll lectures and computer practicals will be delivered in-person. All Lectures are held on Monday from 01:00pm to 02:00pm at the North West Wing (Room G17). All computer lab seminars are delivered on Thursday from 03:00pm to 05:00pm at the North West Wing (Room G07).\n\n\n\n\n\n\nImportant\n\n\n\nPlease bring your laptops with you to the computer practicals on Thursday\n\n\n\n\n\nMoodle is the central point of your learning experience for GEOG0114. Please use it on a regular basis to check for updates concerning the schedule for weekly topics, access to the practical materials and assessment. However, note that all lecture notes, videos, practical materials including reading lists and downloadable data sets will be hosted on this webpage.\nYou can download the lecture notes and data sets for the practical lesson from the table below.\n\n\n\nWeek\nDownloads\nTopics\n\n\n\n\n1\nSlides; Data\nSpatial analysis for data science\n\n\n2\nSlides; Data\nGraphical representation of spatial data\n\n\n3\nSlides; Data\nSpatial autocorrelation\n\n\n4\nSlides; Data\nSuitability mapping I\n\n\n5\nSlides; Data\nSuitability mapping II\n\n\n\n\nReading Week\n\n\n6\nSlides; Data\nGeostatistics using Kriging\n\n\n7\nSlides; Data\nSpatial models I (Lag & Error Regression)\n\n\n8\nSlides\nSpatial models II (Geographically Weighted Regression)\n\n\n9\n[Slides]\nSpatial models III (Bayesian Risk Models)\n\n\n10\n[Slides]\nRevision\n\n\n\n\n\n\nFeel free to contact us via email for help, or book appointments for additional support if need be. We are based at UCL Department of Geography, North West Wing building. Our contact information and office details are:\n\n\n\nName\nEmail\nRoom number\n\n\n\n\nAnwar Musah\na.musah@ucl.ac.uk\n115\n\n\nMichael Xiao\nmichael.xiao@ucl.ac.uk\nChorley Institute",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "00-module_overview.html#structure",
    "href": "00-module_overview.html#structure",
    "title": "Principles of Spatial Analysis (2025/26)",
    "section": "",
    "text": "All lectures and computer practicals will be delivered in-person. All Lectures are held on Monday from 01:00pm to 02:00pm at the North West Wing (Room G17). All computer lab seminars are delivered on Thursday from 03:00pm to 05:00pm at the North West Wing (Room G07).\n\n\n\n\n\n\nImportant\n\n\n\nPlease bring your laptops with you to the computer practicals on Thursday",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "00-module_overview.html#moodle",
    "href": "00-module_overview.html#moodle",
    "title": "Principles of Spatial Analysis (2025/26)",
    "section": "",
    "text": "Moodle is the central point of your learning experience for GEOG0114. Please use it on a regular basis to check for updates concerning the schedule for weekly topics, access to the practical materials and assessment. However, note that all lecture notes, videos, practical materials including reading lists and downloadable data sets will be hosted on this webpage.\nYou can download the lecture notes and data sets for the practical lesson from the table below.\n\n\n\nWeek\nDownloads\nTopics\n\n\n\n\n1\nSlides; Data\nSpatial analysis for data science\n\n\n2\nSlides; Data\nGraphical representation of spatial data\n\n\n3\nSlides; Data\nSpatial autocorrelation\n\n\n4\nSlides; Data\nSuitability mapping I\n\n\n5\nSlides; Data\nSuitability mapping II\n\n\n\n\nReading Week\n\n\n6\nSlides; Data\nGeostatistics using Kriging\n\n\n7\nSlides; Data\nSpatial models I (Lag & Error Regression)\n\n\n8\nSlides\nSpatial models II (Geographically Weighted Regression)\n\n\n9\n[Slides]\nSpatial models III (Bayesian Risk Models)\n\n\n10\n[Slides]\nRevision",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "00-module_overview.html#contact-for-geog0114",
    "href": "00-module_overview.html#contact-for-geog0114",
    "title": "Principles of Spatial Analysis (2025/26)",
    "section": "",
    "text": "Feel free to contact us via email for help, or book appointments for additional support if need be. We are based at UCL Department of Geography, North West Wing building. Our contact information and office details are:\n\n\n\nName\nEmail\nRoom number\n\n\n\n\nAnwar Musah\na.musah@ucl.ac.uk\n115\n\n\nMichael Xiao\nmichael.xiao@ucl.ac.uk\nChorley Institute",
    "crumbs": [
      "Module Overview",
      "Welcome"
    ]
  },
  {
    "objectID": "10-gwr_models.html",
    "href": "10-gwr_models.html",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "The goal for this week’s session is to introduce you to a type of spatial model known as the Geographically Weighted Regression (GWR). GWR is a statistical model which can indicate where non-stationarity may take place across space; it can be used to identify how locally weighted regression coefficients may vary across the study area (unlike its counterpart i.e., the Spatial Lagged and/or Error Models which provides global coefficients). Similar to last week’s sessions, we will first need to explore the residuals from a linear regression model to identify evidence of spatial autocorrelation before implementing a spatial model. But, this time round, we will run a GWR and observe how the coefficients for a set of independent variables vary across space.\n\n\nIn this tutorial we will be using last week’s on the house price dataset in London, alongside the social-risk factor variables such as public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) to see how their association with house prices vary across space. This will be achieved using the GWR model.\n\n\n\nWe will be using the data from last week. You should have already downloaded all this data for last week’s practical lesson - if you have not done so already you can access them by clicking here.\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nspdep: Spatial Dependence (Weighting schemes & Spatial Statistics)\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\n\n# Load packages using library() function\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"spdep\")\nlibrary(\"sp\")\n\nThere is a new package we will need to install:\n\nspgwr: this library enable functions for computing geographically weighted regression models in RStudio. This is based on the work by Chris Brunsdon, Martin Charlton and Stewart Fotheringham. The spgwr package will need the following packages installed in the background for it to work: terra and spDataLarge\ncar: this library enable functions for assessing multicollinearity among independent variables within a regression. A common test is the variance inflation factor (VIF) using the function vif() with threshold of 10.\n\n\n# install the packages using the install.package()\ninstall.packages(\"spgwr\")\ninstall.packages(\"terra\")\ninstall.packages(\"spDataLarge\", repos=\"https://nowosad.github.io/drat/\", type=\"source\")\ninstall.packages(\"car\")\n\n# load the packages with library()\nlibrary(\"spgwr\")\nlibrary(\"car\")\n\nYou will see the following message after using the library() function spgwr package:\n\nNOTE: This package does not constitute approval of GWR\nas a method of spatial analysis; see example(gwr)\n\nYou can ignore this message.\n\n\n\nRemember, in last week’s practical, we used the London LSOA 2015 data.csv in RStudio to implement a spatial lag and error model. It contained the following the description for each column as follows:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nLSOACODE\nUnique identification code for the geographic area\n\n\nAVEPRICE (Dependent variable)\nAverage house price estimated for the LSOA in 2015\n\n\nAVEINCOME\nEstimated average annual income for households within an LSOA in 2015\n\n\nIMDSCORE\nDeprivation score for an LSOA in 2015\n\n\nPTAINDEX\nMeasures levels of access/connectivity to public transport\n\n\nPTACAT\nPTAINDEX rendered into a categorical variable\n\n\n\nYou were also provided two sets of spatial data, one that is LSOA-level required for the statistical analysis and the other for Boroughs which is for customising your map. The shapefile names are as follows:\n\nLondon LSOA shape file: London LSOA Areas.shp\nLondon Borough shape file: London Borough Areas.shp\n\nUse the following functions read.csv() and read_sf() to import the above datasets into RStudio’s memory. The codes are essentially the same as last week’s practical:\n\n# add house price and covariate data \ndata.london &lt;- read.csv(\"London LSOA 2015 data.csv\")\n\n# import shapefile data\nlsoa.shp &lt;- read_sf(\"London LSOA Areas.shp\")\nborough.shp &lt;- read_sf(\"London Borough Areas.shp\")\n\nThe code chunk below generates an empty map with the tmap functions. It inspects the spatial configuration of London’s LSOA with the Boroughs superimposed.\n\ntm_shape(LSOAshp) + \n    tm_polygons() +\ntm_shape(BOROUGHshp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\ntm_compass(position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"left\", \"bottom\"))\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\nUse the merge() function to bring together the LSOA shapefile and house price dataset create a spatial data frame object.\n\n# Merge the two files using the \"LSOACODE\" column\nspatialdatafile &lt;- merge(lsoa.shp, data.london, by.x = \"LSOACODE\", by.y = \"LSOACODE\")\n\n\n\n\n\n\n\nThe GWR is a spatial regression model akin to the lag and error models taught last week. Similarly, in order to implement a GWR, we need to first test the residuals for evidence of spatial autocorrelation. To do this, we must first run a linear regression model get the residuals.\nRecall from last week, we can do this for the log-transformed house price against the independent variables (i.e., income, deprivation and accessibility) using the lm() function:\n\n# lm() function builds a regression model and stores model output into the object 'modelMLR'\nmodelMLR &lt;- lm(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile)\n# Include the 'scipen=7' argument in the summary() function remove those annoying scientific notation!\noptions(scipen = 7)\n# summary() calls report the output stored in object 'modelMLR'\nsummary(modelMLR)\n\n\nCall:\nlm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + \n    log10(PTAINDEX), data = spatialdatafile)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39249 -0.06489 -0.00572  0.06046  0.62993 \n\nCoefficients:\n                  Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)      -4.100992   0.095592 -42.901        &lt; 2e-16 ***\nlog10(IMDSCORE)   0.136713   0.007681  17.798        &lt; 2e-16 ***\nlog10(AVEINCOME)  2.036354   0.019340 105.292        &lt; 2e-16 ***\nlog10(PTAINDEX)   0.030055   0.004816   6.241 0.000000000471 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1027 on 4964 degrees of freedom\nMultiple R-squared:  0.789, Adjusted R-squared:  0.7889 \nF-statistic:  6189 on 3 and 4964 DF,  p-value: &lt; 2.2e-16\n\nFor interpretation of the above global coefficients see last week’s practical!\nYou can check if presence of multicollinearity among the independent variable by using the vif() to ensure the independent variables are not co-linear with each other by ensuring their Variance Inflation Factor (VIF) is less than 10. If it exceeds 10, then those variables will have to be discarded from the model, and such regression will need to be re-run again without those discarded variables.\n\nvif(modelMLR)\n\nOutput from vif() function:\n\nlog10(AVEINCOME)  log10(IMDSCORE)  log10(PTAINDEX) \n        1.963519         2.066221         1.403051 \n\nAll variables are not co-linear with each other since the VIFs are all less than 10. No need to discard any of the variables.\nNow, extract the residuals and deposit them into our spatial data frame spatialdatafile\n\n# Extract residuals from \"modelLMR\" object and dump into \"spatialdatafile\" and call the column \"RESIDUALS\"\nspatialdatafile$RESIDUALS &lt;- modelMLR$residuals\n\nOutput shows mapped residuals:\n\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESIDUALS\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\ntm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nNotice the spatial patterning and clusters of the LSOA areas where there’s an over-prediction of the house prices (i.e., areas that have negative residuals, or blue tones) and under-prediction (i.e., areas that positive residuals, or red tones). This visual inspection of the residuals is an indication that spatial autocorrelation may be present. We can confirm by using an Moran’s I test.\n\n\n\nCreate an adjacency spatial weight matrix apply Moran’s I test using the lm.morantest():\n\n#generate unique number for each row\nspatialdatafile$ROWNUM &lt;- 1:nrow(spatialdatafile)\n# We need to coerce the sf spatialdatafile object into a new sp object\nspatialdatafile_2.0 &lt;- as(spatialdatafile, \"Spatial\")\n# Create spatial weights matrix for areas\nWeights &lt;- poly2nb(spatialdatafile_2.0, row.names = spatialdatafile_2.0$ROWNUM)\nWeightsMatrix &lt;- nb2mat(Weights, style='B')\nResidual_WeightMatrix &lt;- mat2listw(WeightsMatrix , style='W')\n# Run the test on the regression model output object \"modelMLR\" using lm.morantest()\nlm.morantest(modelMLR, Residual_WeightMatrix, alternative=\"two.sided\")\n\nOutput from lm.morantest():\n\nGlobal Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX),\ndata = spatialdatafile)\nweights: Residual_WeightMatrix\n\nMoran I statistic standard deviate = 56.28, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n   0.47489527088   -0.00060260241    0.00007138138 \n\nThe Moran’s I value is 0.475, which is a statistically significant value (i.e., p-value &lt;0.001). This indicates strong evidence of spatial autocorrelation. Now, let’s see how we can use a different spatial model such as a GWR to derive local associations for each area.\n\n\n\n\nGWR overcomes the limitation of the the standard linear, and spatial lag and error regression models of generating a global set of estimates. The basic idea behind GWR is to examine the way in which the relationships between a dependent variable and a set of predictors might vary over space. As explained in the lecture, the GWR operates by moving a search window from one regression point to the next, working sequentially through all the existing regression points in the dataset. A set of regions is then defined around each regression point and within the search window. A regression model is then fitted to all data contained in each of the identified regions around a regression point, with data points closer to the sample point being weighted more heavily than are those farther away. This process is repeated for all samples points in the dataset. For a data set of 4968 observations GWR will fit 4968 weighted regression models. The resulting local estimates can then be mapped at the locations of the regression points to view possible variations in the relationships between variables.\n\n\nThe analysis requires points since the weights are distance-based and thus, to appropriately implement a GWR model on LSOA area data, we will need to calculate the centroids from the LSOAs and then deposit them as coordinates within the spatial data frame.\n\n# calculate the centroids from geometries\nspatialdatafile &lt;- st_centroid(spatialdatafile)\n# insert coordinates into spatialdatafile note longitude column is X and latitude column is Y\nspatialdatafile &lt;- cbind(spatialdatafile, st_coordinates(spatialdatafile))\n\n\n\n\nLet demonstrate with an example using the Adaptive Bandwidth, which is the preferred approach since the algorithm will compute and specify the adaptive kernel that involves using varying bandwidth to define a region around regression points - instead of using some Fixed Bandwidth.\nLet’s find the optimal bandwidth using the Adaptive Bandwidth approach using gwr.sel() function:\n\n# finding the bandwidth \nBwG &lt;- gwr.sel(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile, coords = cbind(spatialdatafile$X, spatialdatafile$Y), adapt = TRUE)\n\n# see optimal bandwidth\nBwG\n\nThe optimal bandwidth is 0.001270292 indicating the proportion of observations (or k-nearest neighbours) to be included in the weighting scheme. In this example, the optimal bandwidth indicates that for a given LSOA, 0.127% of its nearest neighbours should be used to calibrate the relevant local regression; that is about 6 LSOAs. The search window will thus be variable in size depending on the extent of LSOAs.\n\n\n\n\n\n\nNote\n\n\n\nImportant note: Here the optimal bandwidth is defined based on a data point’s k-nearest neighbours. It can also be defined by geographical distance as done above for the fixed spatial kernel.\n\n\nWe next fit a GWR based on an adaptive bandwidth using the gwr() function:\n\n# start timer to time how long it takes to run a gwr() on computer\nstart.timer &lt;- proc.time()\n\n# gwr() model. You need hatmatrix and se.fit specified as TRUE for testing statistical significance \ngwr.model &lt;- gwr(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile, coords = cbind(spatialdatafile$X, spatialdatafile$Y), adapt=BwG, hatmatrix=TRUE, se.fit=TRUE)\n\n# end timer and calculate how it took for model to complete churning\nend.timer &lt;- proc.time() - start.timer\n# report time taken\nend.timer\n\nOutput on time taken:\n\n    user   system  elapsed \n1490.684   96.123 1586.152 \n\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANT NOTE: Due to the following options specified in the gwr() (i.e., hatmatrix and se.fit as TRUE) it might take sometime for the gwr() to complete the estimation of local coefficients, area-specific R2, and standard error. This take approximately 1490.684 seconds (24.84 minutes) on a desktop with 3 GHz 6-core Intel Core i5 processor with 32 GB of RAM memory. It will be interesting to see how long it takes on a standard UCL desktop or on your personal laptops. Here is the ideal time for you to have a second coffee break while it churns.\n\n\n\n# see results, finally!\ngwr.model\n\nOutput of gwr.model object:\n\nCall:\ngwr(formula = log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + \n    log10(PTAINDEX), data = spatialdatafile, coords = cbind(spatialdatafile$X, \n    spatialdatafile$Y), adapt = BwG, hatmatrix = TRUE, se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.001270292 (about 6 of 4968 data points)\nSummary of GWR coefficient estimates at data points:\n                         Min.      1st Qu.       Median      3rd Qu.         Max.  Global\nX.Intercept.     -15.43999209  -4.26990561  -1.88765409   0.41755910  17.35599732 -4.1010\nlog10.AVEINCOME.  -2.44598947   1.14890387   1.61696094   2.08452831   4.27148364  2.0364\nlog10.IMDSCORE.   -0.94681986  -0.10975325  -0.00013244   0.12469522   1.08510758  0.1367\nlog10.PTAINDEX.   -0.58629189  -0.07460266  -0.02198655   0.02452116   0.33555611  0.0301\nNumber of data points: 4968 \nEffective number of parameters (residual: 2traceS - traceS'S): 1549.125 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 3418.875 \nSigma (residual: 2traceS - traceS'S): 0.07033289 \nEffective number of parameters (model: traceS): 1118.3 \nEffective degrees of freedom (model: traceS): 3849.7 \nSigma (model: traceS): 0.06628063 \nSigma (ML): 0.05834576 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): -11242.87 \nAIC (GWR p. 96, eq. 4.22): -13014.96 \nResidual sum of squares: 16.9122 \nQuasi-global R2: 0.9318326 \n\nUpon first glance, much of the outputs, especially the global coefficients, are identical to the outputs of the linear model. However, if we compare the Global R-square values we can see that GWR performs way better than the linear model (i.e, GWR: 0.9318 (93.18%) versus LM: 0.7889 (78.89%)). Let’s proceed to now report the various outputs of this model across each polygon.\n\n\n\nThe results are always stored as a SDF object within the gwr.model output we generated from the gwr(). We can extract the SDF object according with the code below:\n\ngwr.data &lt;- as.data.frame(gwr.model$SDF)\n\n# save the output as a csv so you don't have to run the model again and use in the future!\nwrite.csv(gwr.data, file = \"gwr output.csv\", row.names = FALSE)\n\nVery important notes about the gwr.data object:\n\nThe following columns in the gwr.data contain our LSOA-specific coefficients for log-transformed income, deprivation and accessibility: log10.AVEINCOME., log10.IMDSCORE. and log10.PTAINDEX. respectively. These results tell us the association between the dependent and independent variable within an LSOA.\nThe following columns in the gwr.data contain our LSOA-specific standard error estimates for log-transformed income, deprivation and accessibility: log10.AVEINCOME._se, log10.IMDSCORE._se and log10.PTAINDEX._se. These results helps us calculate a test statistic for assessing whether an association found between the dependent and independent variable in an LSOA is statistically significant or not.\nThe following column localR2 in the gwr.data helps us to assess the model’s performance. Values close to 1 is an indication of a very good model and vice versa.\n\nWe can generate these results as maps. Now, let us bring the results together into one clean spatial data frame:\n\n# create neat spatial data frame by keeping first two columns\nlsoa_result &lt;- st_drop_geometry(spatialdatafile[,c(1,2)])\n\n# insert coefficients into lsoa_result object\nlsoa_result$CoefLogInc &lt;- gwr.data[,\"log10.AVEINCOME.\"]\nlsoa_result$CoefLogIMD &lt;- gwr.data[,\"log10.IMDSCORE.\"]\nlsoa_result$CoefLogPTAL &lt;- gwr.data[,\"log10.PTAINDEX.\"]\n\n# insert standard errors into lsoa_result object\nlsoa_result$SELogInc &lt;- gwr.data[,\"log10.AVEINCOME._se\"]\nlsoa_result$SELogIMD &lt;- gwr.data[,\"log10.IMDSCORE._se\"]\nlsoa_result$SELogPTAL &lt;- gwr.data[,\"log10.PTAINDEX._se\"]\n\n# insert localR2 estimates into lsoa_result object\nlsoa_result$localR2 &lt;- gwr.data[,\"localR2\"]\n\nUsing deprivation score, we report its associated impact on house prices across the LSOAs in London by mapping its LSOA-specific coefficients using the code:\n\ntm_shape(lsoa_result) + \n    tm_polygons(fill = \"CoefLogIMD\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE, title=\"Coefficient: Log(IMD) [%]\")) +\n    tm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\nAlso use the summary() to help with the interpretation:\n\nsummary(lsoa_result$CoefLogIMD)\n\nOutput from the above summary() function:\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.9468199 -0.1097532 -0.0001324  0.0066443  0.1246952  1.0851076\n\n\n\n\n\n\n\nNote\n\n\n\nINTEPRETATION: There is spatial variability in the relationship between our variable socioeconomic deprivation (transformed) and averaged house price (transformed) in London. The GWR outputs reveals that local coefficients range from a minimum value of -0.946 to a maximum value of 1.085, indicating that one percentage point increase in the levels of deprivation in LSOAs of London is associated with a reduction of 0.946% in house prices in some LSOAs and (weirdly) an increase of 1.085% in others. Broadly, the relationship are opposing.\n\n\nNow, while the above map offer some valuable insights to understand the spatial pattering of relationships, they do not identify whether these associations are statistically significant. They may or may not be. Roughly, for a sample that is sufficiently large - if take a coefficient estimate and divide it by its corresponding standard error to get an absolute value (i.e., t-score) that exceeds either -1.96 or +1.96, then it is statistically significant.\nWe can easily compute estimates to determine significance:\n\n# compute t-score statistic\nlsoa_result$tstatIMD &lt;- lsoa_result$CoefLogIMD / lsoa_result$SELogIMD\n# create significance column with: \"Reduction: Significant\", \"Not Significant\", \"Increase: Significant\" \nlsoa_result$significant &lt;- cut(lsoa_result$tstatIMD,\n    breaks=c(min(lsoa_result$tstatIMD), -2, 2, max(lsoa_result$tstatIMD)),\n    labels=c(\"Reduction: Significant\",\"Not Significant\", \"Increase: Significant\"))\n\nNow, let us report which relationship are significant or not by mapping the significance categories using the code:\n\ntm_shape(lsoa_result) + \n    tm_polygons(fill = \"significant\", \n        fill.scale = tm_scale_categorical(values = c(\"red\", \"white\", \"blue\"), \n            labels = c(\"Reduction: Significant\", \"Not Significant\", \"Increase: Significant\")),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nINTEPRETATION: For instance, in the Borough of Hounslow, we can see a significant reduction in house prices in relation to increased levels of socioeconomic deprivation (adjusted for income and accessibility). Such reduction are clustered in the mid-section of Borough of Hounslow which were coloured red. Note that in far north eastern section of the Borough of Hounslow with pockets of LSOA’s coloured blue shows a significant increase in house price in relationship to IMD which is difficult to explain and thus can be interpreted as a chance finding. All sections that are white are not significant.\n\n\nLet finally map the local r-square values to examine model performance:\n\n# map localR2 to examine model performance\ntm_shape(lsoa_result) + \n    tm_polygons(fill = \"localR2\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.spectral\"),\n        fill.legend = tm_legend(frame = FALSE, title=\"Adaptive: Local R2\")) +\n    tm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nINTEPRETATION: The areas that are going towards the shade of dark reds are local regression models that have broadly performed poorly in its prediction for house price and its association with the three variables (income, deprivation and PTAL). Likewise, the areas that are going towards the shade of dark blues are local regression models that have broadly performed very well in its prediction for house price and its association with the three variables (income, deprivation and PTAL).\n\n\n\n\n\n\nMap the local coefficients and significance categories for average income and PTAL. What is the interpretation for the spatial patterning and association with house prices in London?\n\n\n\nThis week’s practical was inspired from:\n\nCheshire, J. & Lansley, G. (2016). An Introduction to Spatial Data Analysis and Visualisation in R. (A) GWR tutorials were originally hosted on the CDRC website (Requires a log in account) LINK. (B) Direct link to the GWR tutorials posted by Professor James Cheshire as a Bookdown via his Github account LINK\nRowe, F. & Arribas-Bel, D. (2022). Spatial Modelling for Data Scientists, Chapter 9: Geographically Weighted Regression LINK\n\n\n\n\n\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 7: Spatial Attribute Analysis with R, (See pages 257 to 262) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 8: Localised Spatial Analysis, (See pages 281 to 289) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [Theory] Lloyd, C.D., et al (2010) Spatial Data Analysis: An Introduction for GIS Users; Chapter 8: Exploring spatial patterning in data values, (See section 8.5.3. [Geographically Weighted Regression] on pages 115 to 123)\nBook: [R Programming] Roger S. Bivand, Edzer J. Pebesma and Virgilio Gomez-Rubio, (2008), Applied Spatial Data Analysis with R; Chapter 10: Modelling of Areal Data, (See section 10.5.3. [Geographically Weighted Regression] on pages 305 to 309)\nPaper [Theory] Comber, A. et al (2022) A Route Map for Successful Application of Geographically Weighted Regression; Geographical Analysis; https://doi.org/10.1111/gean.12316 Click link\n\n\n\n\n\nEnglish indices for Multiple Deprivation (IMD) for 2019 [Source: UK Gov] Click Here\nUK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here\nThe following indicators for averaged house prices, income and PTAL estimates were obtained from London DATASTORE",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#learning-outcomes",
    "href": "10-gwr_models.html#learning-outcomes",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "In this tutorial we will be using last week’s on the house price dataset in London, alongside the social-risk factor variables such as public transport accessibility (PTA), average income and socioeconomic deprivation (IMD) to see how their association with house prices vary across space. This will be achieved using the GWR model.",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#datasets-setting-up-the-work-directory",
    "href": "10-gwr_models.html#datasets-setting-up-the-work-directory",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "We will be using the data from last week. You should have already downloaded all this data for last week’s practical lesson - if you have not done so already you can access them by clicking here.\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nspdep: Spatial Dependence (Weighting schemes & Spatial Statistics)\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\n\n# Load packages using library() function\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"spdep\")\nlibrary(\"sp\")\n\nThere is a new package we will need to install:\n\nspgwr: this library enable functions for computing geographically weighted regression models in RStudio. This is based on the work by Chris Brunsdon, Martin Charlton and Stewart Fotheringham. The spgwr package will need the following packages installed in the background for it to work: terra and spDataLarge\ncar: this library enable functions for assessing multicollinearity among independent variables within a regression. A common test is the variance inflation factor (VIF) using the function vif() with threshold of 10.\n\n\n# install the packages using the install.package()\ninstall.packages(\"spgwr\")\ninstall.packages(\"terra\")\ninstall.packages(\"spDataLarge\", repos=\"https://nowosad.github.io/drat/\", type=\"source\")\ninstall.packages(\"car\")\n\n# load the packages with library()\nlibrary(\"spgwr\")\nlibrary(\"car\")\n\nYou will see the following message after using the library() function spgwr package:\n\nNOTE: This package does not constitute approval of GWR\nas a method of spatial analysis; see example(gwr)\n\nYou can ignore this message.\n\n\n\nRemember, in last week’s practical, we used the London LSOA 2015 data.csv in RStudio to implement a spatial lag and error model. It contained the following the description for each column as follows:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nLSOACODE\nUnique identification code for the geographic area\n\n\nAVEPRICE (Dependent variable)\nAverage house price estimated for the LSOA in 2015\n\n\nAVEINCOME\nEstimated average annual income for households within an LSOA in 2015\n\n\nIMDSCORE\nDeprivation score for an LSOA in 2015\n\n\nPTAINDEX\nMeasures levels of access/connectivity to public transport\n\n\nPTACAT\nPTAINDEX rendered into a categorical variable\n\n\n\nYou were also provided two sets of spatial data, one that is LSOA-level required for the statistical analysis and the other for Boroughs which is for customising your map. The shapefile names are as follows:\n\nLondon LSOA shape file: London LSOA Areas.shp\nLondon Borough shape file: London Borough Areas.shp\n\nUse the following functions read.csv() and read_sf() to import the above datasets into RStudio’s memory. The codes are essentially the same as last week’s practical:\n\n# add house price and covariate data \ndata.london &lt;- read.csv(\"London LSOA 2015 data.csv\")\n\n# import shapefile data\nlsoa.shp &lt;- read_sf(\"London LSOA Areas.shp\")\nborough.shp &lt;- read_sf(\"London Borough Areas.shp\")\n\nThe code chunk below generates an empty map with the tmap functions. It inspects the spatial configuration of London’s LSOA with the Boroughs superimposed.\n\ntm_shape(LSOAshp) + \n    tm_polygons() +\ntm_shape(BOROUGHshp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\ntm_compass(position = c(\"right\", \"top\")) + \ntm_scalebar(position = c(\"left\", \"bottom\"))\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\nUse the merge() function to bring together the LSOA shapefile and house price dataset create a spatial data frame object.\n\n# Merge the two files using the \"LSOACODE\" column\nspatialdatafile &lt;- merge(lsoa.shp, data.london, by.x = \"LSOACODE\", by.y = \"LSOACODE\")",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#implementing-the-linear-regression-model",
    "href": "10-gwr_models.html#implementing-the-linear-regression-model",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "The GWR is a spatial regression model akin to the lag and error models taught last week. Similarly, in order to implement a GWR, we need to first test the residuals for evidence of spatial autocorrelation. To do this, we must first run a linear regression model get the residuals.\nRecall from last week, we can do this for the log-transformed house price against the independent variables (i.e., income, deprivation and accessibility) using the lm() function:\n\n# lm() function builds a regression model and stores model output into the object 'modelMLR'\nmodelMLR &lt;- lm(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile)\n# Include the 'scipen=7' argument in the summary() function remove those annoying scientific notation!\noptions(scipen = 7)\n# summary() calls report the output stored in object 'modelMLR'\nsummary(modelMLR)\n\n\nCall:\nlm(formula = log10(AVEPRICE) ~ log10(IMDSCORE) + log10(AVEINCOME) + \n    log10(PTAINDEX), data = spatialdatafile)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.39249 -0.06489 -0.00572  0.06046  0.62993 \n\nCoefficients:\n                  Estimate Std. Error t value       Pr(&gt;|t|)    \n(Intercept)      -4.100992   0.095592 -42.901        &lt; 2e-16 ***\nlog10(IMDSCORE)   0.136713   0.007681  17.798        &lt; 2e-16 ***\nlog10(AVEINCOME)  2.036354   0.019340 105.292        &lt; 2e-16 ***\nlog10(PTAINDEX)   0.030055   0.004816   6.241 0.000000000471 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.1027 on 4964 degrees of freedom\nMultiple R-squared:  0.789, Adjusted R-squared:  0.7889 \nF-statistic:  6189 on 3 and 4964 DF,  p-value: &lt; 2.2e-16\n\nFor interpretation of the above global coefficients see last week’s practical!\nYou can check if presence of multicollinearity among the independent variable by using the vif() to ensure the independent variables are not co-linear with each other by ensuring their Variance Inflation Factor (VIF) is less than 10. If it exceeds 10, then those variables will have to be discarded from the model, and such regression will need to be re-run again without those discarded variables.\n\nvif(modelMLR)\n\nOutput from vif() function:\n\nlog10(AVEINCOME)  log10(IMDSCORE)  log10(PTAINDEX) \n        1.963519         2.066221         1.403051 \n\nAll variables are not co-linear with each other since the VIFs are all less than 10. No need to discard any of the variables.\nNow, extract the residuals and deposit them into our spatial data frame spatialdatafile\n\n# Extract residuals from \"modelLMR\" object and dump into \"spatialdatafile\" and call the column \"RESIDUALS\"\nspatialdatafile$RESIDUALS &lt;- modelMLR$residuals\n\nOutput shows mapped residuals:\n\ntm_shape(spatialdatafile) + \n    tm_polygons(fill = \"RESIDUALS\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE)) +\ntm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nNotice the spatial patterning and clusters of the LSOA areas where there’s an over-prediction of the house prices (i.e., areas that have negative residuals, or blue tones) and under-prediction (i.e., areas that positive residuals, or red tones). This visual inspection of the residuals is an indication that spatial autocorrelation may be present. We can confirm by using an Moran’s I test.\n\n\n\nCreate an adjacency spatial weight matrix apply Moran’s I test using the lm.morantest():\n\n#generate unique number for each row\nspatialdatafile$ROWNUM &lt;- 1:nrow(spatialdatafile)\n# We need to coerce the sf spatialdatafile object into a new sp object\nspatialdatafile_2.0 &lt;- as(spatialdatafile, \"Spatial\")\n# Create spatial weights matrix for areas\nWeights &lt;- poly2nb(spatialdatafile_2.0, row.names = spatialdatafile_2.0$ROWNUM)\nWeightsMatrix &lt;- nb2mat(Weights, style='B')\nResidual_WeightMatrix &lt;- mat2listw(WeightsMatrix , style='W')\n# Run the test on the regression model output object \"modelMLR\" using lm.morantest()\nlm.morantest(modelMLR, Residual_WeightMatrix, alternative=\"two.sided\")\n\nOutput from lm.morantest():\n\nGlobal Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX),\ndata = spatialdatafile)\nweights: Residual_WeightMatrix\n\nMoran I statistic standard deviate = 56.28, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\nsample estimates:\nObserved Moran I      Expectation         Variance \n   0.47489527088   -0.00060260241    0.00007138138 \n\nThe Moran’s I value is 0.475, which is a statistically significant value (i.e., p-value &lt;0.001). This indicates strong evidence of spatial autocorrelation. Now, let’s see how we can use a different spatial model such as a GWR to derive local associations for each area.",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#geographically-weighted-regression-gwr",
    "href": "10-gwr_models.html#geographically-weighted-regression-gwr",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "GWR overcomes the limitation of the the standard linear, and spatial lag and error regression models of generating a global set of estimates. The basic idea behind GWR is to examine the way in which the relationships between a dependent variable and a set of predictors might vary over space. As explained in the lecture, the GWR operates by moving a search window from one regression point to the next, working sequentially through all the existing regression points in the dataset. A set of regions is then defined around each regression point and within the search window. A regression model is then fitted to all data contained in each of the identified regions around a regression point, with data points closer to the sample point being weighted more heavily than are those farther away. This process is repeated for all samples points in the dataset. For a data set of 4968 observations GWR will fit 4968 weighted regression models. The resulting local estimates can then be mapped at the locations of the regression points to view possible variations in the relationships between variables.\n\n\nThe analysis requires points since the weights are distance-based and thus, to appropriately implement a GWR model on LSOA area data, we will need to calculate the centroids from the LSOAs and then deposit them as coordinates within the spatial data frame.\n\n# calculate the centroids from geometries\nspatialdatafile &lt;- st_centroid(spatialdatafile)\n# insert coordinates into spatialdatafile note longitude column is X and latitude column is Y\nspatialdatafile &lt;- cbind(spatialdatafile, st_coordinates(spatialdatafile))\n\n\n\n\nLet demonstrate with an example using the Adaptive Bandwidth, which is the preferred approach since the algorithm will compute and specify the adaptive kernel that involves using varying bandwidth to define a region around regression points - instead of using some Fixed Bandwidth.\nLet’s find the optimal bandwidth using the Adaptive Bandwidth approach using gwr.sel() function:\n\n# finding the bandwidth \nBwG &lt;- gwr.sel(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile, coords = cbind(spatialdatafile$X, spatialdatafile$Y), adapt = TRUE)\n\n# see optimal bandwidth\nBwG\n\nThe optimal bandwidth is 0.001270292 indicating the proportion of observations (or k-nearest neighbours) to be included in the weighting scheme. In this example, the optimal bandwidth indicates that for a given LSOA, 0.127% of its nearest neighbours should be used to calibrate the relevant local regression; that is about 6 LSOAs. The search window will thus be variable in size depending on the extent of LSOAs.\n\n\n\n\n\n\nNote\n\n\n\nImportant note: Here the optimal bandwidth is defined based on a data point’s k-nearest neighbours. It can also be defined by geographical distance as done above for the fixed spatial kernel.\n\n\nWe next fit a GWR based on an adaptive bandwidth using the gwr() function:\n\n# start timer to time how long it takes to run a gwr() on computer\nstart.timer &lt;- proc.time()\n\n# gwr() model. You need hatmatrix and se.fit specified as TRUE for testing statistical significance \ngwr.model &lt;- gwr(log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + log10(PTAINDEX), data = spatialdatafile, coords = cbind(spatialdatafile$X, spatialdatafile$Y), adapt=BwG, hatmatrix=TRUE, se.fit=TRUE)\n\n# end timer and calculate how it took for model to complete churning\nend.timer &lt;- proc.time() - start.timer\n# report time taken\nend.timer\n\nOutput on time taken:\n\n    user   system  elapsed \n1490.684   96.123 1586.152 \n\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANT NOTE: Due to the following options specified in the gwr() (i.e., hatmatrix and se.fit as TRUE) it might take sometime for the gwr() to complete the estimation of local coefficients, area-specific R2, and standard error. This take approximately 1490.684 seconds (24.84 minutes) on a desktop with 3 GHz 6-core Intel Core i5 processor with 32 GB of RAM memory. It will be interesting to see how long it takes on a standard UCL desktop or on your personal laptops. Here is the ideal time for you to have a second coffee break while it churns.\n\n\n\n# see results, finally!\ngwr.model\n\nOutput of gwr.model object:\n\nCall:\ngwr(formula = log10(AVEPRICE) ~ log10(AVEINCOME) + log10(IMDSCORE) + \n    log10(PTAINDEX), data = spatialdatafile, coords = cbind(spatialdatafile$X, \n    spatialdatafile$Y), adapt = BwG, hatmatrix = TRUE, se.fit = TRUE)\nKernel function: gwr.Gauss \nAdaptive quantile: 0.001270292 (about 6 of 4968 data points)\nSummary of GWR coefficient estimates at data points:\n                         Min.      1st Qu.       Median      3rd Qu.         Max.  Global\nX.Intercept.     -15.43999209  -4.26990561  -1.88765409   0.41755910  17.35599732 -4.1010\nlog10.AVEINCOME.  -2.44598947   1.14890387   1.61696094   2.08452831   4.27148364  2.0364\nlog10.IMDSCORE.   -0.94681986  -0.10975325  -0.00013244   0.12469522   1.08510758  0.1367\nlog10.PTAINDEX.   -0.58629189  -0.07460266  -0.02198655   0.02452116   0.33555611  0.0301\nNumber of data points: 4968 \nEffective number of parameters (residual: 2traceS - traceS'S): 1549.125 \nEffective degrees of freedom (residual: 2traceS - traceS'S): 3418.875 \nSigma (residual: 2traceS - traceS'S): 0.07033289 \nEffective number of parameters (model: traceS): 1118.3 \nEffective degrees of freedom (model: traceS): 3849.7 \nSigma (model: traceS): 0.06628063 \nSigma (ML): 0.05834576 \nAICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): -11242.87 \nAIC (GWR p. 96, eq. 4.22): -13014.96 \nResidual sum of squares: 16.9122 \nQuasi-global R2: 0.9318326 \n\nUpon first glance, much of the outputs, especially the global coefficients, are identical to the outputs of the linear model. However, if we compare the Global R-square values we can see that GWR performs way better than the linear model (i.e, GWR: 0.9318 (93.18%) versus LM: 0.7889 (78.89%)). Let’s proceed to now report the various outputs of this model across each polygon.\n\n\n\nThe results are always stored as a SDF object within the gwr.model output we generated from the gwr(). We can extract the SDF object according with the code below:\n\ngwr.data &lt;- as.data.frame(gwr.model$SDF)\n\n# save the output as a csv so you don't have to run the model again and use in the future!\nwrite.csv(gwr.data, file = \"gwr output.csv\", row.names = FALSE)\n\nVery important notes about the gwr.data object:\n\nThe following columns in the gwr.data contain our LSOA-specific coefficients for log-transformed income, deprivation and accessibility: log10.AVEINCOME., log10.IMDSCORE. and log10.PTAINDEX. respectively. These results tell us the association between the dependent and independent variable within an LSOA.\nThe following columns in the gwr.data contain our LSOA-specific standard error estimates for log-transformed income, deprivation and accessibility: log10.AVEINCOME._se, log10.IMDSCORE._se and log10.PTAINDEX._se. These results helps us calculate a test statistic for assessing whether an association found between the dependent and independent variable in an LSOA is statistically significant or not.\nThe following column localR2 in the gwr.data helps us to assess the model’s performance. Values close to 1 is an indication of a very good model and vice versa.\n\nWe can generate these results as maps. Now, let us bring the results together into one clean spatial data frame:\n\n# create neat spatial data frame by keeping first two columns\nlsoa_result &lt;- st_drop_geometry(spatialdatafile[,c(1,2)])\n\n# insert coefficients into lsoa_result object\nlsoa_result$CoefLogInc &lt;- gwr.data[,\"log10.AVEINCOME.\"]\nlsoa_result$CoefLogIMD &lt;- gwr.data[,\"log10.IMDSCORE.\"]\nlsoa_result$CoefLogPTAL &lt;- gwr.data[,\"log10.PTAINDEX.\"]\n\n# insert standard errors into lsoa_result object\nlsoa_result$SELogInc &lt;- gwr.data[,\"log10.AVEINCOME._se\"]\nlsoa_result$SELogIMD &lt;- gwr.data[,\"log10.IMDSCORE._se\"]\nlsoa_result$SELogPTAL &lt;- gwr.data[,\"log10.PTAINDEX._se\"]\n\n# insert localR2 estimates into lsoa_result object\nlsoa_result$localR2 &lt;- gwr.data[,\"localR2\"]\n\nUsing deprivation score, we report its associated impact on house prices across the LSOAs in London by mapping its LSOA-specific coefficients using the code:\n\ntm_shape(lsoa_result) + \n    tm_polygons(fill = \"CoefLogIMD\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.rd_bu\"),\n        fill.legend = tm_legend(frame = FALSE, title=\"Coefficient: Log(IMD) [%]\")) +\n    tm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\nAlso use the summary() to help with the interpretation:\n\nsummary(lsoa_result$CoefLogIMD)\n\nOutput from the above summary() function:\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.9468199 -0.1097532 -0.0001324  0.0066443  0.1246952  1.0851076\n\n\n\n\n\n\n\nNote\n\n\n\nINTEPRETATION: There is spatial variability in the relationship between our variable socioeconomic deprivation (transformed) and averaged house price (transformed) in London. The GWR outputs reveals that local coefficients range from a minimum value of -0.946 to a maximum value of 1.085, indicating that one percentage point increase in the levels of deprivation in LSOAs of London is associated with a reduction of 0.946% in house prices in some LSOAs and (weirdly) an increase of 1.085% in others. Broadly, the relationship are opposing.\n\n\nNow, while the above map offer some valuable insights to understand the spatial pattering of relationships, they do not identify whether these associations are statistically significant. They may or may not be. Roughly, for a sample that is sufficiently large - if take a coefficient estimate and divide it by its corresponding standard error to get an absolute value (i.e., t-score) that exceeds either -1.96 or +1.96, then it is statistically significant.\nWe can easily compute estimates to determine significance:\n\n# compute t-score statistic\nlsoa_result$tstatIMD &lt;- lsoa_result$CoefLogIMD / lsoa_result$SELogIMD\n# create significance column with: \"Reduction: Significant\", \"Not Significant\", \"Increase: Significant\" \nlsoa_result$significant &lt;- cut(lsoa_result$tstatIMD,\n    breaks=c(min(lsoa_result$tstatIMD), -2, 2, max(lsoa_result$tstatIMD)),\n    labels=c(\"Reduction: Significant\",\"Not Significant\", \"Increase: Significant\"))\n\nNow, let us report which relationship are significant or not by mapping the significance categories using the code:\n\ntm_shape(lsoa_result) + \n    tm_polygons(fill = \"significant\", \n        fill.scale = tm_scale_categorical(values = c(\"red\", \"white\", \"blue\"), \n            labels = c(\"Reduction: Significant\", \"Not Significant\", \"Increase: Significant\")),\n        fill.legend = tm_legend(frame = FALSE)) +\n    tm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nINTEPRETATION: For instance, in the Borough of Hounslow, we can see a significant reduction in house prices in relation to increased levels of socioeconomic deprivation (adjusted for income and accessibility). Such reduction are clustered in the mid-section of Borough of Hounslow which were coloured red. Note that in far north eastern section of the Borough of Hounslow with pockets of LSOA’s coloured blue shows a significant increase in house price in relationship to IMD which is difficult to explain and thus can be interpreted as a chance finding. All sections that are white are not significant.\n\n\nLet finally map the local r-square values to examine model performance:\n\n# map localR2 to examine model performance\ntm_shape(lsoa_result) + \n    tm_polygons(fill = \"localR2\", \n        fill.scale = tm_scale_continuous(midpoint = 0, values = \"-brewer.spectral\"),\n        fill.legend = tm_legend(frame = FALSE, title=\"Adaptive: Local R2\")) +\n    tm_shape(borough.shp) + tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"BOROUGHN\", size = \"AREA\") +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_scalebar(position = c(\"left\", \"bottom\")) +\n    tm_layout(frame = FALSE)\n\nOutput from the above tmap() functions:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nINTEPRETATION: The areas that are going towards the shade of dark reds are local regression models that have broadly performed poorly in its prediction for house price and its association with the three variables (income, deprivation and PTAL). Likewise, the areas that are going towards the shade of dark blues are local regression models that have broadly performed very well in its prediction for house price and its association with the three variables (income, deprivation and PTAL).",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#tasks",
    "href": "10-gwr_models.html#tasks",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "Map the local coefficients and significance categories for average income and PTAL. What is the interpretation for the spatial patterning and association with house prices in London?",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#attributions",
    "href": "10-gwr_models.html#attributions",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "This week’s practical was inspired from:\n\nCheshire, J. & Lansley, G. (2016). An Introduction to Spatial Data Analysis and Visualisation in R. (A) GWR tutorials were originally hosted on the CDRC website (Requires a log in account) LINK. (B) Direct link to the GWR tutorials posted by Professor James Cheshire as a Bookdown via his Github account LINK\nRowe, F. & Arribas-Bel, D. (2022). Spatial Modelling for Data Scientists, Chapter 9: Geographically Weighted Regression LINK",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#references-see-reading-list",
    "href": "10-gwr_models.html#references-see-reading-list",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "Book: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 7: Spatial Attribute Analysis with R, (See pages 257 to 262) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 8: Localised Spatial Analysis, (See pages 281 to 289) Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [Theory] Lloyd, C.D., et al (2010) Spatial Data Analysis: An Introduction for GIS Users; Chapter 8: Exploring spatial patterning in data values, (See section 8.5.3. [Geographically Weighted Regression] on pages 115 to 123)\nBook: [R Programming] Roger S. Bivand, Edzer J. Pebesma and Virgilio Gomez-Rubio, (2008), Applied Spatial Data Analysis with R; Chapter 10: Modelling of Areal Data, (See section 10.5.3. [Geographically Weighted Regression] on pages 305 to 309)\nPaper [Theory] Comber, A. et al (2022) A Route Map for Successful Application of Geographically Weighted Regression; Geographical Analysis; https://doi.org/10.1111/gean.12316 Click link",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "10-gwr_models.html#data-sources",
    "href": "10-gwr_models.html#data-sources",
    "title": "Week 8: Geographically Weighted Regression (GWR)",
    "section": "",
    "text": "English indices for Multiple Deprivation (IMD) for 2019 [Source: UK Gov] Click Here\nUK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here\nThe following indicators for averaged house prices, income and PTAL estimates were obtained from London DATASTORE",
    "crumbs": [
      "Spatial Models",
      "Week 8: Geographically Weighted Regression (GWR)"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html",
    "href": "08-geostatistics_kriging.html",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Today, we will learn how to perform semivariogram analysis which can be used to create continuous predictive maps based on spatial interpolation technique called Kriging. In this session, we will investigate spatial variation in outcomes that are, in theory, spatially continuous; for example, the concentrations of ambient air pollutants such as Sulphur dioxide (\\(SO_{2}\\)) (a toxic gas emitted from sulphur rich fuels (i.e., coal, oil or diesel) when burnt) which can be present anywhere but in practice are only measured at specified point locations, such as air quality monitoring stations.\nWe will assess for the presence of spatial autocorrelation using semivariogram which is distance-based, which describes the correlation of a variable with itself through geographic space. Here, a Positive Autocorrelation exists when measurements close to one another are alike than they would be due to chance or through random sampling. The presence of autocorrelation for spatially continuous phenomena can be established by using semivariograms.\nEstimates from a semivariogram can be are used from model construction for spatial interpolation across a study area, whereby values at unsampled locations are predicted from neighbouring sites. A popular form of interpolation, which is based on the spatial attribute’s outcome variable, is known as Kriging (a technique named after a South African engineer, Danie G. Krige (1919 to 2013)).\n\n\nThe US Environmental Agency have positioned air quality monitors for surveillance of over 100 different types of pollutants that exist as toxic gases, particulates and heavy metals. For Sulphur Dioxide (\\(SO_{2}\\)), there are 458 active air monitors that takes hourly readings for concentrations of \\(SO_{2}\\) (in parts per billion (pbb)). An annual estimate for \\(SO_{2}\\) was calculated for each station at its location. Car usage, urbanisation and social economic deprivation, alongside of other anthropogenic activities such as coal burning, across the USA increases the risk of elevated pollution of \\(SO_{2}\\).\nUsing geostatistical methods and taking into account of car usage, urbanisation and levels of deprivation - what areas in USA have higher concentrations of \\(SO_{2}\\) exceeding the annual average of 40 ppb which is a national safety limit for cause of concern?\nLet’s use Kriging to find out!\n\n\n\nBefore you begin do make sure to download all data by clicking here. Create a folder on called “Week 6” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 6” folder. Open a new R script and set the work directory to Week 6’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 6\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 6\")\n\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nraster: Raster/gridded data analysis and manipulation\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\n\n# Load packages using library() function\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"raster\")\nlibrary(\"sp\")\n\nThe above packages sf, tmap, raster & sp should have been installed in the previous session(s). We will need to install the following package:\n\ngstat: provides functions for univariable and multivariable geostatistical analysis.\ngeoR: provides additional functions for geostatistical and variogram analysis.\n\n\n# Install the packages: gstat using the install.package()\ninstall.packages(\"gstat\")\ninstall.packages(\"geoR\")\n\n# Load the packages with library()\nlibrary(\"gstat\")\nlibrary(\"geoR\")\n\n\n\n\nLet us first import the quantitative data i.e., US 2019 SO2 Emissions data.csv into R/RStudio.\n\n# Use read.csv() to import \ndatafile &lt;- read.csv(file = \"US 2019 SO2 Emissions data.csv\", header = TRUE, sep = \",\")\n\nNOTE: The description of the column names are as follows:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nCountyRef\nThe County & State for where the air quality monitors are located in US\n\n\nLongitude\nLongitude (in decimal degrees)\n\n\nLatitude\nLatitude (in decimal degrees)\n\n\nMean_SO2\nAnnual Mean (ppb) concentrations of Ambient sulphur dioxide (\\(SO_{2}\\)) in 2019\n\n\n\n\n\n\n\n\nShape file: US National border named US Nation Border.shp\nShape file: US State border named US State Borders.shp\nRaster: US Car Usage (5000m resolution) named US Prevalence of Car Usage 5km.tif\nRaster: US Urbanisation Index (5000m resolution) named US Urbanisation Index 5km.tif\nRaster: US Socioeconomic Deprivation (5000m resolution) named US Socioeconomic Deprivation 5km.tif\n\n\n# Use read_sf() function to load shape file \nUS_Nation_Border_shp &lt;- st_read(\"US Nation Border.shp\")\nUS_State_Border_shp &lt;- st_read(\"US State Borders.shp\")\n\n\n\n\nThere are a couple of things we need to do before proceeding with the analysis:\n\nThe datafile is a data frame object in RStudio’s memory, and not a spatial object. We need to coerce into a spatial sf object\nThe shapefiles for US Nation Border.shp & US State Borders.shp are in a different CRS called Spherical Mercator 3857 which measures distance in meters and not in decimal degrees. We need to transform the longitude and latitude of our stations which are in decimal degrees to the CRS of Spherical Mercator 3857\n\n\n# Coerce the spreadsheet into a sf object\n# First tell R that it’s coordinates are currently in decimal degrees (i.e., WGS84 'crs = 4326') before the transformation\ndatafile_sf &lt;- st_as_sf(datafile, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n# Now apply the transformation from WGS84 to Mercator i.e., = 3857\ndatafile_sf_prj &lt;- st_transform(datafile_sf, 3857)\n# Inspect the details\nst_crs(datafile_sf_prj)\n\nThe code chunk below generates an empty map with the tmap functions. It shows just the border of USA and the point locations for the air quality monitoring stations superimposed.\n\ntm_shape(US_Nation_Border_shp) + \n    tm_polygons(fill = \"white\", col = \"black\") + \ntm_shape(datafile_sf_prj) + \n    tm_dots(fill = \"black\") + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nSemivariograms describe how data are related with distance by plotting the semivariance against the separation distance, known as the experimental or empirical semivariogram. The semivariance is defined as half the average squared difference between points separated by some distance h. As the separation distance h between samples increase, we would expect the semivariance to also increase (again, because near samples are more similar than distant samples).\n\n\n\n\n\n\n\n\n\nIn the generic semivariogram shown above, there are three important parameters:\n\nSill: The maximum semivariance value observed, and it indicates the threshold for values beyond (i.e., flatline) which there is no spatial autocorrelation. NOTE: the Partial Sill is a value calculated by taking the difference between the Sill and Nugget (i.e., Partial Sill = Sill - Nugget)\nRange: The maximum separation distance h at which we will expect to find evidence of spatial autocorrelation. A separation distance beyond the range samples are no longer correlated.\nNugget: This describes the variance of the measurement error combined with spatially uncorrelated variations at distances shorter than the sample spacing, namely noise in the data. The larger the nugget relative to the sill, the less spatial dependence there is in the data and less useful Kriging will be.\n\n\n\n\n\n\n\nNote\n\n\n\nTwo important assumptions of a basic semivariogram are that the spatial process under investigation are: i.) stationary, i.e., the spatial autocorrelation between the measurements of same variables in a given area is the same for all locations; and ii.) isotropic, spatial autocorrelation is the same in every direction. If the autocorrelation differs by direction, it is termed as anisotropic.\n\n\nTo be used in Kriging, a semivariogram plot (akin to the above image) must be generated to estimate the 3 parameters (i.e., sill, nugget & range) from the points termed experimental or empirical semivariogram. These are used as initial values to fit a modeled or theoretical semivariogram which can be in one of three major forms:\n\nGaussian Model (Left)\nSpherical Model (Center)\nExponential Model (Right)\n\n\n\n\n\n\n\n\n\n\nOnce the modelled semivariogram has been defined, it can be used in Kriging.\n\n\nUse the function variogram() to create the object for plotting the empirical variogram\n\n# coerce datafile_sf_prj to be a 'sp' spatial dataframe object as it's \n# ---variogram does not use 'sf' objects\n\n# ignore warning message\n\ndatafile_sp_prj &lt;- as(datafile_sf_prj, \"Spatial\")\n# use variogram() function to compute the semivariance with a null model Mean_SO2 as outcome\nSO2_emp.variogram &lt;- variogram(Mean_SO2~1, datafile_sp_prj)\n# Compute the object to reveal a table\nSO2_emp.variogram\n\n\n\n\n\n\n\nNote\n\n\n\nnp in the output is the number of paired considered within the separation distance dist; gamma is the averaged semivariance for the number of paired points within the separation distance dist.\n\n\nLet us plot these values to see the empirical semivariogram\n\nplot(SO2_emp.variogram)\n\n\n\n\n\n\n\n\n\n\nFrom the output (i.e., plot and table), we should note the approximate values for the partial sill, nugget and range.\n\nThe nugget is roughly 17 (i.e. base on starting gamma value from the table).\nThe range is roughly 1180000 meters (i.e. base on peak value for gamma and it’s corresponding dist).\nThe partial sill is 65. This is derived from the peak value for gamma subtracted by the nugget (82 - 17 = 65).\n\nThese initial values give us an idea of what to expect when we proceed to fit a theoretical semivariogram using the fit.variogram(). It will help us to generate the fitted models.\n\n\n\nWe are going to fit a model to the empirical semivariogram in order to determine the appropriate function for Kriging (i.e., spherical (Sph), exponential (Exp) or gaussian (Gau)).\nWe will start fitting the various models:\n\n# Fit exponential\nexp_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(65, \"Exp\", 1180000, 17))\nexp_SO2_emp.variogram\nplot(SO2_emp.variogram, exp_SO2_emp.variogram, main  = \"Exponential model (Nug: 3.6, PSill: 55.9, Range: 296255m)\")\n\n\n\n\n\n\n\n\n\n\n\n# Fit Spherical\nsph_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(65, \"Sph\", 1180000, 17))\nsph_SO2_emp.variogram\nplot(SO2_emp.variogram, sph_SO2_emp.variogram, main  = \"Spherical model (Nug: 10.5, PSill: 49.6, Range: 857452m)\")\n\n\n\n\n\n\n\n\n\n\n\n# Fit gaussian\ngau_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(65, \"Gau\", 1180000, 17))\ngau_SO2_emp.variogram\nplot(SO2_emp.variogram, gau_SO2_emp.variogram, main  = \"Gaussian model (Nug: 12.8, PSill: 39.1, Range: 244807m)\")\n\n\n\n\n\n\n\n\n\n\nBy eyeballing the images - it difficult to discern whether the exponential or spherical model provides a better fit to the empirical semivariogram. We can use the fit.variogram() function to determine which is the best model amongst them.\n\n# select the best model\nbest_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(c(\"Exp\", \"Gau\", \"Sph\")))\nbest_SO2_emp.variogram\n\nFrom the output (see column which says model and row 2 it highlights \"Exp\"), it shows that the exponential model is the best fit with a nugget = 3.6, Partial Sill =  55.9 and Range = 296255m. We therefore select the exponential model in our Kriging to make the spatial prediction for \\(SO_{2}\\). Lets proceed to Krige.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANT NOTES: The interpretation is as follows: the nugget in the exponential model is smaller than the other proposed models. It is small which is also an indication of evidence of larger spatial dependence in the concentrations for \\(SO_{2}\\) across sampling sites in USA; A separation distance with values beyond 296255m (where it curve starts to plateau) and beyond the semivariance’s threshold where it flat lines (sill of 59.5 (i.e., 55.9 + 3.6)) - there should expect that the spatial autocorrelation in the observed levels of \\(SO_{2}\\) are not present anymore.\n\n\n\n\n\n\n\n\nLet us create a template raster for interpolation. The extent of the raster template should based on the points. We are going to make the resolution of grid be at 5000m by 5000m (5km by 5km) accordingly.\n\nRasterTemplate &lt;- raster(datafile_sp_prj)\nres(RasterTemplate) &lt;- 5000\n\nNext, declare template as a spatial grid\n\ngrid.interpolation &lt;- as(RasterTemplate, 'SpatialGrid')\n\n\n\n\nNow, we are going to use the parameters from the exponential theoretical semivariogram model to interpolate the concentrations of ambient SO2 for the entire study region\n\nmodelKrigingExp &lt;- gstat(formula = Mean_SO2~1, locations = datafile_sp_prj, model = exp_SO2_emp.variogram)\n\nThe results are stored in modelKrigingExp object. Lets add the results of the interpolation to our grid template using the predict() function\n\n# this may take roughly 5mins\nKriged_SO2 &lt;- predict(modelKrigingExp, grid.interpolation)\n\nThe above analysis produces two separate rasters: i.) Predicted \\(SO_{2}\\) and ii.) Variation in \\(SO_{2}\\). Let export the results and make some visualisation using the tmap functions\n\n\n\nNow, save both the prediction and variance a multi-layer raster (a ‘brick’ raster):\n\nbrickedKriged_SO2_Results &lt;- brick(Kriged_SO2)\n\nWe can save them individually from the multi-layer raster as follows:\n\n# Separate the rasters accordingly\nexp.prediction &lt;- raster(brickedKriged_SO2_Results, layer = 1)\nexp.variance &lt;- raster(brickedKriged_SO2_Results, layer = 2)\n#  save the output locally on your computer\nwriteRaster(exp.prediction, \"Predicted SO2 levels in USA.tif\", format=\"GTiff\", overwrite = TRUE)\nwriteRaster(exp.variance, \"Variance SO2 levels in USA.tif\", format=\"GTiff\", overwrite = TRUE)\n\n\n\n\nWe will need to perform a couple of steps before visualisation. First, we will need to mask the values of the raster predictions were made outside of US Border’s region. To do this, we use the mask() function:\n\n# mask values of raster outside regions of US Border\nUS_Nation_Border_sp_shp &lt;- as(US_Nation_Border_shp, \"Spatial\")\nexp.prediction_masked &lt;- mask(exp.prediction, US_Nation_Border_shp)\n\nNext, we are going to make the raster image sit perfectly sit within the plot’s frame using the country’s bounding box or extent. We can extract the bounding box by using st_bbox() function on the US_Nation_Border_shp shape file object, this basically gives us the extent of the region.\n\nframeExtent &lt;- st_bbox(US_Nation_Border_shp)\nframeExtent\n\n\n&gt; frameExtent\n     xmin      ymin      xmax      ymax \n-13885235   2819925  -7452828   6340334\n\n\n\n\n\n\n\nNote\n\n\n\nThe above values are essentially the coordinates that form a rectangle, a rectangular area which the USA country is bounded within. We are using this as the full plot region for the raster to prevent parts of the image not showing.\n\n\nNow, let us visualise the predictions:\n\ntm_shape(exp.prediction_masked, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.reds\"), \n        col.legend = tm_legend(title = \"Predicted SO2 ppb\", frame = FALSE)) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nThe above shows the predicted concentrations of ambient \\(SO_{2}\\); however, the predicted surface is very smooth and it difficult to see the spatial patterns. One technique, which is sometimes useful, for raster data is to reclassify the pixels to ordered categories i.e., zones instead of pixel-point estimates.\nWe could reclassify the continuous values stored in the grids/pixels into discrete values using the following scheme:\n\n0 = \"&lt; 1.0 ppb\"\n1 = \"1.0-4.9 ppb\"\n2 = \"5.0-9.9 ppb\"\n3 = \"10.0-14.9 ppb\"\n4 = \"15.0-19.9 ppb\"\n5 = \"20.0-29.9 ppb\"\n6 = \"30.0-39.9 ppb\"\n7 = \"+40.0 ppb\"\n\nYou can do this by using the following code:\n\n# Create a vector for the reclassification -i.e., 1st row captures values \n# --- between 0 and below 1 to reclassify a pixel as 0\n# While the 2nd row in this vector captures values between 1 and below 5 to \n# --- reclassify a pixel as 1 and so on and so forth\nreclassifyRaster &lt;- c(0,1,0,\n1,5,1,\n5,10,2,\n10,15,3,\n15,20,4,\n20,30,5,\n30,40,6,\n40,70,7)\n\n# Then store the values into a matrix \nreclassifyRaster_Mat &lt;- matrix(reclassifyRaster, ncol=3, byrow=TRUE)\nreclassifyRaster_Mat\n\nNow, apply the matrix to the raster object to reclassify the pixels accordingly using the reclassify() function:\n\nexp.prediction_masked_rec &lt;- reclassify(exp.prediction_masked, reclassifyRaster_Mat)\n\nNow, lets visualise the prediction zones:\n\ntm_shape(exp.prediction_masked_rec, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_categorical(\n            values = \"brewer.reds\", \n            labels = c(\"&lt;1.0 ppbs\",\"1.0-4.9 ppb\",\"5.0-9.9 ppb\", \"10.0-14.9 ppb\" , \"15.0-19.9 ppb\", \"20.0-29.9 ppb\", \"30.0-39.9 ppb\",\"+40.0ppb\")),\n        col.legend = tm_legend(title = \"Predicted SO2 ppb\", frame = FALSE, position = c(\"right\", \"bottom\"))) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"top\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nYou can visualise the variance:\n\n# mask values of raster outside regions of US Border\nexp.variance_masked &lt;- mask(exp.variance, US_Nation_Border_sp_shp)\n\ntm_shape(exp.variance_masked, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.oranges\"), \n        col.legend = tm_legend(title = \"Variance for SO2 ppb\", frame = FALSE)) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nAgain, it will be better to reclassify the raster for the variance to see where the model predicts the SO\\(_2\\) with lower and high errors. We can use the reclassify() accordingly. Note that the lowest and highest estimated variance is 4.435438 and 57.59579, respectively. Let us reclassify the variance raster using the following scheme:\n\n0 = \"&lt; 5.0 ppb\"\n1 = \"5.0-9.9 ppb\"\n2 = \"10.0-19.9 ppb\"\n3 = \"20.0-29.9 ppb\"\n4 = \"30.0-39.9 ppb\"\n5 = \"40.0-49.9 ppb\"\n6 = \"+50.0 ppb\"\n\nHere is the code:\n\nreclassifyRaster_var &lt;- c(0,5,0,\n    5,10,1,\n    10,20,2,\n    20,30,3,\n    30,40,4,\n    40,50,5,\n    50,60,6)\n\nreclassifyRaster_Mat_var &lt;- matrix(reclassifyRaster_var, ncol=3, byrow=TRUE)\nreclassifyRaster_Mat_var\n\nexp.variance_masked_rec &lt;- reclassify(exp.variance_masked, reclassifyRaster_Mat_var)\n\ntm_shape(exp.variance_masked_rec, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_categorical(\n        values = \"brewer.oranges\", \n        labels = c(\"&lt;5.0 ppbs\",\"5.0-9.9 ppb\",\"10.0-19.9 ppb\", \"20.0-29.9 ppb\" , \"30.0-39.9 ppb\", \"40.0-49.9 ppb\",\"+50.0ppb\")),\n        col.legend = tm_legend(title = \"Variance for SO2 ppb\", frame = FALSE, position = c(\"right\", \"bottom\"))) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"top\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above example was a null model. We can include risk factors as adjustments for the prediction. The below code shows you how to incorporate other variables in the analysis. We will use the other raster data sets for urbanisation, deprivation and car usage as adjustments in the Kriging model.\n\n\n\n\n\n\nThe code is a repeat of the above but with regression covariate adjustment.\n\nlibrary(\"sf\")\nlibrary(\"sp\")\nlibrary(\"raster\")\nlibrary(\"tmap\")\nlibrary(\"gstat\")\nlibrary(\"geoR\")\n\n# refresh the memory by clear ALL objects in R\nrm(list = ls())\ngc()\n\n# load datasets\ndatafile &lt;- read.csv(file = \"US 2019 SO2 Emissions data.csv\", header=TRUE, sep=\",\")\nUS_Nation_Border_shp &lt;- st_read(\"US Nation Border.shp\")\nUS_State_Borders_shp &lt;- st_read(\"US State Borders.shp\")\n\n# Coerce the spreadsheet into a sf object\n# First tell R that it’s coordinates are currently in decimal degrees (i.e., WGS84 'crs = 4326') before the transformation\ndatafile_sf &lt;- st_as_sf(datafile, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n# Now apply the transformation from WGS84 to Mercator i.e., = 3857\ndatafile_sf_prj &lt;- st_transform(datafile_sf, 3857)\n# Inspect the details\nst_crs(datafile_sf_prj)\n# corece to sp object\ndatafile_sp_prj &lt;- as(datafile_sf_prj, \"Spatial\")\n\n# Step 1: Load rasters. These are the covariates we will use in the gstat() function for the kriging regression\nUS_urbanisation &lt;- raster(\"US Urbanisation Index 5km.tif\")\nUS_SVI &lt;- raster(\"US Socioeconomic Deprivation 5km.tif\")\nUS_CarUsage &lt;- raster(\"US Prevalence of Car Usage 5km.tif\")\n# ignore that stupid warning message as its related to outdated 'rgdal & rgeos'. All raster are in the CRS Mercator 3857\n\n# Step 2: Stack the three rasters together by using the stack() function \npredictors &lt;- stack(US_urbanisation, US_SVI, US_CarUsage)\n\n# Step 3: from the stacked data extract the raster values on the points of pollution stations using the extract() function\npredictors.values &lt;- extract(predictors, datafile_sp_prj)\n\n# Step 4: Stitch the extraction to the spatial point data frame using the cbind() \"column bind\" function\ndatafile_sp_prj@data &lt;- cbind(datafile_sp_prj@data, predictors.values)\n# assign proper names to columns\nnames(datafile_sp_prj@data)[3] &lt;- \"US_Urbanisation_Index_5km\"\nnames(datafile_sp_prj@data)[4] &lt;- \"US_Socioeconomic_Deprivation_5km\"\nnames(datafile_sp_prj@data)[5] &lt;- \"US_Prevalence_of_Car_Usage_5km\"\n\n# You can view the dataframe to see the raster values. What has happened is we extracted the overlapping raster pixels on\n# those pollution stations and assuming those are the levels of urbanisation, deprivation and car usage at those points for\n# which the SO2 levels are observed. \nView(datafile_sp_prj@data)\n\n# Step 5: Linear Regression model to determine which variables are worth to be included in the Kriging model. If the turn out\n# to be statistical significant (i.e., p &lt; 0.05). Then include to include in the kriging.\nlm.model &lt;- lm(Mean_SO2 ~ US_Urbanisation_Index_5km + US_Socioeconomic_Deprivation_5km + US_Prevalence_of_Car_Usage_5km, data = datafile_sp_prj)\nsummary(lm.model)\n\n# all variables are statistically significant. According to this model urbanisation and car usage marginally decreases SO2 levels,\n# while areas of higher deprivation yields higher levels of SO2. Include all variables to the Kriging model since they are\n# significant.\n\n# Step 6: use variogram() function to compute the semivariance with variable in the model\nSO2_adj_emp.variogram &lt;- variogram(Mean_SO2 ~ US_Urbanisation_Index_5km + US_Socioeconomic_Deprivation_5km + US_Prevalence_of_Car_Usage_5km, datafile_sp_prj)\nSO2_adj_emp.variogram\nplot(SO2_adj_emp.variogram)\n\n# Step 7: Determine best model\nbest_SO2_adj_emp.variogram &lt;- fit.variogram(SO2_adj_emp.variogram, model = vgm(c(\"Exp\", \"Gau\", \"Sph\")))\nbest_SO2_adj_emp.variogram\nplot(SO2_adj_emp.variogram, best_SO2_adj_emp.variogram, main = \"Best Model: Exponential (Nug: 5.63, PSill: 49.4, Range: 293891.2m)\")\n\n# HERE ARE THE CORRECTIONS\n# Step 8: Here is where we need to insert the raster values and not the points. In the previous iteration, I mistakenly used the\n# point verison of the car usage data and adapted the code without verifying it to work. I sincerely apologise for this oversight!\nmodelKrigingExp_adj &lt;- gstat(formula = Mean_SO2~US_Urbanisation_Index_5km + US_Socioeconomic_Deprivation_5km + US_Prevalence_of_Car_Usage_5km, locations = datafile_sp_prj, model = best_SO2_adj_emp.variogram)\n\n# Take the stacked rasters for car usage, urbanisation and socioeconomic deprivation and convert it to a grid template\n# Here, we need their values stored inside that grid template, to which we will apply the kriging model to make the prediction\n# while at the same time make adjustments for their value.\n\n# we created the stacked into the 'predictors' object\npredictors.grid &lt;- as(predictors, \"SpatialGridDataFrame\")\n\n# Last time it was just \"SpatialGrid\" because it was empty. This grid has values and thus not empty \n# so we call it \"SpatialGridDataFrame\"\n# rename the raster's names for consistency\n\nnames(predictors.grid) &lt;- c(\"US_Urbanisation_Index_5km\", \"US_Socioeconomic_Deprivation_5km\", \"US_Prevalence_of_Car_Usage_5km\")\nnames(predictors.grid)\n\n# Step 9: We can now do the predictions over the SpatialGridDataFrame\nKriged_SO2_adj &lt;- predict(modelKrigingExp_adj, predictors.grid)\n\n# Brick the layer in order to separate the estimated prediction and variance accordingly\nbrickedKriged_SO2_Results_adj &lt;- brick(Kriged_SO2_adj)\nexp.prediction_adj &lt;- raster(brickedKriged_SO2_Results_adj, layer = 1)\nexp.variance_adj &lt;- raster(brickedKriged_SO2_Results_adj, layer = 2)\n\n# Step 10: Masking the prediction and reclassifying the layer\n# You can mask the prediction with the outline of USA\nUS_Nation_Border_sp_shp &lt;- as(US_Nation_Border_shp, \"Spatial\")\nexp.prediction_adj_masked &lt;- mask(exp.prediction_adj, US_Nation_Border_sp_shp)\n\n# Run this to see minimum and maximum value.\nexp.prediction_adj_masked\n\n# You can see there are negative value as some of the prediction for SO2. \n# Lets reclassify these as an invalid prediction with value -1\nreclassifyRaster &lt;- c(-2,0,-1, \n    0,1,0,\n    1,5,1,\n    5,10,2,\n    10,15,3,\n    15,20,4,\n    20,30,5,\n    30,40,6,\n    40,70,7)\n\nreclassifyRaster_Mat &lt;- matrix(reclassifyRaster, ncol=3, byrow=TRUE)\nreclassifyRaster_Mat\n\n# Apply the categories to masked layer in order to reclassify the predictions that were adjusted those 3 variables\nexp.prediction_adj_masked_rec &lt;- reclassify(exp.prediction_adj_masked, reclassifyRaster_Mat)\n# Create the labels for the 'reclassifyRaster_Mat\" object\nSO2LevelsCategories &lt;- c(\"Invalid\",\"&lt;1.0 ppbs\",\"1.0-4.9 ppb\",\"5.0-9.9 ppb\", \"10.0-14.9 ppb\" , \"15.0-19.9 ppb\", \"20.0-29.9 ppb\", \"30.0-39.9 ppb\",\"+40.0ppb\")\n\n# Finally, we going to force the colour schemes we want. \n# We want the \"Invalid\" category to have a grey colour; and the rest for lowest category\n# ie., \"&lt;1.0ppbs\" to \"+40.0ppb\" to be increase red intensities\"\n\n# Force the colorbrewer schemes grey = #636363\n# Force the reds (from light red down to solid-dark-red) = #fee5d9, #fcbba1, #fc9272, #fb6a4a, #ef3b2c, #cb181d, #99000d\n\n# Create the colour scheme for the above 'SO2LevelsCategories'\nHackedColourPalette &lt;- c(\"#636363\", \"#fee5d9\", \"#fcbba1\", \"#fc9272\", \"#fb6a4a\", \"#ef3b2c\", \"#cb181d\", \"#a50f15\", \"#67000d\")\n\nframeExtent &lt;- st_bbox(US_Nation_Border_shp)\nframeExtent\n\n# Step 11: Visual the adjusted prediction from Universal Kriging Regression\ntm_shape(exp.prediction_adj_masked_rec, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_categorical(\n        values = HackedColourPalette, \n        labels =  SO2LevelsCategories),\ncol.legend = tm_legend(title = \"Predicted adjusted SO2 ppb\", frame = FALSE, position = c(\"right\", \"bottom\"))) +\n    tm_shape(US_State_Borders_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\n    tm_shape(datafile_sf_prj) + \n    tm_dots() + \n    tm_scalebar(position = c(\"left\",\"bottom\")) +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTechnical Document: [R Programming] Gurpreet Singh and Biju Soman, (2020), Spatial Interpolation using Kriging in R. Download here\nTechnical Document: [R Programming] Fernando Antonanzas-Torres, (2014), Geostatistics examples in R: Ordinary Kriging, Universal Kriging and Inverse Distance Weighted. Download here\nTechnical Document: [R Programming] Adela Volfova and Martin Smejkal, (2012), Geostatistical Methods in R. Download here\nBook: [R Programming] Roger S. Bivand, Edzer J. Pebesma and Virgilio Gomez-Rubio, (2008), Applied Spatial Data Analysis with R, Chapter 8: Interpolation and Geostatistics, pages 191 to 235.\nBook: [R Programming] Michael Dorman, (2014), Learning R for Geospatial Analysis, Chapter 8: Spatial Interpolation of Point Data, pages 241 to 279.\nBook: [Theory] Christopher D. Lloyd, (2010), Spatial Data Analysis: An Introduction for GIS Users, Chapter 9: Spatial Interpolation (Section 9.7. Ordinary Kriging), pages 140 to 150.\n\n\n\n\n\nThe pollution data was obtained from the United States Environmental Protection Agency (EPA) Click Here\nSpatial data concerning car usage in the US was sourced from the ACS Vehicle Availability Variables project Click Here. You would need to have Online ArcGIS account to access the resources.\nUS raster data for Social Vulnerability Index 2018 and Urbanization Index (2015) were sourced from the NASA Socioeconomic Data & Applications Center (SEDAC) Click Here. NOTE: Registration required for free access to raster records.\nThe Global Atlas for Helminths Infections (GAHI) Click Here\nExpanded Special Project for Elimination of Neglected Tropical Diseases (ESPEN-NTD) Click Here\n\n\n\n\nCase study: Village survey of hookworm infection in Uganda and Tanzania\nWe will use data on the prevalence of hookworm collected from 393 villages in Uganda and Tanzania. The dataset includes coordinates for each village, which have been geo-referenced to a specific longitude and latitude, as well as the boundaries of the study area for the two countries:\n\nPoint data: Soil_Transmitted_Helminth_Data.csv\nShapefile of region: Study_Area_UG_TZ.shp\nShapefile of region with districts: Study_Area_UG_TZ_Districts.shp\n\nThese data can be downloaded [HERE]\nAs shown in the practical, this analysis involves going through five stages:\n\nData preparation and checking the distribution of points within the study area\nVariogram analysis for generating an empirical variogram\nVariogram analysis for generating a theoretical variogram to determine the appropriate model\nGeostatistical prediction (Ordinary Kriging [null model])\nThematic mapping and visualisation.\n\nUse Kriging to generate a predicted prevalence map of Hookworm infection for the entire study area in Uganda and Tanzania. The World Health Organisation has a classification for grading disease burden of this type of illness in the Global South:\n\nAreas with prevalence of &lt; 1% are deemed as “Negligible”\nAreas with a prevalence from 1% to 10% are deemed “Low risk”\nAreas with a prevalence from 10% to 20% are deemed “Moderate risk”\nAreas with a prevalence from 20% to 50% are deemed “High risk”\nAreas with a prevalence from 50% and more are zones where risks are “Excessive”\n\nProduce an output that reflect the WHO’s classification for this disease.\n\n\n\n\n\n\nImportant\n\n\n\nHave a go at these questions before using solution codes which can be downloaded from [HERE]",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#learning-objectives",
    "href": "08-geostatistics_kriging.html#learning-objectives",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "The US Environmental Agency have positioned air quality monitors for surveillance of over 100 different types of pollutants that exist as toxic gases, particulates and heavy metals. For Sulphur Dioxide (\\(SO_{2}\\)), there are 458 active air monitors that takes hourly readings for concentrations of \\(SO_{2}\\) (in parts per billion (pbb)). An annual estimate for \\(SO_{2}\\) was calculated for each station at its location. Car usage, urbanisation and social economic deprivation, alongside of other anthropogenic activities such as coal burning, across the USA increases the risk of elevated pollution of \\(SO_{2}\\).\nUsing geostatistical methods and taking into account of car usage, urbanisation and levels of deprivation - what areas in USA have higher concentrations of \\(SO_{2}\\) exceeding the annual average of 40 ppb which is a national safety limit for cause of concern?\nLet’s use Kriging to find out!",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#datasets-setting-up-the-work-directory",
    "href": "08-geostatistics_kriging.html#datasets-setting-up-the-work-directory",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Before you begin do make sure to download all data by clicking here. Create a folder on called “Week 6” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 6” folder. Open a new R script and set the work directory to Week 6’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 6\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 6\")",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#loading-and-installing-packages",
    "href": "08-geostatistics_kriging.html#loading-and-installing-packages",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "We will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nraster: Raster/gridded data analysis and manipulation\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\n\n# Load packages using library() function\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"raster\")\nlibrary(\"sp\")\n\nThe above packages sf, tmap, raster & sp should have been installed in the previous session(s). We will need to install the following package:\n\ngstat: provides functions for univariable and multivariable geostatistical analysis.\ngeoR: provides additional functions for geostatistical and variogram analysis.\n\n\n# Install the packages: gstat using the install.package()\ninstall.packages(\"gstat\")\ninstall.packages(\"geoR\")\n\n# Load the packages with library()\nlibrary(\"gstat\")\nlibrary(\"geoR\")",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#loading-datasets",
    "href": "08-geostatistics_kriging.html#loading-datasets",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Let us first import the quantitative data i.e., US 2019 SO2 Emissions data.csv into R/RStudio.\n\n# Use read.csv() to import \ndatafile &lt;- read.csv(file = \"US 2019 SO2 Emissions data.csv\", header = TRUE, sep = \",\")\n\nNOTE: The description of the column names are as follows:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nCountyRef\nThe County & State for where the air quality monitors are located in US\n\n\nLongitude\nLongitude (in decimal degrees)\n\n\nLatitude\nLatitude (in decimal degrees)\n\n\nMean_SO2\nAnnual Mean (ppb) concentrations of Ambient sulphur dioxide (\\(SO_{2}\\)) in 2019\n\n\n\n\n\n\n\n\nShape file: US National border named US Nation Border.shp\nShape file: US State border named US State Borders.shp\nRaster: US Car Usage (5000m resolution) named US Prevalence of Car Usage 5km.tif\nRaster: US Urbanisation Index (5000m resolution) named US Urbanisation Index 5km.tif\nRaster: US Socioeconomic Deprivation (5000m resolution) named US Socioeconomic Deprivation 5km.tif\n\n\n# Use read_sf() function to load shape file \nUS_Nation_Border_shp &lt;- st_read(\"US Nation Border.shp\")\nUS_State_Border_shp &lt;- st_read(\"US State Borders.shp\")",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#data-preparation",
    "href": "08-geostatistics_kriging.html#data-preparation",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "There are a couple of things we need to do before proceeding with the analysis:\n\nThe datafile is a data frame object in RStudio’s memory, and not a spatial object. We need to coerce into a spatial sf object\nThe shapefiles for US Nation Border.shp & US State Borders.shp are in a different CRS called Spherical Mercator 3857 which measures distance in meters and not in decimal degrees. We need to transform the longitude and latitude of our stations which are in decimal degrees to the CRS of Spherical Mercator 3857\n\n\n# Coerce the spreadsheet into a sf object\n# First tell R that it’s coordinates are currently in decimal degrees (i.e., WGS84 'crs = 4326') before the transformation\ndatafile_sf &lt;- st_as_sf(datafile, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n# Now apply the transformation from WGS84 to Mercator i.e., = 3857\ndatafile_sf_prj &lt;- st_transform(datafile_sf, 3857)\n# Inspect the details\nst_crs(datafile_sf_prj)\n\nThe code chunk below generates an empty map with the tmap functions. It shows just the border of USA and the point locations for the air quality monitoring stations superimposed.\n\ntm_shape(US_Nation_Border_shp) + \n    tm_polygons(fill = \"white\", col = \"black\") + \ntm_shape(datafile_sf_prj) + \n    tm_dots(fill = \"black\") + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"bottom\"))",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#semivariogram-analysis",
    "href": "08-geostatistics_kriging.html#semivariogram-analysis",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Semivariograms describe how data are related with distance by plotting the semivariance against the separation distance, known as the experimental or empirical semivariogram. The semivariance is defined as half the average squared difference between points separated by some distance h. As the separation distance h between samples increase, we would expect the semivariance to also increase (again, because near samples are more similar than distant samples).\n\n\n\n\n\n\n\n\n\nIn the generic semivariogram shown above, there are three important parameters:\n\nSill: The maximum semivariance value observed, and it indicates the threshold for values beyond (i.e., flatline) which there is no spatial autocorrelation. NOTE: the Partial Sill is a value calculated by taking the difference between the Sill and Nugget (i.e., Partial Sill = Sill - Nugget)\nRange: The maximum separation distance h at which we will expect to find evidence of spatial autocorrelation. A separation distance beyond the range samples are no longer correlated.\nNugget: This describes the variance of the measurement error combined with spatially uncorrelated variations at distances shorter than the sample spacing, namely noise in the data. The larger the nugget relative to the sill, the less spatial dependence there is in the data and less useful Kriging will be.\n\n\n\n\n\n\n\nNote\n\n\n\nTwo important assumptions of a basic semivariogram are that the spatial process under investigation are: i.) stationary, i.e., the spatial autocorrelation between the measurements of same variables in a given area is the same for all locations; and ii.) isotropic, spatial autocorrelation is the same in every direction. If the autocorrelation differs by direction, it is termed as anisotropic.\n\n\nTo be used in Kriging, a semivariogram plot (akin to the above image) must be generated to estimate the 3 parameters (i.e., sill, nugget & range) from the points termed experimental or empirical semivariogram. These are used as initial values to fit a modeled or theoretical semivariogram which can be in one of three major forms:\n\nGaussian Model (Left)\nSpherical Model (Center)\nExponential Model (Right)\n\n\n\n\n\n\n\n\n\n\nOnce the modelled semivariogram has been defined, it can be used in Kriging.\n\n\nUse the function variogram() to create the object for plotting the empirical variogram\n\n# coerce datafile_sf_prj to be a 'sp' spatial dataframe object as it's \n# ---variogram does not use 'sf' objects\n\n# ignore warning message\n\ndatafile_sp_prj &lt;- as(datafile_sf_prj, \"Spatial\")\n# use variogram() function to compute the semivariance with a null model Mean_SO2 as outcome\nSO2_emp.variogram &lt;- variogram(Mean_SO2~1, datafile_sp_prj)\n# Compute the object to reveal a table\nSO2_emp.variogram\n\n\n\n\n\n\n\nNote\n\n\n\nnp in the output is the number of paired considered within the separation distance dist; gamma is the averaged semivariance for the number of paired points within the separation distance dist.\n\n\nLet us plot these values to see the empirical semivariogram\n\nplot(SO2_emp.variogram)\n\n\n\n\n\n\n\n\n\n\nFrom the output (i.e., plot and table), we should note the approximate values for the partial sill, nugget and range.\n\nThe nugget is roughly 17 (i.e. base on starting gamma value from the table).\nThe range is roughly 1180000 meters (i.e. base on peak value for gamma and it’s corresponding dist).\nThe partial sill is 65. This is derived from the peak value for gamma subtracted by the nugget (82 - 17 = 65).\n\nThese initial values give us an idea of what to expect when we proceed to fit a theoretical semivariogram using the fit.variogram(). It will help us to generate the fitted models.\n\n\n\nWe are going to fit a model to the empirical semivariogram in order to determine the appropriate function for Kriging (i.e., spherical (Sph), exponential (Exp) or gaussian (Gau)).\nWe will start fitting the various models:\n\n# Fit exponential\nexp_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(65, \"Exp\", 1180000, 17))\nexp_SO2_emp.variogram\nplot(SO2_emp.variogram, exp_SO2_emp.variogram, main  = \"Exponential model (Nug: 3.6, PSill: 55.9, Range: 296255m)\")\n\n\n\n\n\n\n\n\n\n\n\n# Fit Spherical\nsph_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(65, \"Sph\", 1180000, 17))\nsph_SO2_emp.variogram\nplot(SO2_emp.variogram, sph_SO2_emp.variogram, main  = \"Spherical model (Nug: 10.5, PSill: 49.6, Range: 857452m)\")\n\n\n\n\n\n\n\n\n\n\n\n# Fit gaussian\ngau_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(65, \"Gau\", 1180000, 17))\ngau_SO2_emp.variogram\nplot(SO2_emp.variogram, gau_SO2_emp.variogram, main  = \"Gaussian model (Nug: 12.8, PSill: 39.1, Range: 244807m)\")\n\n\n\n\n\n\n\n\n\n\nBy eyeballing the images - it difficult to discern whether the exponential or spherical model provides a better fit to the empirical semivariogram. We can use the fit.variogram() function to determine which is the best model amongst them.\n\n# select the best model\nbest_SO2_emp.variogram &lt;- fit.variogram(SO2_emp.variogram, model = vgm(c(\"Exp\", \"Gau\", \"Sph\")))\nbest_SO2_emp.variogram\n\nFrom the output (see column which says model and row 2 it highlights \"Exp\"), it shows that the exponential model is the best fit with a nugget = 3.6, Partial Sill =  55.9 and Range = 296255m. We therefore select the exponential model in our Kriging to make the spatial prediction for \\(SO_{2}\\). Lets proceed to Krige.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIMPORTANT NOTES: The interpretation is as follows: the nugget in the exponential model is smaller than the other proposed models. It is small which is also an indication of evidence of larger spatial dependence in the concentrations for \\(SO_{2}\\) across sampling sites in USA; A separation distance with values beyond 296255m (where it curve starts to plateau) and beyond the semivariance’s threshold where it flat lines (sill of 59.5 (i.e., 55.9 + 3.6)) - there should expect that the spatial autocorrelation in the observed levels of \\(SO_{2}\\) are not present anymore.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#kriging-modelling-null",
    "href": "08-geostatistics_kriging.html#kriging-modelling-null",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Let us create a template raster for interpolation. The extent of the raster template should based on the points. We are going to make the resolution of grid be at 5000m by 5000m (5km by 5km) accordingly.\n\nRasterTemplate &lt;- raster(datafile_sp_prj)\nres(RasterTemplate) &lt;- 5000\n\nNext, declare template as a spatial grid\n\ngrid.interpolation &lt;- as(RasterTemplate, 'SpatialGrid')\n\n\n\n\nNow, we are going to use the parameters from the exponential theoretical semivariogram model to interpolate the concentrations of ambient SO2 for the entire study region\n\nmodelKrigingExp &lt;- gstat(formula = Mean_SO2~1, locations = datafile_sp_prj, model = exp_SO2_emp.variogram)\n\nThe results are stored in modelKrigingExp object. Lets add the results of the interpolation to our grid template using the predict() function\n\n# this may take roughly 5mins\nKriged_SO2 &lt;- predict(modelKrigingExp, grid.interpolation)\n\nThe above analysis produces two separate rasters: i.) Predicted \\(SO_{2}\\) and ii.) Variation in \\(SO_{2}\\). Let export the results and make some visualisation using the tmap functions\n\n\n\nNow, save both the prediction and variance a multi-layer raster (a ‘brick’ raster):\n\nbrickedKriged_SO2_Results &lt;- brick(Kriged_SO2)\n\nWe can save them individually from the multi-layer raster as follows:\n\n# Separate the rasters accordingly\nexp.prediction &lt;- raster(brickedKriged_SO2_Results, layer = 1)\nexp.variance &lt;- raster(brickedKriged_SO2_Results, layer = 2)\n#  save the output locally on your computer\nwriteRaster(exp.prediction, \"Predicted SO2 levels in USA.tif\", format=\"GTiff\", overwrite = TRUE)\nwriteRaster(exp.variance, \"Variance SO2 levels in USA.tif\", format=\"GTiff\", overwrite = TRUE)\n\n\n\n\nWe will need to perform a couple of steps before visualisation. First, we will need to mask the values of the raster predictions were made outside of US Border’s region. To do this, we use the mask() function:\n\n# mask values of raster outside regions of US Border\nUS_Nation_Border_sp_shp &lt;- as(US_Nation_Border_shp, \"Spatial\")\nexp.prediction_masked &lt;- mask(exp.prediction, US_Nation_Border_shp)\n\nNext, we are going to make the raster image sit perfectly sit within the plot’s frame using the country’s bounding box or extent. We can extract the bounding box by using st_bbox() function on the US_Nation_Border_shp shape file object, this basically gives us the extent of the region.\n\nframeExtent &lt;- st_bbox(US_Nation_Border_shp)\nframeExtent\n\n\n&gt; frameExtent\n     xmin      ymin      xmax      ymax \n-13885235   2819925  -7452828   6340334\n\n\n\n\n\n\n\nNote\n\n\n\nThe above values are essentially the coordinates that form a rectangle, a rectangular area which the USA country is bounded within. We are using this as the full plot region for the raster to prevent parts of the image not showing.\n\n\nNow, let us visualise the predictions:\n\ntm_shape(exp.prediction_masked, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.reds\"), \n        col.legend = tm_legend(title = \"Predicted SO2 ppb\", frame = FALSE)) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nThe above shows the predicted concentrations of ambient \\(SO_{2}\\); however, the predicted surface is very smooth and it difficult to see the spatial patterns. One technique, which is sometimes useful, for raster data is to reclassify the pixels to ordered categories i.e., zones instead of pixel-point estimates.\nWe could reclassify the continuous values stored in the grids/pixels into discrete values using the following scheme:\n\n0 = \"&lt; 1.0 ppb\"\n1 = \"1.0-4.9 ppb\"\n2 = \"5.0-9.9 ppb\"\n3 = \"10.0-14.9 ppb\"\n4 = \"15.0-19.9 ppb\"\n5 = \"20.0-29.9 ppb\"\n6 = \"30.0-39.9 ppb\"\n7 = \"+40.0 ppb\"\n\nYou can do this by using the following code:\n\n# Create a vector for the reclassification -i.e., 1st row captures values \n# --- between 0 and below 1 to reclassify a pixel as 0\n# While the 2nd row in this vector captures values between 1 and below 5 to \n# --- reclassify a pixel as 1 and so on and so forth\nreclassifyRaster &lt;- c(0,1,0,\n1,5,1,\n5,10,2,\n10,15,3,\n15,20,4,\n20,30,5,\n30,40,6,\n40,70,7)\n\n# Then store the values into a matrix \nreclassifyRaster_Mat &lt;- matrix(reclassifyRaster, ncol=3, byrow=TRUE)\nreclassifyRaster_Mat\n\nNow, apply the matrix to the raster object to reclassify the pixels accordingly using the reclassify() function:\n\nexp.prediction_masked_rec &lt;- reclassify(exp.prediction_masked, reclassifyRaster_Mat)\n\nNow, lets visualise the prediction zones:\n\ntm_shape(exp.prediction_masked_rec, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_categorical(\n            values = \"brewer.reds\", \n            labels = c(\"&lt;1.0 ppbs\",\"1.0-4.9 ppb\",\"5.0-9.9 ppb\", \"10.0-14.9 ppb\" , \"15.0-19.9 ppb\", \"20.0-29.9 ppb\", \"30.0-39.9 ppb\",\"+40.0ppb\")),\n        col.legend = tm_legend(title = \"Predicted SO2 ppb\", frame = FALSE, position = c(\"right\", \"bottom\"))) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"top\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nYou can visualise the variance:\n\n# mask values of raster outside regions of US Border\nexp.variance_masked &lt;- mask(exp.variance, US_Nation_Border_sp_shp)\n\ntm_shape(exp.variance_masked, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.oranges\"), \n        col.legend = tm_legend(title = \"Variance for SO2 ppb\", frame = FALSE)) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"bottom\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nAgain, it will be better to reclassify the raster for the variance to see where the model predicts the SO\\(_2\\) with lower and high errors. We can use the reclassify() accordingly. Note that the lowest and highest estimated variance is 4.435438 and 57.59579, respectively. Let us reclassify the variance raster using the following scheme:\n\n0 = \"&lt; 5.0 ppb\"\n1 = \"5.0-9.9 ppb\"\n2 = \"10.0-19.9 ppb\"\n3 = \"20.0-29.9 ppb\"\n4 = \"30.0-39.9 ppb\"\n5 = \"40.0-49.9 ppb\"\n6 = \"+50.0 ppb\"\n\nHere is the code:\n\nreclassifyRaster_var &lt;- c(0,5,0,\n    5,10,1,\n    10,20,2,\n    20,30,3,\n    30,40,4,\n    40,50,5,\n    50,60,6)\n\nreclassifyRaster_Mat_var &lt;- matrix(reclassifyRaster_var, ncol=3, byrow=TRUE)\nreclassifyRaster_Mat_var\n\nexp.variance_masked_rec &lt;- reclassify(exp.variance_masked, reclassifyRaster_Mat_var)\n\ntm_shape(exp.variance_masked_rec, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_categorical(\n        values = \"brewer.oranges\", \n        labels = c(\"&lt;5.0 ppbs\",\"5.0-9.9 ppb\",\"10.0-19.9 ppb\", \"20.0-29.9 ppb\" , \"30.0-39.9 ppb\", \"40.0-49.9 ppb\",\"+50.0ppb\")),\n        col.legend = tm_legend(title = \"Variance for SO2 ppb\", frame = FALSE, position = c(\"right\", \"bottom\"))) +\ntm_shape(US_State_Border_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\ntm_shape(datafile_sf_prj) + \n    tm_dots() + \ntm_scalebar(position = c(\"left\",\"bottom\")) +\ntm_compass(position = c(\"right\", \"top\")) +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above example was a null model. We can include risk factors as adjustments for the prediction. The below code shows you how to incorporate other variables in the analysis. We will use the other raster data sets for urbanisation, deprivation and car usage as adjustments in the Kriging model.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#demonstrations-of-kriging-regression-model",
    "href": "08-geostatistics_kriging.html#demonstrations-of-kriging-regression-model",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "The code is a repeat of the above but with regression covariate adjustment.\n\nlibrary(\"sf\")\nlibrary(\"sp\")\nlibrary(\"raster\")\nlibrary(\"tmap\")\nlibrary(\"gstat\")\nlibrary(\"geoR\")\n\n# refresh the memory by clear ALL objects in R\nrm(list = ls())\ngc()\n\n# load datasets\ndatafile &lt;- read.csv(file = \"US 2019 SO2 Emissions data.csv\", header=TRUE, sep=\",\")\nUS_Nation_Border_shp &lt;- st_read(\"US Nation Border.shp\")\nUS_State_Borders_shp &lt;- st_read(\"US State Borders.shp\")\n\n# Coerce the spreadsheet into a sf object\n# First tell R that it’s coordinates are currently in decimal degrees (i.e., WGS84 'crs = 4326') before the transformation\ndatafile_sf &lt;- st_as_sf(datafile, coords = c(\"Longitude\", \"Latitude\"), crs = 4326)\n# Now apply the transformation from WGS84 to Mercator i.e., = 3857\ndatafile_sf_prj &lt;- st_transform(datafile_sf, 3857)\n# Inspect the details\nst_crs(datafile_sf_prj)\n# corece to sp object\ndatafile_sp_prj &lt;- as(datafile_sf_prj, \"Spatial\")\n\n# Step 1: Load rasters. These are the covariates we will use in the gstat() function for the kriging regression\nUS_urbanisation &lt;- raster(\"US Urbanisation Index 5km.tif\")\nUS_SVI &lt;- raster(\"US Socioeconomic Deprivation 5km.tif\")\nUS_CarUsage &lt;- raster(\"US Prevalence of Car Usage 5km.tif\")\n# ignore that stupid warning message as its related to outdated 'rgdal & rgeos'. All raster are in the CRS Mercator 3857\n\n# Step 2: Stack the three rasters together by using the stack() function \npredictors &lt;- stack(US_urbanisation, US_SVI, US_CarUsage)\n\n# Step 3: from the stacked data extract the raster values on the points of pollution stations using the extract() function\npredictors.values &lt;- extract(predictors, datafile_sp_prj)\n\n# Step 4: Stitch the extraction to the spatial point data frame using the cbind() \"column bind\" function\ndatafile_sp_prj@data &lt;- cbind(datafile_sp_prj@data, predictors.values)\n# assign proper names to columns\nnames(datafile_sp_prj@data)[3] &lt;- \"US_Urbanisation_Index_5km\"\nnames(datafile_sp_prj@data)[4] &lt;- \"US_Socioeconomic_Deprivation_5km\"\nnames(datafile_sp_prj@data)[5] &lt;- \"US_Prevalence_of_Car_Usage_5km\"\n\n# You can view the dataframe to see the raster values. What has happened is we extracted the overlapping raster pixels on\n# those pollution stations and assuming those are the levels of urbanisation, deprivation and car usage at those points for\n# which the SO2 levels are observed. \nView(datafile_sp_prj@data)\n\n# Step 5: Linear Regression model to determine which variables are worth to be included in the Kriging model. If the turn out\n# to be statistical significant (i.e., p &lt; 0.05). Then include to include in the kriging.\nlm.model &lt;- lm(Mean_SO2 ~ US_Urbanisation_Index_5km + US_Socioeconomic_Deprivation_5km + US_Prevalence_of_Car_Usage_5km, data = datafile_sp_prj)\nsummary(lm.model)\n\n# all variables are statistically significant. According to this model urbanisation and car usage marginally decreases SO2 levels,\n# while areas of higher deprivation yields higher levels of SO2. Include all variables to the Kriging model since they are\n# significant.\n\n# Step 6: use variogram() function to compute the semivariance with variable in the model\nSO2_adj_emp.variogram &lt;- variogram(Mean_SO2 ~ US_Urbanisation_Index_5km + US_Socioeconomic_Deprivation_5km + US_Prevalence_of_Car_Usage_5km, datafile_sp_prj)\nSO2_adj_emp.variogram\nplot(SO2_adj_emp.variogram)\n\n# Step 7: Determine best model\nbest_SO2_adj_emp.variogram &lt;- fit.variogram(SO2_adj_emp.variogram, model = vgm(c(\"Exp\", \"Gau\", \"Sph\")))\nbest_SO2_adj_emp.variogram\nplot(SO2_adj_emp.variogram, best_SO2_adj_emp.variogram, main = \"Best Model: Exponential (Nug: 5.63, PSill: 49.4, Range: 293891.2m)\")\n\n# HERE ARE THE CORRECTIONS\n# Step 8: Here is where we need to insert the raster values and not the points. In the previous iteration, I mistakenly used the\n# point verison of the car usage data and adapted the code without verifying it to work. I sincerely apologise for this oversight!\nmodelKrigingExp_adj &lt;- gstat(formula = Mean_SO2~US_Urbanisation_Index_5km + US_Socioeconomic_Deprivation_5km + US_Prevalence_of_Car_Usage_5km, locations = datafile_sp_prj, model = best_SO2_adj_emp.variogram)\n\n# Take the stacked rasters for car usage, urbanisation and socioeconomic deprivation and convert it to a grid template\n# Here, we need their values stored inside that grid template, to which we will apply the kriging model to make the prediction\n# while at the same time make adjustments for their value.\n\n# we created the stacked into the 'predictors' object\npredictors.grid &lt;- as(predictors, \"SpatialGridDataFrame\")\n\n# Last time it was just \"SpatialGrid\" because it was empty. This grid has values and thus not empty \n# so we call it \"SpatialGridDataFrame\"\n# rename the raster's names for consistency\n\nnames(predictors.grid) &lt;- c(\"US_Urbanisation_Index_5km\", \"US_Socioeconomic_Deprivation_5km\", \"US_Prevalence_of_Car_Usage_5km\")\nnames(predictors.grid)\n\n# Step 9: We can now do the predictions over the SpatialGridDataFrame\nKriged_SO2_adj &lt;- predict(modelKrigingExp_adj, predictors.grid)\n\n# Brick the layer in order to separate the estimated prediction and variance accordingly\nbrickedKriged_SO2_Results_adj &lt;- brick(Kriged_SO2_adj)\nexp.prediction_adj &lt;- raster(brickedKriged_SO2_Results_adj, layer = 1)\nexp.variance_adj &lt;- raster(brickedKriged_SO2_Results_adj, layer = 2)\n\n# Step 10: Masking the prediction and reclassifying the layer\n# You can mask the prediction with the outline of USA\nUS_Nation_Border_sp_shp &lt;- as(US_Nation_Border_shp, \"Spatial\")\nexp.prediction_adj_masked &lt;- mask(exp.prediction_adj, US_Nation_Border_sp_shp)\n\n# Run this to see minimum and maximum value.\nexp.prediction_adj_masked\n\n# You can see there are negative value as some of the prediction for SO2. \n# Lets reclassify these as an invalid prediction with value -1\nreclassifyRaster &lt;- c(-2,0,-1, \n    0,1,0,\n    1,5,1,\n    5,10,2,\n    10,15,3,\n    15,20,4,\n    20,30,5,\n    30,40,6,\n    40,70,7)\n\nreclassifyRaster_Mat &lt;- matrix(reclassifyRaster, ncol=3, byrow=TRUE)\nreclassifyRaster_Mat\n\n# Apply the categories to masked layer in order to reclassify the predictions that were adjusted those 3 variables\nexp.prediction_adj_masked_rec &lt;- reclassify(exp.prediction_adj_masked, reclassifyRaster_Mat)\n# Create the labels for the 'reclassifyRaster_Mat\" object\nSO2LevelsCategories &lt;- c(\"Invalid\",\"&lt;1.0 ppbs\",\"1.0-4.9 ppb\",\"5.0-9.9 ppb\", \"10.0-14.9 ppb\" , \"15.0-19.9 ppb\", \"20.0-29.9 ppb\", \"30.0-39.9 ppb\",\"+40.0ppb\")\n\n# Finally, we going to force the colour schemes we want. \n# We want the \"Invalid\" category to have a grey colour; and the rest for lowest category\n# ie., \"&lt;1.0ppbs\" to \"+40.0ppb\" to be increase red intensities\"\n\n# Force the colorbrewer schemes grey = #636363\n# Force the reds (from light red down to solid-dark-red) = #fee5d9, #fcbba1, #fc9272, #fb6a4a, #ef3b2c, #cb181d, #99000d\n\n# Create the colour scheme for the above 'SO2LevelsCategories'\nHackedColourPalette &lt;- c(\"#636363\", \"#fee5d9\", \"#fcbba1\", \"#fc9272\", \"#fb6a4a\", \"#ef3b2c\", \"#cb181d\", \"#a50f15\", \"#67000d\")\n\nframeExtent &lt;- st_bbox(US_Nation_Border_shp)\nframeExtent\n\n# Step 11: Visual the adjusted prediction from Universal Kriging Regression\ntm_shape(exp.prediction_adj_masked_rec, bbox = frameExtent) + \n    tm_raster(col.scale = tm_scale_categorical(\n        values = HackedColourPalette, \n        labels =  SO2LevelsCategories),\ncol.legend = tm_legend(title = \"Predicted adjusted SO2 ppb\", frame = FALSE, position = c(\"right\", \"bottom\"))) +\n    tm_shape(US_State_Borders_shp) + \n    tm_polygons(fill_alpha = 0, col = \"black\") + tm_text(\"STUSPS\", size = \"AREA\") +\n    tm_shape(datafile_sf_prj) + \n    tm_dots() + \n    tm_scalebar(position = c(\"left\",\"bottom\")) +\n    tm_compass(position = c(\"right\", \"top\")) +\n    tm_layout(frame = FALSE)",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#references-see-reading-list",
    "href": "08-geostatistics_kriging.html#references-see-reading-list",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Technical Document: [R Programming] Gurpreet Singh and Biju Soman, (2020), Spatial Interpolation using Kriging in R. Download here\nTechnical Document: [R Programming] Fernando Antonanzas-Torres, (2014), Geostatistics examples in R: Ordinary Kriging, Universal Kriging and Inverse Distance Weighted. Download here\nTechnical Document: [R Programming] Adela Volfova and Martin Smejkal, (2012), Geostatistical Methods in R. Download here\nBook: [R Programming] Roger S. Bivand, Edzer J. Pebesma and Virgilio Gomez-Rubio, (2008), Applied Spatial Data Analysis with R, Chapter 8: Interpolation and Geostatistics, pages 191 to 235.\nBook: [R Programming] Michael Dorman, (2014), Learning R for Geospatial Analysis, Chapter 8: Spatial Interpolation of Point Data, pages 241 to 279.\nBook: [Theory] Christopher D. Lloyd, (2010), Spatial Data Analysis: An Introduction for GIS Users, Chapter 9: Spatial Interpolation (Section 9.7. Ordinary Kriging), pages 140 to 150.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#data-sources",
    "href": "08-geostatistics_kriging.html#data-sources",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "The pollution data was obtained from the United States Environmental Protection Agency (EPA) Click Here\nSpatial data concerning car usage in the US was sourced from the ACS Vehicle Availability Variables project Click Here. You would need to have Online ArcGIS account to access the resources.\nUS raster data for Social Vulnerability Index 2018 and Urbanization Index (2015) were sourced from the NASA Socioeconomic Data & Applications Center (SEDAC) Click Here. NOTE: Registration required for free access to raster records.\nThe Global Atlas for Helminths Infections (GAHI) Click Here\nExpanded Special Project for Elimination of Neglected Tropical Diseases (ESPEN-NTD) Click Here",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "08-geostatistics_kriging.html#exercise",
    "href": "08-geostatistics_kriging.html#exercise",
    "title": "Week 6: Geostatistics using Kriging",
    "section": "",
    "text": "Case study: Village survey of hookworm infection in Uganda and Tanzania\nWe will use data on the prevalence of hookworm collected from 393 villages in Uganda and Tanzania. The dataset includes coordinates for each village, which have been geo-referenced to a specific longitude and latitude, as well as the boundaries of the study area for the two countries:\n\nPoint data: Soil_Transmitted_Helminth_Data.csv\nShapefile of region: Study_Area_UG_TZ.shp\nShapefile of region with districts: Study_Area_UG_TZ_Districts.shp\n\nThese data can be downloaded [HERE]\nAs shown in the practical, this analysis involves going through five stages:\n\nData preparation and checking the distribution of points within the study area\nVariogram analysis for generating an empirical variogram\nVariogram analysis for generating a theoretical variogram to determine the appropriate model\nGeostatistical prediction (Ordinary Kriging [null model])\nThematic mapping and visualisation.\n\nUse Kriging to generate a predicted prevalence map of Hookworm infection for the entire study area in Uganda and Tanzania. The World Health Organisation has a classification for grading disease burden of this type of illness in the Global South:\n\nAreas with prevalence of &lt; 1% are deemed as “Negligible”\nAreas with a prevalence from 1% to 10% are deemed “Low risk”\nAreas with a prevalence from 10% to 20% are deemed “Moderate risk”\nAreas with a prevalence from 20% to 50% are deemed “High risk”\nAreas with a prevalence from 50% and more are zones where risks are “Excessive”\n\nProduce an output that reflect the WHO’s classification for this disease.\n\n\n\n\n\n\nImportant\n\n\n\nHave a go at these questions before using solution codes which can be downloaded from [HERE]",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 6: Geostatistics using Kriging"
    ]
  },
  {
    "objectID": "06-suitability_mapping_AHP.html",
    "href": "06-suitability_mapping_AHP.html",
    "title": "Week 4: Multi-Decision Criteria Analysis",
    "section": "",
    "text": "Now, with the proliferation of open spatial dataset, risk models derived from environmental, sociodemographic and topological factors are becoming increasingly available for open research. Such models have broadly shown to be useful in delineating geographical areas of risk or suitability for a certain outcomes. Today, we are going to explore this qualitatively using a knowledge-driven approach.\n\n\nTo provide an introductory overview to the applicability of knowledge-driven methods, in particular, we are going to learn the Multi-Criteria Decision Approach (MCDA) which is a method that use decision rules from existing knowledge to identify areas potential suitability for an outcome. It is especially useful in data-sparse situations, or when for the first time exploring the potential geographical limits of certain outcome.\nFor instance, using modest number of raster layers such as population density, urbanisation, approximating to street segments, house prices and deprivation; it is possible to combine such information so to determine regions for which crime events such as burglaries are likely to occur, or suitable in that matter. This approach has been widely used in a number of disciplines over the last decades, and has gained prominence in public health related fields such as vector-borne disease prevention, and disaster sciences such as landslides. We will learn how to apply these methods to the two context.\n\n\n\nBefore you begin do make sure to download all data by clicking here. Create a folder on called “Week 4” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 4” folder. Open a new R script and set the work directory to Week 4’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 4\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 4\")\n\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\nThe above packages sf, tmap & sp should have been installed in the previous session(s). We will need to install a new package:\n\nraster: Raster/gridded data analysis and manipulation.\nspatialEco: Spatial Analysis and Modelling Utilities package, which provides the user further functions for dealing with raster data.\n\n\ninstall.packages(\"spatialEco\")\ninstall.packages(\"raster\")\n\n# Load the packages with library()\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"raster\")\nlibrary(\"sp\")\nlibrary(\"spatialEco\")\n\n\n\n\nWe will be dealing with raster data only for this exercise. These will be a series of climate, environmental and other social-anthropogenic gridded data from Kenya. We are going to combine them in order to determine areas that are suitable for disease transmission of Lymphatic Filariasis, a neglected tropical disease, caused by a range of mosquitoes that spread the parasitic worms called the Wuchereria bancrofti through a mosquito bite.\nThere is evidence that indicates that precipitation, temperature, population density, aridity, dryness, land surface elevation and vegetation are risk factors for mosquito infestation, and in turn, greatly influencing the way mosquitoes breed and feeding behaviour within human dwellings thus maintaining disease transmission. We will use the following techniques for mapping areas of suitability:\n\nBinary Classification & Ranking\nSuitability Mapping using Weighted Overlay Analysis\nMCDA Mapping\n\nLet us begin loading the following list of raster files, each is a variable of interest:\n\nRaster: Mean Temperature in Kenya (1000m resolution) named Kenya Mean Teperature.tif\nRaster: Normalized Difference Vegetation Index in Kenya (1000m resolution) named Kenya NDVI.tif\nRaster: Precipitation in Kenya (1000m resolution) named Kenya Precipitation.tif\nRaster: Population Density in Kenya (1000m resolution) named Kenya Population Density.tif\nRaster: Land Surface Elevation in Kenya (1000m resolution) named Kenya Elevation.tif\nRaster: Levels of Dryness in Kenya (1000m resolution) named Kenya Aridity Index.tif\n\n\ntemp &lt;- raster(\"Kenya Mean Teperature.tif\")\nnvdi &lt;- raster(\"Kenya NDVI.tif\")\nprec &lt;- raster(\"Kenya Precipitation.tif\")\npopl &lt;- raster(\"Kenya Population Density.tif\")\nelev &lt;- raster(\"Kenya Elevation.tif\")\narid &lt;- raster(\"Kenya Aridity Index.tif\")\n\nLoad the shapefile nation and state borders for Kenya:\n\nShape file: Kenya’s National border named Kenya_Border_3857.shp\nShape file: Kenya’s State border named Kenya_States_3857.shp\n\n\n# load the shapefiles\nkenya_border &lt;- st_read(\"Kenya_Border_3857.shp\")\nkenya_states &lt;- st_read(\"Kenya_States_3857.shp\")\n\n\n\n\n\n\n\nNote\n\n\n\nAll shape file and raster data were in projected to the CRS: Spherical mercator 3857\n\n\nYou can inspect each raster to know its dimension, extent, resolution and minimum and maximum values. Since are going to stack all the rasters together, you definitely want the: dimension, extent and resolution to be the same. If there’s a slight difference the stack won’t work.\n\n# for instance temp and aridity\ntemp\narid\n\nTo visualize raster data - you can write the following code:\n\n# For instance take the mean temperature for Kenya\n# Spectral colours are useful for diverging scales \"-brewer.spectral\" is Rd-Or-Yl-Gr-Bu. \"-brewer.spectral\" reverses the order\ntm_shape(temp) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Mean Temperature\", frame = FALSE)) +\ntm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne thing to know about this approach - the mappings are purely knowledge-driven and not analytical. For instance, according to previous studies (see example of papers: 1 & 2), we have learnt a bit about the limits or thresholds for these predictors from or below which we can consider an area being suitable for the occurrence of Lymphatic Filariasis (LF).\nWe have summarized these limits or thresholds in a table as follows:\n\n\n\n\n\n\n\nRisk factor\nThreshold for suitability\n\n\n\n\nTemperature\n&gt; 15.0 (degree Celsius)\n\n\nPrecipitation\n&gt; 350 (mm of rainfall)\n\n\nAridity\n&gt; 0.20 (i.e., semi-arid (&gt; 0.5) dry-sub-humid)\n\n\nElevation\n&lt; 1200m (low land)\n\n\nPopulation Density\n&gt; 0 (Inhabitants must exist source for blood meal)\n\n\nNDVI\n&gt; 0.5 (mild levels of vegetation)\n\n\n\nWe should use the aforementioned thresholds to produce binary or Boolean maps using the above criteria. This means that the pixel values of each raster layer will be equal to 0 indicating that its an unsuitable condition for LF transmission, and 1 for suitable conditions for LF transmission.\nLet us reclassify each layer according to the above criteria, starting with temperature:\n\n# reclassify temperature as a binary or Boolean layer\ntemp\n# lowest value = 1.2\n# highest value = 29.6\n# reclassify anything below 15 as 0, and the rest above 15 as 1\n\n# reclassify the values into two groups \n# all values &lt;= 15 change to 0\n# all values &gt; 15 change to 1\ntemp_cl &lt;- c(\n    -Inf, 15, 0, \n    15, Inf, 1\n    )\n\n# convert into a matrix format\ntemp_cl_mat &lt;- matrix(temp_cl, ncol = 3, byrow = TRUE)\n# see matrix\ntemp_cl_mat\n# apply matrix to reclassify() function to categorize the raster accordingly\ntemp_recl &lt;- reclassify(temp, temp_cl_mat)\n\nWhen you reclassify the raster for temp to temp_recl. This is what the output should look like:\n\ntm_shape(temp_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"#F1948A\"), labels = c(\"Unsuitable (&lt;15.0)\", \"Suitable (15 & above)\")),\n        col.legend = tm_legend(title = \"Temperature\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nLets repeat the same steps for elevation to reclassify it’s layer according to the given criteria:\n\n# reclassify elevation as a binary or Boolean layer\nelev\n# lowest value = -11m (below sea level)\n# highest value = 4865m (above sea level)\n# reclassify anything below 1200m as 1, and the rest above 1200m as 0\n\n# all values &lt;= 1200.0 change to 1\n# all values &gt; 1200.0 change to 0\nelev_cl &lt;- c(\n    -Inf, 1200, 1, \n    1200, Inf, 0\n    )\n\n# convert into a matrix format\nelev_cl_mat &lt;- matrix(elev_cl, ncol = 3, byrow = TRUE) \n# see matrix\nelev_cl_mat\n# apply matrix to reclassify() function to categorize the raster accordingly\nelev_recl &lt;- reclassify(elev, elev_cl_mat)\n\nThe elevation output should look something like:\n\ntm_shape(elev_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"orange\"), labels = c(\"Unsuitable (&gt;1200m)\", \"Suitable (1200m & below)\")),\n        col.legend = tm_legend(title = \"Elevation (m)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nTry reclassifying the remaining raster arid, prec, popl and ndvi. The solutions are provided in the code chunks below.\n\n\nClick here\n\nSolutions\n\n# nvdi\nnvdi\n\nnvdi_cl &lt;- c(\n    -Inf, 0.5, 0, \n    0.5, Inf, 1\n    )\n\nnvdi_cl_mat &lt;- matrix(nvdi_cl, ncol = 3, byrow = TRUE)\nnvdi_cl_mat\nnvdi_recl &lt;- reclassify(nvdi, nvdi_cl_mat)\n\ntm_shape(nvdi_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"green\"), labels = c(\"Unsuitable (0.5 & Below)\", \"Suitable (&gt; 0.5)\")),\n        col.legend = tm_legend(title = \"NDVI (Vegetation)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# prec\nprec\n\nprec_cl &lt;- c(\n    -Inf, 350, 0, \n    350, Inf, 1\n    )\n\nprec_cl_mat &lt;- matrix(prec_cl, ncol = 3, byrow = TRUE) \nprec_cl_mat\n\nprec_recl &lt;- reclassify(prec, prec_cl_mat)\n\ntm_shape(prec_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"skyblue\"), labels = c(\"Unsuitable (350mm & Below)\", \"Suitable (&gt; 350mm)\")),\n        col.legend = tm_legend(title = \"Precipitation (mm)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# popl\npopl\n\npopl_cl &lt;- c(\n    -Inf, 0, 0, \n    0, Inf, 1\n    )\n\npopl_cl_mat &lt;- matrix(popl_cl, ncol = 3, byrow = TRUE)\npopl_cl_mat\npopl_recl &lt;- reclassify(popl , popl_cl_mat)\n\ntm_shape(popl_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"orange\"), labels = c(\"Unsuitable (0 people)\", \"Suitable (at least 1 person)\")),\n        col.legend = tm_legend(title = \"Population density\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# arid\narid\n\narid_cl &lt;- c(\n    -Inf, 0.20, 0, \n    0.20, Inf, 1\n    )\n\narid_cl_mat &lt;- matrix(arid_cl, ncol = 3, byrow = TRUE)\narid_cl_mat\narid_recl &lt;- reclassify(arid, arid_cl_mat)\n\ntm_shape(arid_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"orange\"), labels = c(\"Unsuitable (0.2 & below)\", \"Suitable (&gt; 0.2)\")),\n        col.legend = tm_legend(title = \"Aridity (Dryness)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\nWe can identify the environmentally suitable areas for occurrence of LF by simply multiplying the binary maps. Therefore, only the cells or areas with the value of 1 will be kept in the output raster layer. You can do this by writing the following formula:\n\nSuitable_LF_Binary &lt;- temp_recl*nvdi_recl*prec_recl*elev_recl*popl_recl*arid_recl\n\nVisualizing the output:\n\ntm_shape(Suitable_LF_Binary) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"#f0f0f0\", \"red\"), labels = c(\"Zone: Not Suitable\", \"Zone: Highly Suitable\")),\n        col.legend = tm_legend(title = \"Suitability Map (Binary)\", frame = FALSE)) +\ntm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"NAME_1\", size = \"AREA\") +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nRed region corresponds to areas that are highly suitable for LF. South East of Kenya (i.e., Kwale, Kilifi and Lamu) are environmental suitable based on binary classification\n\n\n\n\nWe can identify the environmentally suitable areas for occurrence of LF by simply summing the binary maps generate ratings for each pixel to show the intensity of suitability for LF. A pixel with a summed value would mean the following:\n\n0 would mean that none of the six factors are present at pixel.\n1 would mean at least one out of the six factors are present at that pixel.\n2 would mean at least two out of the six risk factors are present at that pixel (and so on…)\n6 is the maximum meaning that all factors are present there. Therefore, pixel is rated as the highest levels of suitability for LF.\n\nTo sum up the raster, we would first need to use the stack() function before using the calc() for the summation.\n\nRasterStack &lt;- stack(temp_recl, nvdi_recl, prec_recl, elev_recl, popl_recl, arid_recl)\nSuitable_LF_Summed &lt;- calc(RasterStack, sum)\n# check for minimum and maximum\nSuitable_LF_Summed@data@min\nSuitable_LF_Summed@data@max\n# minimum = 2\n# maximum = 6\n\nVisualizing the output:\n\ntm_shape(Suitable_LF_Summed) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"#fff5f0\", \"#FADBD8\", \"#F5B7B1\", \"#F1948A\", \"#E74C3C\"), \n        labels=c(\"Low (2)\", \"Modest (3)\", \"Medium (4)\", \"High (5)\", \"Highest (6)\")),\n        col.legend = tm_legend(title = \"Suitability Map (Summation)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"NAME_1\", size = \"AREA\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing this approach allows the user to see of the fly the intensity for LF suitability. States such as Mandera, Wajir, Isiolo and Turkana have areas that have largely low or modest ratings for LF suitability. Whereas, again the highly suitable areas for LF are the South East states (i.e., Kwale, Kilifi and Lamu).\n\n\nThe rating approach is much better because you can see the intensity for LF suitability. Both approaches highly accessible especially for data and public health program managers in sub-Saharan Africa. These kinds of maps are particularly useful to optimize resources (especially within a low-resource setting) for identifying high-risk areas through knowledge-driven approach (and averting to survey potential unsuitable areas to not wasting limited resources).\nThis approach is great for descriptive knowledge-based decision analysis; however, it does not take into account the factor that certain variables have more importance than others. For instance, precipitation is significantly more important than NDVI because elevated levels of rainfall has a positive impact on a mosquitoes’ survival lifespan and it provides an reservoir for female mosquitoes to breed in standing water. Higher rainfall within a densely populated environment provides not only a breeding ground, but a location closer to their food source (i.e., human and animal blood). Thus, population density is important an factor above NDVI but not as important as precipitation (i.e., high importance: precipitation &gt; population density &gt; NDVI :less importance).\nWe can take this into account by applying weights for each variable determined by importance for the MCDA process - this approach is referred to as the Saaty’s Analytical Hierarchy Process (AHP). Let us see how we apply this methodology.\n\n\n\n\n\n\nWe are going to use Saaty’s Analytical Hierarchy Process (AHP). We will need to standardize our raster factors in order to make comparisons and combination possible, as all of them contain different measures: temp (degree Celsius), prec (mm), elev (meters), popl (counts/sqkm); while nvdi and arid are derived indices without any units. Before deriving the weights and applying to the equation that is a linear combination of the above variables to estimate the suitability index for LF, we can standardize them on to the same scale.\n\n\n\n\n\n\nNote\n\n\n\nWe can use the Raster reclassification to standardise all rasters on the scale from 0 to 10.\n\n\\[ \\text{Standardisation} = \\frac{R - R_{\\min}}{R_{\\max} - R_{\\min}} \\quad (10) \\]\n\n\n\nLet’s begin to standardize the first variable temp to get the raster scales from 1 to 10.\n\ntemp_standardised_10 &lt;- ((temp - temp@data@min)/(temp@data@max-temp@data@min))*10\nplot(temp_standardised_10)\n\nVisualize the output with the scale from 1 to 10:\n\ntm_shape(temp_standardised_10) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Temperature [Rescaled]\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nWe need to repeat this process of standardisation for the remaining raster grids for prec, popl and ndvi. The solutions are provided in the hidden code chunks below.\n\n\nClick here\n\nSolutions\n\n# precipitation\nprec_standardised_10 &lt;- ((prec - prec@data@min)/(prec@data@max-prec@data@min))*10\nplot(prec_standardised_10)\n\n# population\npopl_standardised_10 &lt;- ((popl - popl@data@min)/(popl@data@max-popl@data@min))*10\nplot(popl_standardised_10)\n\n# normalised vegetation difference index\nnvdi_standardised_10 &lt;- ((nvdi - nvdi@data@min)/(nvdi@data@max-nvdi@data@min))*10\nplot(nvdi_standardised_10)\n\n\nFor elev, we will treat this differently - of course, we’ll apply the standardisation. But for elevation, the risk of LF decreases with higher values for elevation. Therefore, we need to flip the raster values accordingly to invert the raster to appear as increasing values reflect that elevation is decreasing\n\n# see what the original raster looks like\nplot(elev)\n\n# standardise the elevation\nelev_standardised_10 &lt;- ((elev - elev@data@min)/(elev@data@max-elev@data@min))*10\nplot(elev_standardised_10)\n\n# clone the standardised elevation layer and rename it\nflipped_elev_standardised_10 &lt;- elev_standardised_10\n# invert the raster simply that taking the maximum value from it and subtract it by its pixel value\nvalues(flipped_elev_standardised_10) &lt;- elev_standardised_10@data@max - values(elev_standardised_10)\n# view inverted raster\nplot(flipped_elev_standardised_10)\n\nVisualize the inverted output (NOTE: Blue: High elevation, Red: low elevation):\n\ntm_shape(flipped_elev_standardised_10) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Inverted Elev (Rescale)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nNote that for arid, we are going to treat it as a constraint (i.e., binary). Anything below 0.2 threshold are environments considered as hyperarid (extremely dry) areas and thus mosquitoes are non-existent there (- meaning that there is no point for us to include that part of the map with hyperarid environments in the suitability analysis!). We will simply mask out that entire region with any pixel contain that value 0.\nLet’s create that masking layer:\n\n# to filter only the values == 1\narid_recl[arid_recl[] !=0 ] = NA\nplot(arid_recl)\n\n\n\n\nTo estimate the weights, a helpful tool has been developed and provided in an Excel Worksheet. It has been created for you so that you can see how the calculations are carried out step-by-step to derive the weights. You can interact with the cells to see the formulas and explanations are provided at each step.\nOpen the spreadsheet Guidance - AHP Calculator Tool.xlsx and read to follow through the steps carefully. These are steps summarized here:\n\nPairwise comparison & Judgement values: Carry out a pairwise comparison of the factors by constructing a matrix as seen in step 1 in the spreadsheet. Use the criteria and make a “Judgement” on the pairwise variables/factors (it could be based on expert opinion or from literature) asserting a variable’s importance over the other.\nMatrices: Create a square matrix with the judgement values inserted to the matrix accordingly (see step 2)\nReciprocals: Take the reciprocals in the matrix (see step 3)\nTake column sums: Sum each of the columns accordingly (see step 4)\nNormalization of matrix: For each element in a column of the square matrix, divide it by it’s corresponding column sum. Repeat this step for all other elements in that matrix (see step 5).\nPriority Vectors/Weights: Once completed, sum-up the elements across each row and divide it by the number of variables to obtain the priority vector or weights. As a sanity check, you will know that the calculations are correct if the sum of the weights are equal to 1 (see step 6).\nValidation of whether the judgement values are reasonable: We need to calculate a Consistency Ratio (CR), which is derived from the Consistency Index (CI) divided by the Random Index (RI). For the CI, we must estimate an eigenvalue which is derived from the summed products between the summed column and weights (see step 7 and click on the cell E94 to view the formula). Use that eigenvalue (i.e., Lambda_Max) and estimate the CI (see formula in the lecture notes (slide 25), and click on the cell E95). Next, use the Random Index table (developed by Saaty, 1980) to determine the RI based on the number of factors (in this case, it is 5). Finally, calculate the CR by dividing the CI/RI (see step 7 and click on the cell E111 to view the formula). Note that if the CI &lt; 0.1, the judgement values assigned in step 1 were acceptable. If CR is bigger than 0.1, then the judgement values in the pairwise comparison in step 1 were unreasonable (and thus you will have to repeat the step with different values again until you get an acceptable CR estimate that is below 0.1!). Here, our CR is 0.0351 &lt; 0.1.\n\n\n\n\nOur model uses the Weighted Linear Combination (WLC) approach as the decision rule. The formula to estimate the suitability of LF is as follows:\n\nSuitability (LF) = \\(w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + w_6x_6\\)\n\nThe \\(w_i\\) and \\(x_i\\) represent the weights deprived from AHP analysis and raster variables, respectively. The weights are multiplied to its corresponding raster to obtain a raster with values weighted but ultimately scaled with an upper limit of 10.\nThis is the derived formula to use in calculating the suitability regions for LF in RStudio:\n\n# use the rescaled columns in the formula\nsuitablemap_WLC &lt;- 0.372*prec_standardised_10 + 0.356*temp_standardised_10 + 0.159*popl_standardised_10 + 0.077*flipped_elev_standardised_10 + 0.037 * nvdi_standardised_10\n# see information\nsuitablemap_WLC\n\n\n\n\n\n\n\nImportant\n\n\n\nEXPLANATION\n\n\n\nComponent\nMax Contribution (approx)\n\n\n\n\n0.372 × 10 (precipitation)\n3.72\n\n\n0.356 × 10 (temperature)\n3.56\n\n\n0.159 × 10 (population)\n1.59\n\n\n0.077 × 10 (flipped elevation)\n0.77\n\n\n0.037 × 10 (NDVI)\n0.37\n\n\nTotal Theoretical Max\n10.01\n\n\n\nThe observed max ≈ 6.786, after taking the weighted sums, meaning that no pixel achieved the maximum 10 across all factors simultaneous. So, 6.786 is highest achievable weighted suitability given your dataset. Likewise, 2.81112 is the lowest achieveable weighted suitability from that given dataset.\nOptional: Re-standardised weighted suitability map to 0-10, add the constrained by aridity layer, and finally apply a criteria to help with the interpretation of risk\n\n\n\nRange\nCategory\n\n\n\n\n0.00 – 0.20\nVery Low\n\n\n0.20 – 0.40\nLow\n\n\n0.40 – 0.60\nModerate\n\n\n0.60 – 0.80\nHigh\n\n\n0.80 – 1.00\nVery High\n\n\n\n\n\nFinal output:\n\n# re-standardised weighted suitability map, constrained by aridity layer\nSWLC_standardised_10 &lt;- ((suitablemap_WLC - suitablemap_WLC@data@min)/(suitablemap_WLC@data@max-suitablemap_WLC@data@min))*10\nplot(SWLC_standardised_10)\n\n# create the classification i.e., risk labels\nSWLC_cl &lt;- c(\n    -Inf,0,0,\n  0,2,1,\n    2,4,2, \n    4,6,3, \n    6,8,4, \n    8,10,5\n    )\n\nSWLC_cl_mat &lt;- matrix(SWLC_cl, ncol = 3, byrow = TRUE)\nSWLC_cl_mat\nSWLC_recl &lt;- reclassify(SWLC_standardised_10, SWLC_cl_mat)\n\ntable(SWLC_recl@data@values)\n\n# code for producing the final output\ntm_shape(SWLC_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"white\", \"#74add1\", \"#e0f3f8\", \"#ffffbf\", \"#fee08b\", \"#f46d43\"), \n        labels = c(\"Not Suitable (0)\", \"Very Low (1)\", \"Low (2)\", \"Moderate (3)\", \"High (4)\", \"Very High (5)\")),\n        col.legend = tm_legend(title = \"Suitability Map (MCDA-AHP)\", frame = FALSE)) +\ntm_shape(arid_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = \"white\"), \n        col.legend = tm_legend(show = FALSE)) + \n    tm_shape(kenya_states) + \n        tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"NAME_1\", size = 0.8) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAreas with high suitability for mosquito occupancy based on the optimal conditions (from selected environmental variables and constrained on aridity) are locations in the far south-east region, small pockets in the central region, and far into the deep-west region of Kenya.\n\n\n\n\n\n\n\n\n\nPaper: R.W. Saaty (1987). The Analytical Hierarchy Process - What it is and how it is used, Mathl Modelling, Volume 9(3-5), pages 161 to 176 LINK\nTechnical Document: IDRISI Kilimanjaro: Guide to GIS and Image Processing. Chapter 12: Decision Support - Decision Strategy Analysis, Section on ‘A Typology of Decisions’, pages 151 to 154. DOWNLOAD\nPaper: A.A. Ali, A. Ahmad (2019). Spatial susceptibility analysis of vector-borne diseases in KMC using geospatial technique and MCDM approach, Modeling Earth Systems and Environment, Issue 5, pages 1135–1159 LINK\nPaper: X. Zhao et al., (2020). Malaria Risk Map Using Spatial Multi-Criteria Decision Analysis along Yunnan Border During the Pre-elimination Period, Am J Trop Med Hyg, Volume 103 (2), pages 793–809 LINK\nPaper: B. Ahmed (2015). Landslide susceptibility mapping using multi-criteria evaluation techniques in Chittagong Metropolitan Area, Bangladesh. Landslides, Issue 12, pages 1077–1095 LINK\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Downloadable)\n\n\n\n\n\nThe population density 2015 data for Kenya was obtained from Worldpop and resampled at 1km Click Here\nRaster for annual precipitation was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here\nRaster for annual temperature was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here\nAridity Index data was obtained for the Global Aridity and PET Database and clipped to Kenya Click Here\nNormalized Differenced Vegetation Index was obtained from NASA MODIS Click Here (Registration required).\nElevation was obtained from the SRTM CSI CGIAR Project, and cropped to Kenya Click Here",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 4: Multi-Decision Criteria Analysis"
    ]
  },
  {
    "objectID": "06-suitability_mapping_AHP.html#introduction",
    "href": "06-suitability_mapping_AHP.html#introduction",
    "title": "Week 4: Multi-Decision Criteria Analysis",
    "section": "",
    "text": "Now, with the proliferation of open spatial dataset, risk models derived from environmental, sociodemographic and topological factors are becoming increasingly available for open research. Such models have broadly shown to be useful in delineating geographical areas of risk or suitability for a certain outcomes. Today, we are going to explore this qualitatively using a knowledge-driven approach.\n\n\nTo provide an introductory overview to the applicability of knowledge-driven methods, in particular, we are going to learn the Multi-Criteria Decision Approach (MCDA) which is a method that use decision rules from existing knowledge to identify areas potential suitability for an outcome. It is especially useful in data-sparse situations, or when for the first time exploring the potential geographical limits of certain outcome.\nFor instance, using modest number of raster layers such as population density, urbanisation, approximating to street segments, house prices and deprivation; it is possible to combine such information so to determine regions for which crime events such as burglaries are likely to occur, or suitable in that matter. This approach has been widely used in a number of disciplines over the last decades, and has gained prominence in public health related fields such as vector-borne disease prevention, and disaster sciences such as landslides. We will learn how to apply these methods to the two context.\n\n\n\nBefore you begin do make sure to download all data by clicking here. Create a folder on called “Week 4” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 4” folder. Open a new R script and set the work directory to Week 4’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 4\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 4\")\n\n\n\n\nWe will need to load the following packages:\n\nsf: Simple Features\ntmap: Thematic Mapping\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\n\nThe above packages sf, tmap & sp should have been installed in the previous session(s). We will need to install a new package:\n\nraster: Raster/gridded data analysis and manipulation.\nspatialEco: Spatial Analysis and Modelling Utilities package, which provides the user further functions for dealing with raster data.\n\n\ninstall.packages(\"spatialEco\")\ninstall.packages(\"raster\")\n\n# Load the packages with library()\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"raster\")\nlibrary(\"sp\")\nlibrary(\"spatialEco\")\n\n\n\n\nWe will be dealing with raster data only for this exercise. These will be a series of climate, environmental and other social-anthropogenic gridded data from Kenya. We are going to combine them in order to determine areas that are suitable for disease transmission of Lymphatic Filariasis, a neglected tropical disease, caused by a range of mosquitoes that spread the parasitic worms called the Wuchereria bancrofti through a mosquito bite.\nThere is evidence that indicates that precipitation, temperature, population density, aridity, dryness, land surface elevation and vegetation are risk factors for mosquito infestation, and in turn, greatly influencing the way mosquitoes breed and feeding behaviour within human dwellings thus maintaining disease transmission. We will use the following techniques for mapping areas of suitability:\n\nBinary Classification & Ranking\nSuitability Mapping using Weighted Overlay Analysis\nMCDA Mapping\n\nLet us begin loading the following list of raster files, each is a variable of interest:\n\nRaster: Mean Temperature in Kenya (1000m resolution) named Kenya Mean Teperature.tif\nRaster: Normalized Difference Vegetation Index in Kenya (1000m resolution) named Kenya NDVI.tif\nRaster: Precipitation in Kenya (1000m resolution) named Kenya Precipitation.tif\nRaster: Population Density in Kenya (1000m resolution) named Kenya Population Density.tif\nRaster: Land Surface Elevation in Kenya (1000m resolution) named Kenya Elevation.tif\nRaster: Levels of Dryness in Kenya (1000m resolution) named Kenya Aridity Index.tif\n\n\ntemp &lt;- raster(\"Kenya Mean Teperature.tif\")\nnvdi &lt;- raster(\"Kenya NDVI.tif\")\nprec &lt;- raster(\"Kenya Precipitation.tif\")\npopl &lt;- raster(\"Kenya Population Density.tif\")\nelev &lt;- raster(\"Kenya Elevation.tif\")\narid &lt;- raster(\"Kenya Aridity Index.tif\")\n\nLoad the shapefile nation and state borders for Kenya:\n\nShape file: Kenya’s National border named Kenya_Border_3857.shp\nShape file: Kenya’s State border named Kenya_States_3857.shp\n\n\n# load the shapefiles\nkenya_border &lt;- st_read(\"Kenya_Border_3857.shp\")\nkenya_states &lt;- st_read(\"Kenya_States_3857.shp\")\n\n\n\n\n\n\n\nNote\n\n\n\nAll shape file and raster data were in projected to the CRS: Spherical mercator 3857\n\n\nYou can inspect each raster to know its dimension, extent, resolution and minimum and maximum values. Since are going to stack all the rasters together, you definitely want the: dimension, extent and resolution to be the same. If there’s a slight difference the stack won’t work.\n\n# for instance temp and aridity\ntemp\narid\n\nTo visualize raster data - you can write the following code:\n\n# For instance take the mean temperature for Kenya\n# Spectral colours are useful for diverging scales \"-brewer.spectral\" is Rd-Or-Yl-Gr-Bu. \"-brewer.spectral\" reverses the order\ntm_shape(temp) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Mean Temperature\", frame = FALSE)) +\ntm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\ntm_layout(frame = FALSE)",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 4: Multi-Decision Criteria Analysis"
    ]
  },
  {
    "objectID": "06-suitability_mapping_AHP.html#suitability-mapping-using-simple-overlays",
    "href": "06-suitability_mapping_AHP.html#suitability-mapping-using-simple-overlays",
    "title": "Week 4: Multi-Decision Criteria Analysis",
    "section": "",
    "text": "One thing to know about this approach - the mappings are purely knowledge-driven and not analytical. For instance, according to previous studies (see example of papers: 1 & 2), we have learnt a bit about the limits or thresholds for these predictors from or below which we can consider an area being suitable for the occurrence of Lymphatic Filariasis (LF).\nWe have summarized these limits or thresholds in a table as follows:\n\n\n\n\n\n\n\nRisk factor\nThreshold for suitability\n\n\n\n\nTemperature\n&gt; 15.0 (degree Celsius)\n\n\nPrecipitation\n&gt; 350 (mm of rainfall)\n\n\nAridity\n&gt; 0.20 (i.e., semi-arid (&gt; 0.5) dry-sub-humid)\n\n\nElevation\n&lt; 1200m (low land)\n\n\nPopulation Density\n&gt; 0 (Inhabitants must exist source for blood meal)\n\n\nNDVI\n&gt; 0.5 (mild levels of vegetation)\n\n\n\nWe should use the aforementioned thresholds to produce binary or Boolean maps using the above criteria. This means that the pixel values of each raster layer will be equal to 0 indicating that its an unsuitable condition for LF transmission, and 1 for suitable conditions for LF transmission.\nLet us reclassify each layer according to the above criteria, starting with temperature:\n\n# reclassify temperature as a binary or Boolean layer\ntemp\n# lowest value = 1.2\n# highest value = 29.6\n# reclassify anything below 15 as 0, and the rest above 15 as 1\n\n# reclassify the values into two groups \n# all values &lt;= 15 change to 0\n# all values &gt; 15 change to 1\ntemp_cl &lt;- c(\n    -Inf, 15, 0, \n    15, Inf, 1\n    )\n\n# convert into a matrix format\ntemp_cl_mat &lt;- matrix(temp_cl, ncol = 3, byrow = TRUE)\n# see matrix\ntemp_cl_mat\n# apply matrix to reclassify() function to categorize the raster accordingly\ntemp_recl &lt;- reclassify(temp, temp_cl_mat)\n\nWhen you reclassify the raster for temp to temp_recl. This is what the output should look like:\n\ntm_shape(temp_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"#F1948A\"), labels = c(\"Unsuitable (&lt;15.0)\", \"Suitable (15 & above)\")),\n        col.legend = tm_legend(title = \"Temperature\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nLets repeat the same steps for elevation to reclassify it’s layer according to the given criteria:\n\n# reclassify elevation as a binary or Boolean layer\nelev\n# lowest value = -11m (below sea level)\n# highest value = 4865m (above sea level)\n# reclassify anything below 1200m as 1, and the rest above 1200m as 0\n\n# all values &lt;= 1200.0 change to 1\n# all values &gt; 1200.0 change to 0\nelev_cl &lt;- c(\n    -Inf, 1200, 1, \n    1200, Inf, 0\n    )\n\n# convert into a matrix format\nelev_cl_mat &lt;- matrix(elev_cl, ncol = 3, byrow = TRUE) \n# see matrix\nelev_cl_mat\n# apply matrix to reclassify() function to categorize the raster accordingly\nelev_recl &lt;- reclassify(elev, elev_cl_mat)\n\nThe elevation output should look something like:\n\ntm_shape(elev_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"orange\"), labels = c(\"Unsuitable (&gt;1200m)\", \"Suitable (1200m & below)\")),\n        col.legend = tm_legend(title = \"Elevation (m)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nTry reclassifying the remaining raster arid, prec, popl and ndvi. The solutions are provided in the code chunks below.\n\n\nClick here\n\nSolutions\n\n# nvdi\nnvdi\n\nnvdi_cl &lt;- c(\n    -Inf, 0.5, 0, \n    0.5, Inf, 1\n    )\n\nnvdi_cl_mat &lt;- matrix(nvdi_cl, ncol = 3, byrow = TRUE)\nnvdi_cl_mat\nnvdi_recl &lt;- reclassify(nvdi, nvdi_cl_mat)\n\ntm_shape(nvdi_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"green\"), labels = c(\"Unsuitable (0.5 & Below)\", \"Suitable (&gt; 0.5)\")),\n        col.legend = tm_legend(title = \"NDVI (Vegetation)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# prec\nprec\n\nprec_cl &lt;- c(\n    -Inf, 350, 0, \n    350, Inf, 1\n    )\n\nprec_cl_mat &lt;- matrix(prec_cl, ncol = 3, byrow = TRUE) \nprec_cl_mat\n\nprec_recl &lt;- reclassify(prec, prec_cl_mat)\n\ntm_shape(prec_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"skyblue\"), labels = c(\"Unsuitable (350mm & Below)\", \"Suitable (&gt; 350mm)\")),\n        col.legend = tm_legend(title = \"Precipitation (mm)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# popl\npopl\n\npopl_cl &lt;- c(\n    -Inf, 0, 0, \n    0, Inf, 1\n    )\n\npopl_cl_mat &lt;- matrix(popl_cl, ncol = 3, byrow = TRUE)\npopl_cl_mat\npopl_recl &lt;- reclassify(popl , popl_cl_mat)\n\ntm_shape(popl_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"orange\"), labels = c(\"Unsuitable (0 people)\", \"Suitable (at least 1 person)\")),\n        col.legend = tm_legend(title = \"Population density\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n# arid\narid\n\narid_cl &lt;- c(\n    -Inf, 0.20, 0, \n    0.20, Inf, 1\n    )\n\narid_cl_mat &lt;- matrix(arid_cl, ncol = 3, byrow = TRUE)\narid_cl_mat\narid_recl &lt;- reclassify(arid, arid_cl_mat)\n\ntm_shape(arid_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"grey\", \"orange\"), labels = c(\"Unsuitable (0.2 & below)\", \"Suitable (&gt; 0.2)\")),\n        col.legend = tm_legend(title = \"Aridity (Dryness)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\nWe can identify the environmentally suitable areas for occurrence of LF by simply multiplying the binary maps. Therefore, only the cells or areas with the value of 1 will be kept in the output raster layer. You can do this by writing the following formula:\n\nSuitable_LF_Binary &lt;- temp_recl*nvdi_recl*prec_recl*elev_recl*popl_recl*arid_recl\n\nVisualizing the output:\n\ntm_shape(Suitable_LF_Binary) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"#f0f0f0\", \"red\"), labels = c(\"Zone: Not Suitable\", \"Zone: Highly Suitable\")),\n        col.legend = tm_legend(title = \"Suitability Map (Binary)\", frame = FALSE)) +\ntm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"NAME_1\", size = \"AREA\") +\ntm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nRed region corresponds to areas that are highly suitable for LF. South East of Kenya (i.e., Kwale, Kilifi and Lamu) are environmental suitable based on binary classification\n\n\n\n\nWe can identify the environmentally suitable areas for occurrence of LF by simply summing the binary maps generate ratings for each pixel to show the intensity of suitability for LF. A pixel with a summed value would mean the following:\n\n0 would mean that none of the six factors are present at pixel.\n1 would mean at least one out of the six factors are present at that pixel.\n2 would mean at least two out of the six risk factors are present at that pixel (and so on…)\n6 is the maximum meaning that all factors are present there. Therefore, pixel is rated as the highest levels of suitability for LF.\n\nTo sum up the raster, we would first need to use the stack() function before using the calc() for the summation.\n\nRasterStack &lt;- stack(temp_recl, nvdi_recl, prec_recl, elev_recl, popl_recl, arid_recl)\nSuitable_LF_Summed &lt;- calc(RasterStack, sum)\n# check for minimum and maximum\nSuitable_LF_Summed@data@min\nSuitable_LF_Summed@data@max\n# minimum = 2\n# maximum = 6\n\nVisualizing the output:\n\ntm_shape(Suitable_LF_Summed) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"#fff5f0\", \"#FADBD8\", \"#F5B7B1\", \"#F1948A\", \"#E74C3C\"), \n        labels=c(\"Low (2)\", \"Modest (3)\", \"Medium (4)\", \"High (5)\", \"Highest (6)\")),\n        col.legend = tm_legend(title = \"Suitability Map (Summation)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"NAME_1\", size = \"AREA\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUsing this approach allows the user to see of the fly the intensity for LF suitability. States such as Mandera, Wajir, Isiolo and Turkana have areas that have largely low or modest ratings for LF suitability. Whereas, again the highly suitable areas for LF are the South East states (i.e., Kwale, Kilifi and Lamu).\n\n\nThe rating approach is much better because you can see the intensity for LF suitability. Both approaches highly accessible especially for data and public health program managers in sub-Saharan Africa. These kinds of maps are particularly useful to optimize resources (especially within a low-resource setting) for identifying high-risk areas through knowledge-driven approach (and averting to survey potential unsuitable areas to not wasting limited resources).\nThis approach is great for descriptive knowledge-based decision analysis; however, it does not take into account the factor that certain variables have more importance than others. For instance, precipitation is significantly more important than NDVI because elevated levels of rainfall has a positive impact on a mosquitoes’ survival lifespan and it provides an reservoir for female mosquitoes to breed in standing water. Higher rainfall within a densely populated environment provides not only a breeding ground, but a location closer to their food source (i.e., human and animal blood). Thus, population density is important an factor above NDVI but not as important as precipitation (i.e., high importance: precipitation &gt; population density &gt; NDVI :less importance).\nWe can take this into account by applying weights for each variable determined by importance for the MCDA process - this approach is referred to as the Saaty’s Analytical Hierarchy Process (AHP). Let us see how we apply this methodology.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 4: Multi-Decision Criteria Analysis"
    ]
  },
  {
    "objectID": "06-suitability_mapping_AHP.html#analytical-hierarchy-process-ahp",
    "href": "06-suitability_mapping_AHP.html#analytical-hierarchy-process-ahp",
    "title": "Week 4: Multi-Decision Criteria Analysis",
    "section": "",
    "text": "We are going to use Saaty’s Analytical Hierarchy Process (AHP). We will need to standardize our raster factors in order to make comparisons and combination possible, as all of them contain different measures: temp (degree Celsius), prec (mm), elev (meters), popl (counts/sqkm); while nvdi and arid are derived indices without any units. Before deriving the weights and applying to the equation that is a linear combination of the above variables to estimate the suitability index for LF, we can standardize them on to the same scale.\n\n\n\n\n\n\nNote\n\n\n\nWe can use the Raster reclassification to standardise all rasters on the scale from 0 to 10.\n\n\\[ \\text{Standardisation} = \\frac{R - R_{\\min}}{R_{\\max} - R_{\\min}} \\quad (10) \\]\n\n\n\nLet’s begin to standardize the first variable temp to get the raster scales from 1 to 10.\n\ntemp_standardised_10 &lt;- ((temp - temp@data@min)/(temp@data@max-temp@data@min))*10\nplot(temp_standardised_10)\n\nVisualize the output with the scale from 1 to 10:\n\ntm_shape(temp_standardised_10) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Temperature [Rescaled]\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nWe need to repeat this process of standardisation for the remaining raster grids for prec, popl and ndvi. The solutions are provided in the hidden code chunks below.\n\n\nClick here\n\nSolutions\n\n# precipitation\nprec_standardised_10 &lt;- ((prec - prec@data@min)/(prec@data@max-prec@data@min))*10\nplot(prec_standardised_10)\n\n# population\npopl_standardised_10 &lt;- ((popl - popl@data@min)/(popl@data@max-popl@data@min))*10\nplot(popl_standardised_10)\n\n# normalised vegetation difference index\nnvdi_standardised_10 &lt;- ((nvdi - nvdi@data@min)/(nvdi@data@max-nvdi@data@min))*10\nplot(nvdi_standardised_10)\n\n\nFor elev, we will treat this differently - of course, we’ll apply the standardisation. But for elevation, the risk of LF decreases with higher values for elevation. Therefore, we need to flip the raster values accordingly to invert the raster to appear as increasing values reflect that elevation is decreasing\n\n# see what the original raster looks like\nplot(elev)\n\n# standardise the elevation\nelev_standardised_10 &lt;- ((elev - elev@data@min)/(elev@data@max-elev@data@min))*10\nplot(elev_standardised_10)\n\n# clone the standardised elevation layer and rename it\nflipped_elev_standardised_10 &lt;- elev_standardised_10\n# invert the raster simply that taking the maximum value from it and subtract it by its pixel value\nvalues(flipped_elev_standardised_10) &lt;- elev_standardised_10@data@max - values(elev_standardised_10)\n# view inverted raster\nplot(flipped_elev_standardised_10)\n\nVisualize the inverted output (NOTE: Blue: High elevation, Red: low elevation):\n\ntm_shape(flipped_elev_standardised_10) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Inverted Elev (Rescale)\", frame = FALSE)) +\n    tm_shape(kenya_states) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nNote that for arid, we are going to treat it as a constraint (i.e., binary). Anything below 0.2 threshold are environments considered as hyperarid (extremely dry) areas and thus mosquitoes are non-existent there (- meaning that there is no point for us to include that part of the map with hyperarid environments in the suitability analysis!). We will simply mask out that entire region with any pixel contain that value 0.\nLet’s create that masking layer:\n\n# to filter only the values == 1\narid_recl[arid_recl[] !=0 ] = NA\nplot(arid_recl)\n\n\n\n\nTo estimate the weights, a helpful tool has been developed and provided in an Excel Worksheet. It has been created for you so that you can see how the calculations are carried out step-by-step to derive the weights. You can interact with the cells to see the formulas and explanations are provided at each step.\nOpen the spreadsheet Guidance - AHP Calculator Tool.xlsx and read to follow through the steps carefully. These are steps summarized here:\n\nPairwise comparison & Judgement values: Carry out a pairwise comparison of the factors by constructing a matrix as seen in step 1 in the spreadsheet. Use the criteria and make a “Judgement” on the pairwise variables/factors (it could be based on expert opinion or from literature) asserting a variable’s importance over the other.\nMatrices: Create a square matrix with the judgement values inserted to the matrix accordingly (see step 2)\nReciprocals: Take the reciprocals in the matrix (see step 3)\nTake column sums: Sum each of the columns accordingly (see step 4)\nNormalization of matrix: For each element in a column of the square matrix, divide it by it’s corresponding column sum. Repeat this step for all other elements in that matrix (see step 5).\nPriority Vectors/Weights: Once completed, sum-up the elements across each row and divide it by the number of variables to obtain the priority vector or weights. As a sanity check, you will know that the calculations are correct if the sum of the weights are equal to 1 (see step 6).\nValidation of whether the judgement values are reasonable: We need to calculate a Consistency Ratio (CR), which is derived from the Consistency Index (CI) divided by the Random Index (RI). For the CI, we must estimate an eigenvalue which is derived from the summed products between the summed column and weights (see step 7 and click on the cell E94 to view the formula). Use that eigenvalue (i.e., Lambda_Max) and estimate the CI (see formula in the lecture notes (slide 25), and click on the cell E95). Next, use the Random Index table (developed by Saaty, 1980) to determine the RI based on the number of factors (in this case, it is 5). Finally, calculate the CR by dividing the CI/RI (see step 7 and click on the cell E111 to view the formula). Note that if the CI &lt; 0.1, the judgement values assigned in step 1 were acceptable. If CR is bigger than 0.1, then the judgement values in the pairwise comparison in step 1 were unreasonable (and thus you will have to repeat the step with different values again until you get an acceptable CR estimate that is below 0.1!). Here, our CR is 0.0351 &lt; 0.1.\n\n\n\n\nOur model uses the Weighted Linear Combination (WLC) approach as the decision rule. The formula to estimate the suitability of LF is as follows:\n\nSuitability (LF) = \\(w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + w_6x_6\\)\n\nThe \\(w_i\\) and \\(x_i\\) represent the weights deprived from AHP analysis and raster variables, respectively. The weights are multiplied to its corresponding raster to obtain a raster with values weighted but ultimately scaled with an upper limit of 10.\nThis is the derived formula to use in calculating the suitability regions for LF in RStudio:\n\n# use the rescaled columns in the formula\nsuitablemap_WLC &lt;- 0.372*prec_standardised_10 + 0.356*temp_standardised_10 + 0.159*popl_standardised_10 + 0.077*flipped_elev_standardised_10 + 0.037 * nvdi_standardised_10\n# see information\nsuitablemap_WLC\n\n\n\n\n\n\n\nImportant\n\n\n\nEXPLANATION\n\n\n\nComponent\nMax Contribution (approx)\n\n\n\n\n0.372 × 10 (precipitation)\n3.72\n\n\n0.356 × 10 (temperature)\n3.56\n\n\n0.159 × 10 (population)\n1.59\n\n\n0.077 × 10 (flipped elevation)\n0.77\n\n\n0.037 × 10 (NDVI)\n0.37\n\n\nTotal Theoretical Max\n10.01\n\n\n\nThe observed max ≈ 6.786, after taking the weighted sums, meaning that no pixel achieved the maximum 10 across all factors simultaneous. So, 6.786 is highest achievable weighted suitability given your dataset. Likewise, 2.81112 is the lowest achieveable weighted suitability from that given dataset.\nOptional: Re-standardised weighted suitability map to 0-10, add the constrained by aridity layer, and finally apply a criteria to help with the interpretation of risk\n\n\n\nRange\nCategory\n\n\n\n\n0.00 – 0.20\nVery Low\n\n\n0.20 – 0.40\nLow\n\n\n0.40 – 0.60\nModerate\n\n\n0.60 – 0.80\nHigh\n\n\n0.80 – 1.00\nVery High\n\n\n\n\n\nFinal output:\n\n# re-standardised weighted suitability map, constrained by aridity layer\nSWLC_standardised_10 &lt;- ((suitablemap_WLC - suitablemap_WLC@data@min)/(suitablemap_WLC@data@max-suitablemap_WLC@data@min))*10\nplot(SWLC_standardised_10)\n\n# create the classification i.e., risk labels\nSWLC_cl &lt;- c(\n    -Inf,0,0,\n  0,2,1,\n    2,4,2, \n    4,6,3, \n    6,8,4, \n    8,10,5\n    )\n\nSWLC_cl_mat &lt;- matrix(SWLC_cl, ncol = 3, byrow = TRUE)\nSWLC_cl_mat\nSWLC_recl &lt;- reclassify(SWLC_standardised_10, SWLC_cl_mat)\n\ntable(SWLC_recl@data@values)\n\n# code for producing the final output\ntm_shape(SWLC_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"white\", \"#74add1\", \"#e0f3f8\", \"#ffffbf\", \"#fee08b\", \"#f46d43\"), \n        labels = c(\"Not Suitable (0)\", \"Very Low (1)\", \"Low (2)\", \"Moderate (3)\", \"High (4)\", \"Very High (5)\")),\n        col.legend = tm_legend(title = \"Suitability Map (MCDA-AHP)\", frame = FALSE)) +\ntm_shape(arid_recl) + \n    tm_raster(col.scale = tm_scale_categorical(values = \"white\"), \n        col.legend = tm_legend(show = FALSE)) + \n    tm_shape(kenya_states) + \n        tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"NAME_1\", size = 0.8) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAreas with high suitability for mosquito occupancy based on the optimal conditions (from selected environmental variables and constrained on aridity) are locations in the far south-east region, small pockets in the central region, and far into the deep-west region of Kenya.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 4: Multi-Decision Criteria Analysis"
    ]
  },
  {
    "objectID": "06-suitability_mapping_AHP.html#references-see-reading-list",
    "href": "06-suitability_mapping_AHP.html#references-see-reading-list",
    "title": "Week 4: Multi-Decision Criteria Analysis",
    "section": "",
    "text": "Paper: R.W. Saaty (1987). The Analytical Hierarchy Process - What it is and how it is used, Mathl Modelling, Volume 9(3-5), pages 161 to 176 LINK\nTechnical Document: IDRISI Kilimanjaro: Guide to GIS and Image Processing. Chapter 12: Decision Support - Decision Strategy Analysis, Section on ‘A Typology of Decisions’, pages 151 to 154. DOWNLOAD\nPaper: A.A. Ali, A. Ahmad (2019). Spatial susceptibility analysis of vector-borne diseases in KMC using geospatial technique and MCDM approach, Modeling Earth Systems and Environment, Issue 5, pages 1135–1159 LINK\nPaper: X. Zhao et al., (2020). Malaria Risk Map Using Spatial Multi-Criteria Decision Analysis along Yunnan Border During the Pre-elimination Period, Am J Trop Med Hyg, Volume 103 (2), pages 793–809 LINK\nPaper: B. Ahmed (2015). Landslide susceptibility mapping using multi-criteria evaluation techniques in Chittagong Metropolitan Area, Bangladesh. Landslides, Issue 12, pages 1077–1095 LINK\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Downloadable)\n\n\n\n\n\nThe population density 2015 data for Kenya was obtained from Worldpop and resampled at 1km Click Here\nRaster for annual precipitation was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here\nRaster for annual temperature was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here\nAridity Index data was obtained for the Global Aridity and PET Database and clipped to Kenya Click Here\nNormalized Differenced Vegetation Index was obtained from NASA MODIS Click Here (Registration required).\nElevation was obtained from the SRTM CSI CGIAR Project, and cropped to Kenya Click Here",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 4: Multi-Decision Criteria Analysis"
    ]
  },
  {
    "objectID": "05-spatial_autocorrelation.html",
    "href": "05-spatial_autocorrelation.html",
    "title": "Week 3: Spatial autocorrelation",
    "section": "",
    "text": "This week, we focus on the first of two key properties of spatial data: spatial dependence. Spatial dependence is the idea that the observed value of a variable in one location is often dependent (to some degree) on the observed value of the same value in a nearby location. For spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Spatial autocorrelation occurs when these values are not independent of one another and instead cluster together across geographic space.\nA critical first step of spatial autocorrelation is to define the criteria under which a spatial unit (e.g. an areal or point unit) can be understood as a “neighbour” to another unit. As highlighted in previous weeks, spatial properties can often take on several meanings, and as a result, have an impact on the validity and accuracy of spatial analysis. This multiplicity also can be applied to the concept of spatial neighbours which can be defined through adjacency, contiguity or distance-based measures. As the specification of these criteria can impact the results, the definition followed therefore need to be grounded in particular theory that aims to represent the process and variable investigated.\n\n\nUnderstanding the notion that spatial dependence refers to the degree of spatial autocorrelation between independently measured values observed in geographical space. We will learn how to estimate global measures of spatial autocorrelation (e.g., Moran’s I) to provide a singular measure of spatial dependence. We will learn how Local indicators of spatial association (LISA) to perform a similar function but yield multiple location-specific measures of spatial dependence.\n\n\n\nTo enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script. Remember, you can save your script with the quick save shortcuts (e.g. cmd + s (mac) / ctrl + s (windows)).\n\n\n\nBefore you begin, if you have not done so already, please make sure to download all data by clicking here. Create a sub folder called “Week 3” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the downloaded zip folder and store it into “Week 3” folder. Use your newly open R script and set the work directory to Week 3’s folder.\nFor Windows, the code for setting the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 3\")\n\nFor MAC, the code for setting the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 3\")\n\n\n\n\nWe will need to load the following packages from previous practicals:\n\nsf: Simple Features\ntmap: Thematic Mapping\ntidyverse: Contains a collection of packages to support efficient data managing\n\nThe above packages sf, tmap and tidyverse should have been installed previously in the last session. Therefore, we do not have to install them again, we only need to activate them using the library() function. There will be some new packages we need to install:\n\nnngeo: The nngeo package includes functions for spatial join of layers based on k-nearest neighbor relation between features\nspdep: Provides access to a collection of functions to create spatial weights matrix objects from polygon contiguities, and testing spatial dependence\nsp: Provides access to a collection of functions for handling different classes and methods for spatial data\ndata.table: Gives us access to function that deals with faster aggregation of large data frames etc.,\n\n\n# install the packages using the install.package()\ninstall.packages(\"nngeo\")\ninstall.packages(\"spdep\")\ninstall.packages(\"sp\")\ninstall.packages(\"data.table\")\n\n# Load the packages with library()\nlibrary(\"tidyverse\")\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"nngeo\")\nlibrary(\"spdep\")\nlibrary(\"sp\")\nlibrary(\"data.table\")",
    "crumbs": [
      "Foundation & Theory",
      "Week 3: Spatial autocorrelation"
    ]
  },
  {
    "objectID": "05-spatial_autocorrelation.html#introduction",
    "href": "05-spatial_autocorrelation.html#introduction",
    "title": "Week 3: Spatial autocorrelation",
    "section": "",
    "text": "This week, we focus on the first of two key properties of spatial data: spatial dependence. Spatial dependence is the idea that the observed value of a variable in one location is often dependent (to some degree) on the observed value of the same value in a nearby location. For spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Spatial autocorrelation occurs when these values are not independent of one another and instead cluster together across geographic space.\nA critical first step of spatial autocorrelation is to define the criteria under which a spatial unit (e.g. an areal or point unit) can be understood as a “neighbour” to another unit. As highlighted in previous weeks, spatial properties can often take on several meanings, and as a result, have an impact on the validity and accuracy of spatial analysis. This multiplicity also can be applied to the concept of spatial neighbours which can be defined through adjacency, contiguity or distance-based measures. As the specification of these criteria can impact the results, the definition followed therefore need to be grounded in particular theory that aims to represent the process and variable investigated.\n\n\nUnderstanding the notion that spatial dependence refers to the degree of spatial autocorrelation between independently measured values observed in geographical space. We will learn how to estimate global measures of spatial autocorrelation (e.g., Moran’s I) to provide a singular measure of spatial dependence. We will learn how Local indicators of spatial association (LISA) to perform a similar function but yield multiple location-specific measures of spatial dependence.\n\n\n\nTo enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script. Remember, you can save your script with the quick save shortcuts (e.g. cmd + s (mac) / ctrl + s (windows)).\n\n\n\nBefore you begin, if you have not done so already, please make sure to download all data by clicking here. Create a sub folder called “Week 3” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the downloaded zip folder and store it into “Week 3” folder. Use your newly open R script and set the work directory to Week 3’s folder.\nFor Windows, the code for setting the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 3\")\n\nFor MAC, the code for setting the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 3\")\n\n\n\n\nWe will need to load the following packages from previous practicals:\n\nsf: Simple Features\ntmap: Thematic Mapping\ntidyverse: Contains a collection of packages to support efficient data managing\n\nThe above packages sf, tmap and tidyverse should have been installed previously in the last session. Therefore, we do not have to install them again, we only need to activate them using the library() function. There will be some new packages we need to install:\n\nnngeo: The nngeo package includes functions for spatial join of layers based on k-nearest neighbor relation between features\nspdep: Provides access to a collection of functions to create spatial weights matrix objects from polygon contiguities, and testing spatial dependence\nsp: Provides access to a collection of functions for handling different classes and methods for spatial data\ndata.table: Gives us access to function that deals with faster aggregation of large data frames etc.,\n\n\n# install the packages using the install.package()\ninstall.packages(\"nngeo\")\ninstall.packages(\"spdep\")\ninstall.packages(\"sp\")\ninstall.packages(\"data.table\")\n\n# Load the packages with library()\nlibrary(\"tidyverse\")\nlibrary(\"sf\")\nlibrary(\"tmap\")\nlibrary(\"nngeo\")\nlibrary(\"spdep\")\nlibrary(\"sp\")\nlibrary(\"data.table\")",
    "crumbs": [
      "Foundation & Theory",
      "Week 3: Spatial autocorrelation"
    ]
  },
  {
    "objectID": "05-spatial_autocorrelation.html#case-study",
    "href": "05-spatial_autocorrelation.html#case-study",
    "title": "Week 3: Spatial autocorrelation",
    "section": "2 Case study",
    "text": "2 Case study\nThis week looks at spatial dependence and autocorrelation in detail, focusing on the different methods of assessment. As part of this, we look at the multiple methods to defining spatial neighbours and their suitability of use across different spatial phenomena – and how this approach is used to generate spatial weights for use within these spatial autocorrelation methods as well as their potential to generate spatially-explicit variables.\nWe put these learnings into practice through an analysis of spatial dependence of areal crime data, experimenting with the deployment of different neighbours and the impact of their analyses. For this practical we will look at the distribution of thefts from persons in the borough of Camden.\n\n2.1 Neighbours\nIf we want to come up with quantifiable descriptions of variables and how they vary over space, then we need to find ways of quantifying the distance from point to point. When you attach values to the polygons of wards in London, and visualise them, different patterns appear, and the different shapes and sizes of the polygons effect what these patterns look like. There can appear to be clusters, or the distribution can be random. If you want to explain and discuss variables, the underlying causes, and the possible solutions to issues, it becomes useful to quantify how clustered, or at the opposite end, how random these distributions are. This issue is known as spatial autocorrelation.\nIn raster data, variables are measured at regular spatial intervals (or interpolated to be represented as such). Each measurement is regularly spaced from its neighbours, like the pixels on the screen you are reading this from. With vector data, the distance of measurement to measurement, and the size and shape of the “pixel” of measurement becomes part of the captured information. Whilst this can allow for more nuanced representations of spatial phenomena, it also means that the quantity and type of distance between measurements needs to be acknowledged.\nIf you want to calculate the relative spatial relation of distributions, knowledge of what counts as a “neighbour” becomes useful. Neighbours can be neighbours due to euclidean distance (distance in space), or they can be due to shared relationships, like a shared boundary, or they can simply be the nearest neighbour, if there aren’t many other vectors around. Depending on the variable you are measuring the appropriateness of neighbourhood calculation techniques can change.\n\n2.1.1 Loading our data sets\nNow we have the data in the correct folders, we can load and plot the shape data.\n\n# load Camden boundaries\ncamden_oas &lt;- st_read('OAs_camden_2011.shp', crs=27700)\n\n# inspect\ntm_shape(camden_oas) +\n  tm_polygons()\n\n\n\n\n\n\n\n\n\n\nYou can see how one of these output areas could have many more neighbours than others, they vary a great deal in size and shape. The dimensions of these objects change over space, as a result the measurements within them must change too.\nOutput areas are designed to convey and contain census information, so they are created in a way that maintains a similar number of residents in each one. The more sparsely populated an OA the larger it is. Output Areas are designed to cover the entirety of the land of England and Wales so they stretch over places where there are no people. In the north of Camden the largest Ouput Areas span over Hampstead Heath, a large park.\nLet’s explore how to find different kinds of neighbours using the example of one ‘randomly’ selected output area (E00004174) that happens to contain the UCL main campus.\n\n# highlight E00004174\ntm_shape(camden_oas) +\n    tm_polygons(fill = \"white\", col= \"black\") +\n    tm_shape(camden_oas[camden_oas$OA11CD=='E00004174',]) +\n    tm_polygons(fill = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.2 Euclidean neighbours\nThe first way we are going to call something a neighbour is by using Euclidean distance. As our OA shapefile is projected in BNG (British National Grid), the coordinates are planar, going up 1 is the same distance as going sideways 1. Even better the coordinates are in metric measurements so it’s easy to make up heuristic distances.\nLet’s call every output area with a centroid 500m or less away from the centroid of our chosen OA a neighbour:\n\nwe select only the the centroid of our chosen output area and all other areas (with st_centroid())\nwe set the maximum number of neighbours we want to find to “50” (with parameter k)\nwe set the maximum distance of calling an OA centroid a neigbour to “500” (with parameter maxdist)\nwe return a sparse matrix that tells us whether each OA is a neighbour or not (with parameter sparse)\n\n\n# assign our chosen OA to a variable \nchosen_oa &lt;- 'E00004174'\n\n# identify neighbours\nchosen_oa_neighbours &lt;- st_nn(st_geometry(st_centroid(camden_oas[camden_oas$OA11CD==chosen_oa,])), \n                              st_geometry(st_centroid(camden_oas)),\n                              sparse = TRUE,\n                              k = 50,\n                              maxdist = 500) \n\n# inspect\nclass(chosen_oa_neighbours)\n\n# get the names (codes) of these neighbours\nneighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],]\nneighbour_names &lt;- neighbour_names$OA11CD\n\n# inspect\n# add base layer\ntm_shape(camden_oas) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n# add neighbours layer to highlight only the neighbours of E00004174\ntm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + \n    tm_polygons(fill = \"green\", col = \"black\") +\n# add to show chosen OA E00004174\ntm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + \n    tm_polygons(fill = \"red\", col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.3 Shared boundary neighbours\nThe next way of calculating neighbours takes into account the actual shape and location of the polygons in our shapefile. This has only recently been added to the world of sf(), previously we would have reverted to using the sp() package and others that depend on it such as spdep().\nWe can create two functions that check whether any polygons share boundaries or overlap one another, and then also check by how much. These new functions are based on the st_relate() function. The different cases of these are known as queen, and rook. These describe the relations in a similar way to the possible chess board movements of these pieces.\n\n\n\n\n\n\nNote\n\n\n\nDo have a look at the short lectures by Luc Anselin on Moran’s I, the interpretation of Moran’s I, and neighbours and spatial weights for some additional explanation on measuring spatial autocorrelation with Moran’s I.\nThese functions are created for the sole purpose of demonstrating what contiguity spatial weight matrices are:\n\n# for rook case\nst_rook &lt;- function(a, b = a) st_relate(a, b, pattern = 'F***1****')\n\n# for queen case\nst_queen &lt;- function(a, b = a) st_relate(a, b, pattern = 'F***T****')\n\n\n\nNow that we’ve created the functions lets try them out.\nQueen\n\n# identify neighbours based on Queen\nchosen_oa_neighbours_queen &lt;- st_queen(st_geometry(camden_oas[camden_oas$OA11CD==chosen_oa,]), \n    st_geometry(camden_oas))\n\n# get the names (codes) of these neighbours\nneighbour_names_qn &lt;- camden_oas[chosen_oa_neighbours_queen[[1]],]\nneighbour_names_qn &lt;- neighbour_names_qn$OA11CD\n\n# inspect\n# add base layer\ntm_shape(camden_oas) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    # add neighbours layer to highlight only the neighbours of E00004174\n    tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names_qn,]) + \n    tm_polygons(fill = \"green\", col = \"black\") +\n    # add to show chosen OA E00004174\n    tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + \n    tm_polygons(fill = \"red\", col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBecause the tolerance of the shared boundaries in the st_rook() pattern and the st_queen() pattern, in this example they both assign the same neighbours. This is true for many non-square polygons as the difference is often given as whether two shapes share one or more points. Therefore the difference can have more to do with the resolution and alignment of your polygons than the actual spatial properties they represent. They can and do find different neighbours in other situations. Follow the grid example in the st_relate() documentation if you want to see it working.\n\n\n\n\n\n2.2 Theft in Camden\nNow that we have found the different ways of finding neighbours we can consider how they relate to one another. There are two ways of looking at spatial autocorrelation:\n\nGlobal: This is a way of creating a metric of how regularly or irregularly clustered the variables are over the entire area studied.\nLocal: This is the difference between an area and its neighbours. You would expect neighbours to be similar, but you can find exceptional places and results by seeing if places are quantifiably more like or dislike their neighbours than the average other place.\n\nBut before we start that let’s get into the data we are going to use! We’ll be using personal theft data from around Camden. Our neighbourhood analysis of spatial autocorrelation should allow us to quantify the pattern of distribution of reported theft from persons in Camden in 2019.\n\n# load theft data\ncamden_theft &lt;- read.csv('2019_camden_theft_from_person.csv')\n\n# convert csv to sf object\ncamden_theft &lt;- st_as_sf(camden_theft, coords = c('X','Y'), crs = 27700)\n\n# inspect\ntm_shape(camden_oas) +\n  tm_polygons() +\ntm_shape(camden_theft) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\nThis is point data, but we are interested in the polygons and how this data relates to the administrative boundaries it is within. Let’s count the number of thefts in each OA. This is a spatial operation that is often called “point in polygon”. As we are just counting the number of occurrences in each polygon it is quite easy. In the future you may often want to aggregate over points for an area, or in reverse assign values from the polygon to the points.\n\n# thefts in Camden\ncamden_oas$n_thefts &lt;- lengths(st_intersects(camden_oas, camden_theft))\n\n# inspect\ntm_shape(camden_oas) +\n    tm_polygons(fill = \"n_thefts\", \n        fill.scale = tm_scale_intervals(breaks=c(0,1,50,100,150,200,250,300,350)),\n        fill.legend = tm_legend(title =\"Reported Thefts\"))\n\n\n\n\n\n\n\n\n\n\nYou can see our map is skewed by central London, meaning that the results in central London (the south of Camden) are so much larger than those in the north that it makes it harder to see the smaller differences between other areas. We’ll take the square root of the number of thefts to remedy this.\n\n# square root of thefts\ncamden_oas$sqrt_n_thefts &lt;- sqrt(camden_oas$n_thefts)\n\n# inspect\ntm_shape(camden_oas) +\n    tm_polygons(fill = \"sqrt_n_thefts\", \n        fill.scale = tm_scale_intervals(breaks=c(0,1,5,10,15,20)),\n        fill.legend = tm_legend(title =\"Reported Thefts (Squared)\"))\n\n\n\n\n\n\n\n\n\n\nThere: a slightly more nuanced picture\n\n\n2.3 Global Moran’s I\nWith a Global Moran’s I we can test how “random” the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a “random” distribution, and 1 is a “non-random” distribution of clearly defined clusters.\nTo calculate the Global Moran’s I you need an adjacency matrix that contains the information of whether or not an OA is next to another. For an even more nuanced view you can include distance, or a distance weighting in the matrix rather than just the TRUE or FALSE, to take into account the strength of the neighbourhoodness. Because of the way Moran’s I functions in R it is necessary to use the sp and spdep libraries (which we have loaded earlier). As you will see these methods and functions have quite esoteric and complicated syntax. Some of the operations they will do will be similar to the examples shown earlier, but the way they assign and store variables makes it much quicker to run complex spatial operations.\n\n# inspect\nclass(camden_oas)\n\n\n## [1] \"sf\"         \"data.frame\"\n\n\n# convert to sp\ncamden_oas_sp &lt;- as_Spatial(camden_oas, IDs=camden_oas$OA11CD)\n\n# inspect\nclass(camden_oas_sp)\n\n\n## [1] \"SpatialPolygonsDataFrame\"\n## attr(,\"package\")\n## [1] \"sp\"\n\nNow we can make the esoteric and timesaving “nb” object in which we store for each OA which other OAs are considered to be neighbours.\n\n# create an nb object\ncamden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD)\n\n# inspect\nclass(camden_oas_nb)\n\n\n## [1] \"nb\"\n\n\n# inspect\nstr(camden_oas_nb,list.len=10)\n\n\n## List of 749\n##  $ : int [1:7] 10 15 215 303 327 375 464\n##  $ : int [1:5] 19 72 309 365 430\n##  $ : int [1:3] 133 152 709\n##  $ : int [1:7] 78 131 152 286 314 582 651\n##  $ : int [1:5] 67 316 486 492 703\n##  $ : int [1:8] 7 68 317 487 556 612 625 638\n##  $ : int [1:3] 6 68 317\n##  $ : int [1:7] 57 58 164 358 429 605 684\n##  $ : int [1:5] 58 164 489 609 700\n##  $ : int [1:7] 1 215 245 311 327 366 644\n##   [list output truncated]\n##  - attr(*, \"class\")= chr \"nb\"\n##  - attr(*, \"region.id\")= chr [1:749] \"E00004395\" \"E00004314\" \"E00004578\" \"E00004579\" ...\n##  - attr(*, \"call\")= language poly2nb(pl = camden_oas_sp, row.names = camden_oas_sp$OA11CD)\n##  - attr(*, \"type\")= chr \"queen\"\n##  - attr(*, \"sym\")= logi TRUE\n\nNext, we need to assign weights to each neighbouring polygon. In our case, each neighbouring polygon will be assigned equal weight with style='W'. After this, we can calculate a value for the Global Moran’s I.\n\n# create the list weights object\nnb_weights_list &lt;- nb2listw(camden_oas_nb, style='W')\n\n# inspect\nclass(nb_weights_list)\n\n\n## [1] \"listw\" \"nb\"\n\n\n# Moran's I\nmi_value &lt;- moran(camden_oas_sp$n_thefts,nb_weights_list,n=length(nb_weights_list$neighbours),S0=Szero(nb_weights_list))\n\n# inspect\nmi_value\n\n\n## $I\n## [1] 0.4772137\n## \n## $K\n## [1] 75.21583\n\nThe Global Moran’s I seems to indicate that there is indeed some spatial autocorrelation in our data, however, this is just a quick way to check the score. To do so properly we need to compare our score a randomly distributed version of the variables. We can do this by using something called a Monte Carlo simulation.\n\n# run a Monte Carlo simulation 599 times\nmc_model &lt;- moran.mc(camden_oas_sp$n_thefts, nb_weights_list, nsim=599)\n\n# inspect\nmc_model\n\n\n## Monte-Carlo simulation of Moran I\n## \n## data:  camden_oas_sp$n_thefts \n## weights: nb_weights_list  \n## number of simulations + 1: 600 \n## \n## statistic = 0.47721, observed rank = 600, p-value = 0.001667\n## alternative hypothesis: greater\n\nThis model shows that our distribution of thefts differs significantly from a random distribution. As such, we can conclude that there is significant spatial autocorrelation in our theft data set.\n\n\n2.4 Local Moran’s I (or LISA)\nWith a measurement of local spatial autocorrelation we could find hotspots of theft that are surrounded by areas of much lower theft. According to the previous global statistic these are not randomly distributed pockets but would be outliers against the general trend of clusteredness! These could be areas that contain very specific locations, where interventions could be made that drastically reduce the rate of crime rather than other areas where there is a high level of ambient crime.\n\n# Local Moran's I\nlocal_moran_camden_oa_theft &lt;- localmoran(camden_oas_sp$n_thefts, nb_weights_list)\n\nRemember in the lecture the following conditions:\n\n\n\n\n\n\n\n\n\nTo properly utilise these local statistics and make an intuitively useful map, we need to combine them with our crime count variable. Because of the way the new variable will be calculated, we first need to rescale our variable so that the mean is 0.\n\n# rescale\ncamden_oas_sp$scale_n_thefts &lt;- scale(camden_oas_sp$n_thefts)\n\nTo compare this rescaled value against its neighbours, we subsequently need to create a new column that carries information about the neighbours. This is called a spatial lag function. The “lag” just refers to the fact you are comparing one observation against another, this can also be used between timed observations. In this case, the “lag” we are looking at is between neighbours.\n\n# create a spatial lag variable \ncamden_oas_sp$lag_scale_n_thefts &lt;- lag.listw(nb_weights_list, camden_oas_sp$scale_n_thefts)\n\nNow we have used sp for all it is worth it’s time to head back to the safety of sf() before exploring any forms of more localised patterns.\n\n# convert to sf\ncamden_oas_moran_stats &lt;- st_as_sf(camden_oas_sp)\n\nTo make a human readable version of the map we will generate some labels for our findings from the Local Moran’s I stats. This process calculates what the value of each polygon is compared to its neighbours and works out if they are similar or dissimilar and in which way, then gives them a text label to describe the relationship.\n\n# MAP 1: VERSION WITH FULL CLUSTER HH, HL, LH, LL DETAILS\n# classification without significance value\ncamden_oas_moran_stats$quad_non_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 & \n                                              camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, \n                                              'high-high', \n                                       ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 & \n                                              camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, \n                                              'low-low', \n                                       ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 & \n                                              camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, \n                                              'high-low', \n                                       ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 & \n                                              camden_oas_moran_stats$lag_scale_n_thefts &gt; 0,\n                                              'low-high',NA))))\n\n# MAP 2: VERSION WITH SIGNIFICANCE\n# set a significance value\nsig_level &lt;- 0.1\n# classification with significance value\ncamden_oas_moran_stats$quad_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 & \n                                          camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 & \n                                          local_moran_camden_oa_theft[,5] &lt;= sig_level, \n                                          'high-high', \n                                   ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 & \n                                          camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 & \n                                          local_moran_camden_oa_theft[,5] &lt;= sig_level, \n                                          'low-low', \n                                   ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 & \n                                          camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 & \n                                          local_moran_camden_oa_theft[,5] &lt;= sig_level, \n                                          'high-low', \n                                   ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 & \n                                          camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 & \n                                          local_moran_camden_oa_theft[,5] &lt;= sig_level, \n                                          'low-high',\n                                   ifelse(local_moran_camden_oa_theft[,5] &gt; sig_level, \n                                          'not-significant', \n                                          'not-significant')))))\n\nTo understand how this is working we can look at the data non-spatially. As we rescaled the data, our axes should split the data neatly into their different area vs spatial lag relationship categories. Let’s make the scatterplot using the scaled number of thefts for the areas in the x axis and their spatially lagged results in the y axis.\n\n# plot 1\n# plot the results without the statistical significance\nggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, \n                                   y = lag_scale_n_thefts, \n                                   color = quad_non_sig)) +\n  geom_vline(xintercept = 0) + # plot vertical line\n  geom_hline(yintercept = 0) + # plot horizontal line\n  xlab('Scaled Thefts (n)') +\n  ylab('Lagged Scaled Thefts (n)') +\n  labs(colour='Relative to neighbours') +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n# plot 2\n# plot the results with the statistical significance\nggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, \n                                   y = lag_scale_n_thefts, \n                                   color = quad_sig)) +\n  geom_vline(xintercept = 0) + # plot vertical line\n  geom_hline(yintercept = 0) + # plot horizontal line\n  xlab('Scaled Thefts (n)') +\n  ylab('Lagged Scaled Thefts (n)') +\n  labs(colour='Relative to neighbours') +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\nNow let’s see how they are arranged spatially.\n\n# map all of the results here\ntm_shape(camden_oas_moran_stats) +\n    tm_polygons(fill = \"quad_non_sig\", fill.scale = tm_scale_categorical(values = c(\"#de2d26\", \"#fee0d2\", \"#deebf7\", \"#3182bd\")))\n\n\n\n\n\n\n\n\n\n\n\n# map only the statistically significant results here\ntm_shape(camden_oas_moran_stats) +\n    tm_polygons(fill = \"quad_sig\", fill.scale = tm_scale_categorical(values = c(\"#de2d26\", \"#deebf7\", \"white\")))\n\n\n\n\n\n\n\n\n\n\nAs our data are so spatially clustered we can’t see any outlier places (once we have ignored the non-significant results). This suggests that the pattern of theft from persons is not highly concentrated in very small areas or particular Output Areas, and instead is spread on a larger scale than we have used here. To go further than we have today it would be possible to run the exact same code but using a larger scale, perhaps LSOA, or Ward, and compare how this changes the Moran’s I statistics globally and locally. Or, to gain statistical significance in looking at the difference between areas getting more data perhaps over a longer timescale, where there are less areas with 0 thefts.",
    "crumbs": [
      "Foundation & Theory",
      "Week 3: Spatial autocorrelation"
    ]
  },
  {
    "objectID": "05-spatial_autocorrelation.html#attributions",
    "href": "05-spatial_autocorrelation.html#attributions",
    "title": "Week 3: Spatial autocorrelation",
    "section": "3 Attributions",
    "text": "3 Attributions\nThis week’s practical uses content and inspiration from:\n\nLong, A. 2020. Spatial autocorrelation. [Source]\nDijk, J.V. 2021. Spatial autocorrelation. [Source]\nGimond, M. 2021. Spatial autocorrelation in R. [Source]",
    "crumbs": [
      "Foundation & Theory",
      "Week 3: Spatial autocorrelation"
    ]
  },
  {
    "objectID": "05-spatial_autocorrelation.html#references-see-reading-list",
    "href": "05-spatial_autocorrelation.html#references-see-reading-list",
    "title": "Week 3: Spatial autocorrelation",
    "section": "4 References (see reading list)",
    "text": "4 References (see reading list)\n\nGitbook: [R Programming] Gimond, M. (2021) Intro to GIS and spatial analysis. Chapter 13: Spatial autocorrelation Click link\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & systems (4th Edition); Chapter 2: The Nature of Geographic Data Click link\nBook: [Theory] Longley, P. _*et al_ (2015) Geographic Information Science & systems (4th Edition); Chapter 13: Spatial Analysis Click link\nPaper: [Research] Radil, S. 2016. Spatial analysis of crime. In: Huebner, B. and Bynum, T. The Handbook of Measurement Issues in Criminology and Criminal Justice, Chapter 24, pp.536-554. Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital paper)",
    "crumbs": [
      "Foundation & Theory",
      "Week 3: Spatial autocorrelation"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html",
    "href": "03-spatial_analysis_for_data_science.html",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "",
    "text": "The goal for this week’s session is to get you started with using RStudio, and being familiar with its environment. The session aims to introduce you to the basic programming etiquette, as well as building confidence for using RStudio as a GIS tool. At the end of this session, you should be able to perform some basic data managing tasks as well as generate a simple choropleth map in RStudio.\n\n\nThe first task includes getting you started with RStudio by installing the needed software(s) (i.e., RStudio and R) on to your personal laptop, and becoming familiar with its environment and panels. We will begin a soft introduction on the basics of managing data in RStudio. This includes learning how to create various objects in RStudio such as vector and data frame objects. The crucial part of this session we be to know how to the set working directories as well as import your dataset in RStudio. Finally, we will learn how to perform the basic visualisation of spatial data in RStudio.\nLet us begin.\n\n\n\nYou should by now have opened RStudio on your laptop. When opening RStudio for the first time, you are greeted with its interface. The window is split into three panels: 1.) R Console, 2.) Environments and 3.) Files, help & Output.\n\n\n\n\n\n\n\n\n\n\nPanel 1: The Console lets the user type in R-codes to perform quick commands and basic calculations.\nPanel 2: The Environments lets the user see which datasets, spatial objects and other files are currently stored in RStudio’s memory\nPanel 3: Under the File tab, it lets the user access other folders stored in the computer to open datasets. Under the Help tab, it also allows the user to view the help menu for codes and commands. Finally, under the Plots tab, the user can perusal his/her generated plots (e.g., histogram, scatterplot, maps etc.).\n\nThe above section is the Menu Bar. You can access other functions for saving, editing, and opening a new Script File for writing codes. Opening a new Script File will reveal a fourth panel above the Console.\nYou can open a Script File by:\n\nClicking on the File tab listed inside the Menu Bar. A scroll down bar will reveal itself. Here, you can scroll to the section that says New File.\nUnder New File, click on R Script. This should open a new Script File titled “Untitled 1”.\n\n\nImportant Notes: Throughout the course, and in all practical tutorials, you will be encouraged to use an R Script for collating and saving the codes you have written for carrying out spatial analysis. However, we will start writing codes in a script later when we reach to section 1.3 of this tutorial. For now, let us start with the absolute basics, which is interacting with the R Console and using it as a basic calculator for typing simple code.\n\n\n\n\nThe R console window (i.e., Panel 1) is the place where RStudio is waiting for you to tell it what to do. It will show the code you have commanded RStudio to execute, and it will also show the results from that command. You can type the commands directly into the window for execution as well.\nLet us start by using the console window as a basic calculator for typing in addition (+), subtraction (-), multiplication (*), division (/), exponents (^) and performing other complex sums.\nClick inside the R Console window and type 19+8, and press enter key button ↵ to get your answer. Quickly perform the following maths by typing them inside the R Console window:\n\n# Perform addition\n19+8\n\n# Perform subtraction\n20-89\n\n# Perform multiplication\n18*20\n\n# Perform division\n27/3\n\n# To number to a power e.g., 2 raise to the power of 8\n2^8\n\n# Perform complex sums\n(5*(170-3.405)/91)+1002\n\nAside from basic arithmetic operations, we can use some basic mathematical functions such as the exponential and logarithms:\n\nexp() is the exponential function\nlog() is the logarithmic function\n\nDo not worry at all about these functions as you will use them later in the weeks to come for transforming variables. Perform the following by typing them inside the R Console window:\n\n# use exp() to apply an exponential to a value\nexp(5) \n\n# use log() to transforrm a value on to a logarithm scale\nlog(3)\n\n\n\n\nNow that we are familiar with using the console as a calculator. Let us build from this and learn one of the most important codes in RStudio which is called the Assignment Operator.\nThis arrow symbol &lt;- is called the Assignment Operator. It is typed by pressing the less than symbol key &lt; followed by the hyphen symbol key -. It allows the user to assign values to an Object in R.\nObjects are defined as stored quantities in RStudio’s environment. These objects can be assigned anything from numeric values to character string values. For instance, say we want to create a numeric object called x and assign it with a value of 3. We do this by typing x &lt;- 3. When you enter the object x in the console and press enter ↵, it will return the numeric value 3.\nAnother example, suppose we want to create a string object called y and assign it with some text \"Hello!\". We do this typing y &lt;- \"Hello!\". When you enter y in console, it will return the text value Hello.\nLet us create the objects a,b, c, and d and assign them with numeric values. Perform the following by typing them inside the R Console window:\n\n# Create an object called 'a' and assign the value 17 to it\na &lt;- 17\n# Type the object 'a' in console as a command to return value 17\na\n\n# Create an object called 'b' and assign the value 10 to it\nb &lt;- 10\n# Type the object 'b' in console as a command to return value 10\nb\n\n# Create an object called 'c' and assign the value 9 to it\nc &lt;- 9\n# Type the object 'c' in console as a command to return value 9\nc\n\n# Create an object called 'd' and assign the value 8 to it\nd &lt;- 8\n# Type the object 'd' in console as a command to return value 8\nd\n\nNotice how the objects a, b, c and d and its value are stored in RStudio’s environment panel. We can perform the following arithmetic operations with these object values:\n\n# type the following and return an answer\n(a + b + c + d)/5\n\n# type the following and return an answer\n(5*(a-c)/d)^2\n\nLet us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data you will need to cover them with quotation marks \"...\". For example, say we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\".\nTry these examples of assigning the following character text to an object:\n\n# Create an object called 'e' and assign the character string \"RStudio\"\ne &lt;- \"RStudio\"\n# Type the object 'e' in the console as a command to return \"RStudio\"\ne\n\n# Create an object called 'f', assign character string \"Hello world\" \nf &lt;- \"Hello world\"\n\n# Type the object 'f' in the console as a command to return \"Hello world\"\nf\n\n# Create an object called 'g' and assign \"Blade Runner is amazing\"\ng &lt;- \"Blade Runner is amazing\"\n# Type the object 'g' in the console to return the result\ng\n\nWe are now familiar with using the console and assigning values (i.e., numeric and string values) to objects. The parts covered here are the initial steps and building blocks for coding and creating datasets in RStudio.\nLet us progress to section 1.3. Here is where the serious stuff start. We will learn the basics of managing data and some coding etiquette - this includes creating data frames, importing & exporting spreadsheets, setting up work directories, column manipulations and merging two data frames. Learning these basic tasks are key for managing data in RStudio.\n\nPoint of no return: From here on out - let us open a script file and type codes there instead of the Console. We are getting serious now, we will never use the Console again.",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html#introduction",
    "href": "03-spatial_analysis_for_data_science.html#introduction",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "",
    "text": "The goal for this week’s session is to get you started with using RStudio, and being familiar with its environment. The session aims to introduce you to the basic programming etiquette, as well as building confidence for using RStudio as a GIS tool. At the end of this session, you should be able to perform some basic data managing tasks as well as generate a simple choropleth map in RStudio.\n\n\nThe first task includes getting you started with RStudio by installing the needed software(s) (i.e., RStudio and R) on to your personal laptop, and becoming familiar with its environment and panels. We will begin a soft introduction on the basics of managing data in RStudio. This includes learning how to create various objects in RStudio such as vector and data frame objects. The crucial part of this session we be to know how to the set working directories as well as import your dataset in RStudio. Finally, we will learn how to perform the basic visualisation of spatial data in RStudio.\nLet us begin.\n\n\n\nYou should by now have opened RStudio on your laptop. When opening RStudio for the first time, you are greeted with its interface. The window is split into three panels: 1.) R Console, 2.) Environments and 3.) Files, help & Output.\n\n\n\n\n\n\n\n\n\n\nPanel 1: The Console lets the user type in R-codes to perform quick commands and basic calculations.\nPanel 2: The Environments lets the user see which datasets, spatial objects and other files are currently stored in RStudio’s memory\nPanel 3: Under the File tab, it lets the user access other folders stored in the computer to open datasets. Under the Help tab, it also allows the user to view the help menu for codes and commands. Finally, under the Plots tab, the user can perusal his/her generated plots (e.g., histogram, scatterplot, maps etc.).\n\nThe above section is the Menu Bar. You can access other functions for saving, editing, and opening a new Script File for writing codes. Opening a new Script File will reveal a fourth panel above the Console.\nYou can open a Script File by:\n\nClicking on the File tab listed inside the Menu Bar. A scroll down bar will reveal itself. Here, you can scroll to the section that says New File.\nUnder New File, click on R Script. This should open a new Script File titled “Untitled 1”.\n\n\nImportant Notes: Throughout the course, and in all practical tutorials, you will be encouraged to use an R Script for collating and saving the codes you have written for carrying out spatial analysis. However, we will start writing codes in a script later when we reach to section 1.3 of this tutorial. For now, let us start with the absolute basics, which is interacting with the R Console and using it as a basic calculator for typing simple code.\n\n\n\n\nThe R console window (i.e., Panel 1) is the place where RStudio is waiting for you to tell it what to do. It will show the code you have commanded RStudio to execute, and it will also show the results from that command. You can type the commands directly into the window for execution as well.\nLet us start by using the console window as a basic calculator for typing in addition (+), subtraction (-), multiplication (*), division (/), exponents (^) and performing other complex sums.\nClick inside the R Console window and type 19+8, and press enter key button ↵ to get your answer. Quickly perform the following maths by typing them inside the R Console window:\n\n# Perform addition\n19+8\n\n# Perform subtraction\n20-89\n\n# Perform multiplication\n18*20\n\n# Perform division\n27/3\n\n# To number to a power e.g., 2 raise to the power of 8\n2^8\n\n# Perform complex sums\n(5*(170-3.405)/91)+1002\n\nAside from basic arithmetic operations, we can use some basic mathematical functions such as the exponential and logarithms:\n\nexp() is the exponential function\nlog() is the logarithmic function\n\nDo not worry at all about these functions as you will use them later in the weeks to come for transforming variables. Perform the following by typing them inside the R Console window:\n\n# use exp() to apply an exponential to a value\nexp(5) \n\n# use log() to transforrm a value on to a logarithm scale\nlog(3)\n\n\n\n\nNow that we are familiar with using the console as a calculator. Let us build from this and learn one of the most important codes in RStudio which is called the Assignment Operator.\nThis arrow symbol &lt;- is called the Assignment Operator. It is typed by pressing the less than symbol key &lt; followed by the hyphen symbol key -. It allows the user to assign values to an Object in R.\nObjects are defined as stored quantities in RStudio’s environment. These objects can be assigned anything from numeric values to character string values. For instance, say we want to create a numeric object called x and assign it with a value of 3. We do this by typing x &lt;- 3. When you enter the object x in the console and press enter ↵, it will return the numeric value 3.\nAnother example, suppose we want to create a string object called y and assign it with some text \"Hello!\". We do this typing y &lt;- \"Hello!\". When you enter y in console, it will return the text value Hello.\nLet us create the objects a,b, c, and d and assign them with numeric values. Perform the following by typing them inside the R Console window:\n\n# Create an object called 'a' and assign the value 17 to it\na &lt;- 17\n# Type the object 'a' in console as a command to return value 17\na\n\n# Create an object called 'b' and assign the value 10 to it\nb &lt;- 10\n# Type the object 'b' in console as a command to return value 10\nb\n\n# Create an object called 'c' and assign the value 9 to it\nc &lt;- 9\n# Type the object 'c' in console as a command to return value 9\nc\n\n# Create an object called 'd' and assign the value 8 to it\nd &lt;- 8\n# Type the object 'd' in console as a command to return value 8\nd\n\nNotice how the objects a, b, c and d and its value are stored in RStudio’s environment panel. We can perform the following arithmetic operations with these object values:\n\n# type the following and return an answer\n(a + b + c + d)/5\n\n# type the following and return an answer\n(5*(a-c)/d)^2\n\nLet us create more objects but this time we will assign character string(s) to them. Please note that when typing a string of characters as data you will need to cover them with quotation marks \"...\". For example, say we want to create a string object called y and assign it with some text \"Hello!\". We do this by typing y &lt;- \"Hello!\".\nTry these examples of assigning the following character text to an object:\n\n# Create an object called 'e' and assign the character string \"RStudio\"\ne &lt;- \"RStudio\"\n# Type the object 'e' in the console as a command to return \"RStudio\"\ne\n\n# Create an object called 'f', assign character string \"Hello world\" \nf &lt;- \"Hello world\"\n\n# Type the object 'f' in the console as a command to return \"Hello world\"\nf\n\n# Create an object called 'g' and assign \"Blade Runner is amazing\"\ng &lt;- \"Blade Runner is amazing\"\n# Type the object 'g' in the console to return the result\ng\n\nWe are now familiar with using the console and assigning values (i.e., numeric and string values) to objects. The parts covered here are the initial steps and building blocks for coding and creating datasets in RStudio.\nLet us progress to section 1.3. Here is where the serious stuff start. We will learn the basics of managing data and some coding etiquette - this includes creating data frames, importing & exporting spreadsheets, setting up work directories, column manipulations and merging two data frames. Learning these basic tasks are key for managing data in RStudio.\n\nPoint of no return: From here on out - let us open a script file and type codes there instead of the Console. We are getting serious now, we will never use the Console again.",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html#basics-of-managing-data-in-rstudio",
    "href": "03-spatial_analysis_for_data_science.html#basics-of-managing-data-in-rstudio",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "2 Basics of managing data in RStudio",
    "text": "2 Basics of managing data in RStudio\n\n2.1 How do we enter data into RStudio?\nAs you have already seen, RStudio is an object-oriented software package and so entering data is slightly different for the usual way of inputting information into a spreadsheet (e.g., Microsoft Excel). Here, you will need to enter the information as a Vector object before combining them into a Data Frame object.\nConsider this crude example of data containing the additional health information for 4 people. It contains the variable (or column) names ‘id’, ‘name’, ‘height’, ‘weight’ and ‘gender’\n\n\n\nid\nname\nheight\nweight\ngender\n\n\n\n\n1\nKofi\n1.65\n64.2\nM\n\n\n2\nHarry\n1.77\n80.3\nM\n\n\n3\nHuijun\n1.70\n58.7\nF\n\n\n4\nFatima\n1.68\n75.0\nF\n\n\n\nNow, when entering data to RStudio it is not like Microsoft Excel where we enter data into the cells of a spreadsheet. In RStudio, data is entered as a sequence of elements and listed inside an object called a vector. For instance, if we have three age values of 12, 57 and 26 years, and we want to enter this in RStudio, we need to use the combine function c() and combine these three elements into a vector object. Hence, the code will be c(12, 57, 26). We can assign this data by typing this code as age &lt;- c(12, 57, 26). Any time you type ‘age’ into RStudio console it will hence return these three values unless you chose to overwrite it with different information.\nLet us look at this more closely with the 'id' variable in the above data. Each person has an ID number from 1 to 4. We are going to list the numbers 1, 2, 3 and 4 as a sequence of elements into a vector using the combine function c() and then assign it to as a vector object calling it 'id'.\n\n# Create 'id' vector object \nid &lt;- c(1, 2, 3, 4)\n\n# Type the vector object 'id' in console to see output\nid\n\nNow, let us enter the information the same way for the remaining columns for ‘name’, ‘height’, ‘weight’ and ‘gender’ like we did for ‘id’:\n\n# Create 'name' vector object\nname &lt;- c(\"Kofi\", \"Harry\", \"Huijun\", \"Fatima\")\n\n# Create 'height' (in meters) vector object\nheight &lt;- c(1.65, 1.77, 1.70, 1.68)\n\n# Create 'weight' (in kg) vector object\nweight &lt;- c(64.2, 80.3, 58.7, 75.0)\n\n# Create 'gender' vector object\ngender &lt;- c(\"M\", \"M\", \"F\", \"F\")\n\nNow, that we have the vector objects ready. Let us bring them together to create a proper dataset. This new object is called a Data frame. We need to list the vectors inside the data.frame() function.\n\n# Create a dataset (data frame)\ndataset &lt;- data.frame(id, name, height, weight, gender)\n\n# Type the data frame object 'dataset' in console to see output\ndataset\n\n# You can also see dataset in a data viewer, type View() to data:\nView(dataset)\n\n\n\n\n\n\n\nNote\n\n\n\nThe column ‘id’ is a numeric variable with integers. The second column ‘name’ is a text variable with strings. The third & fourth columns ‘height’ and ‘weight’ are examples of numeric variables with real numbers with continuous measures. The variable ‘gender’ is a text variable with strings – however, this type of variable is classed as a categorical variable as individuals were categorised as either ‘M’ and ‘F’.\n\n\n\n\n2.2 How do we create a variable based on other existing variables in our data frame?\nTo access a variable by its name within a data frame, you will need to first type the name of the data frame followed by a $ (dollar sign), and then typing the variable’s name of interest. For instance, suppose you just want to see the height values in the Console viewer - you just type:\n\n# to access height - you need to type 'dataset$height'\ndataset$height\n\nWe can use other columns or variables within our data frame to create another variable. This technique is essentially important when cleaning and managing data. From this dataset, it is possible to derive the body mass index bmi from height and weight using the formula:\n\n\n\\(BMI = weight/height^2\\)\n\n\nTo generate bmi into our data frame, we would need to access the height (m) and weight (kg) columns using the $ from the data frame its stored to, and apply the above formula as a code to generate the new bmi column:\n\n# Create 'bmi' in the data frame i.e.,'dataset' and calculate 'bmi'\n# using the $weight and $height\ndataset$bmi &lt;- dataset$weight/((dataset$height)^2)\n# View the data frame ‘dataset’ and you will see the new bmi variable inside\nView(dataset)\n\nYou can overwrite the height (m) column to change its units into centimeters by multiplying it to 100; equally, the weight (kg) column can be overwritten and converted from units of kilograms to grams by multiplying it to 1000.\n\n# using $height and *100 \ndataset$height &lt;- dataset$height*100\n# using $weight and *100\ndataset$weight &lt;- dataset$weight*1000\n# use View() the data frame ‘dataset’ and you will see the updated variables\nView(dataset)\n\n\n\n2.3 How do we set the working directory in our computer by connecting our folder to RStudio with the setwd() function?\nNow, we are getting very serious here!\n\n\n\n\n\n\nImportant\n\n\n\nBefore we do anything - make sure to have downloaded the data set for week 1 if you haven’t done so by clicking here. In your computer, create a new folder on your desktop page and rename the folder to “GEOG0114”, and create another folder within “GEOG0114” and rename it as “Week 1”. Make sure to unzip and transfer ALL the downloaded data directly to the Week 1 folder.\n\n\nNow, this part of the practicals are probably the most important section of this tutorial. It’s usually the “make” or “break” phase (i.e., you ending up loving RStudio OR you hating it and not ever wanting to pick up R again).\nWe are going to learn how to set-up a working directory. This basically refers to us connecting the RStudio to the folder containing our dataset. It allows the user to tell RStudio to open data from a folder once it knows the path location. The path location specifies the whereabouts of the data file(s) stored within a computer. Setting your directory in RStudio beforehand makes life incredibly easier in terms of finding, importing, exporting and saving data in and out of RStudio.\nTo illustrate what a path location is – suppose on my desktop (mac/widows) there is a folder called “GEOG0114”, and within that folder, exists another folder called “Week 1”. Finally, suppose a comma separated value (.csv) data file called “London_LSOA_FireHazards.csv” is store in this last folder i.e., Week 1. If via RStudio you want to open this CSV data file located in within the “Week 1” folder. You will need to first set the path to “Week 1” in RStudio using the setwd() function.\nTherefore, the path location to this folder on a Windows machine would be written as follows, \"C:/Users/accountName/Desktop/GEOG0114/Week 1\". You can access this piece of information simply by:\n\nOpen the GEOG0114 folder to reveal the Week 1 folder.\nOpen the Week 1 folder in the data files are stored.\nNow, click on the bar at the top which shows GEOG0114 &gt; Week 1. This should highlight and show \"C:\\Users\\accountName\\Desktop\\GEOG0114\\Week 1\" (see image below):\n\n\n\n\n\n\n\n\n\n\n\nNow, copy \"C:\\Users\\accountName\\Desktop\\GEOG0114\\Week 1\" and paste the path name into the setwd() function in your R script.\nLastly, change all the back slashes \\ in the path name to forward slashes / and run the code. It should look like this: setwd(\"C:/Users/accountName/Desktop/GEOG0114/Week 1\").\n\nFor Windows, the setwd() is as follows:\n\n# set work directory in windows\nsetwd(\"C:/Users/accountName/Desktop/GEOG0114/Week 1\")\n\nFor MAC users, its marginally different. The path location would be written as follows, \"/Users/accountName/Desktop/GEOG0114/Week 1\". You can access this piece of information simply by:\n\nRight-clicking on the folder “Week 1” (not file) in which the files are stored.\nHold the “Option” ⌥ key down\n\n\n\n\n\n\n\n\n\n\n\nClick Copy \"filename\" as Pathname\nPaste the copied path name into the function setwd() and run the code\n\nFor Mac, the setwd() is as follows:\n\n# set work directory in macs\nsetwd(\"/Users/accountName/Desktop/GEOG0114/Week 1\")\n\nThis should set the working directory. Now, let us learn how to import a CSV data into RStudio.\n\n\n2.4 How do we import, merge and export CSV data with RStudio?\n\n2.4.1 Importing data using read.csv()\nAs you will be working mostly with comma separated value formatted data (i.e., csv) we will therefore learn how to import and export in RStudio. There are two files that we are going to import into RStudio from Week 1’s folder:\n\nLondon_LSOA_Deprivation_2019.csv which contains information on seven indicators (as scores) and an IMD ranking for socioeconomic deprivation for 4,836 postcodes across London in 2019.\nLondon_LSOA_FireHazards_2019.csv which contains details about the observed and estimated expected number of fire-related accidents to have occurred in residential premises with the overall number of houses in 4,836 postcodes across London in 2019.\n\nTo import a csv into RStudio, we use the read.csv() function. To demonstrate this, let us import the data for fires into an data frame object and name it as Fire_data\n\n# Import data using read.csv() function \nFire_data &lt;- read.csv(file=\"London_LSOA_FireHazards_2019.csv\", header = TRUE, sep = \",\")\n\nJust in case…suppose if we did NOT set the working directory earlier. We would have to go through the hassle of typing the path location in the read.csv().\nFor windows:\n\nFire_data &lt;- read.csv(file=\"C:/Users/accountName/Desktop/GEOG0114/Week 1/London_LSOA_FireHazards_2019.csv\", header = TRUE, sep = \",\")\n\nFor Mac:\n\nFire_data &lt;- read.csv(file=\"/Users/accountName/Desktop/GEOG0114/Week 1/London_LSOA_FireHazards_2019.csv\", header = TRUE, sep = \",\")\n\nI do not recommend doing it this way. Just set the work directory with setwd() to make life easier for yourself.\n\n\n\n\n\n\nImportant\n\n\n\nThe arguments used in read.csv() function – 1.) ‘file =’ is a mandatory option where you quote the name of the file to be imported; 2.) ‘header = TRUE’ option is set to TRUE which is telling RStudio that the file that is about to be imported has column names on the first row so it should not treat as observations; and 3.) ‘sep = \",\"’ we are telling RStudio that the format of the dataset is comma separated.\n\n\nWe have imported the fire hazards data. Now, let us import the second data for deprivation using the read.csv() function and call it Deprivation_data. The code would look something as follows:\n\n# Import data using read.csv() function \nDeprivation_data &lt;- read.csv(file=\"London_LSOA_Deprivation_2019.csv\", header = TRUE, sep = \",\")\n\n# Show viewer the data sets\nView(Deprivation_data)\nView(Fire_data)\n\n\n\n2.4.2 Joining two datasets by merger using the merge() function\nYou will often find yourself merging two or more data frames together, especially bringing together a spatial object with a non-spatial object. We cannot stress the importance of merging objects in the correct order so that the spatial attributes are preserved.\nIn this instance, we are just dealing with two ordinary dataset which are non-spatial - i.e., one data set contains information on Fires that occurred in geographic units in London know as Lower Super Output Areas (LSOA), while the other contains its corresponding information for socioeconomic indicators that describe the LSOA. Hence, it is possible to merge the two data frames uniquely using a common key variable like LSOA_code.\n\n\n\n\n\n\n\n\n\nThis task can be done using the merge function merge(). Consequently, we want the format of the merge code to look something akin to this syntax merge(target_object, selected_object, by=”LSOA”).\nMerging data frames is indeed a very important technique to know especially if you need to bring together event information with no spatial dimension with actual spatial data. Alright, let’s merge the deprivation data on the fire records using the LSOA_code column, and generate a bigger data frame that contains both the fire and socioeconomic information:\n\n# Using the merge() function \nFull_data &lt;- merge(Fire_data, Deprivation_data, by.x = “LSOA_code”, by.y = “LSOA_code”, all.x = TRUE)\n\n# View the datasets\nView(Full_data)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe arguments used in merge.csv():\n\nFire_data is the target data frame we want something to be merged on to.\nDeprivation_data is the selected data frame we are using to merge with the Fire_data.\nby.x = \"LSOA_code\" option we are specifying the name of the join column from the target data frame i.e., Fire_data.\nby.y = \"LSOA_code\" option we are specifying the name of the join column from the selected data frame i.e., Deprivation_data\nall.x=TRUE option we are telling RStudio to retain all rows that are originally from the target data after merging regardless of whether or not they are present in the selected data frame. So even if a row from the selected data does not find a unique link with any of the rows in target data to match too - it will still preserve the target data frame by not discarding unlinked rows. But it will discard the unmatched rows from the selected data frame.\n\n\n\n\n\n2.4.3 Saving your dataset using the write.csv() function\nLet us save a version of this as a .csv file as a saved product named “London_Fire_and_Deprivation_2019.csv”. This can be done by using the write.csv() function. It will export the data frame object into a .csv format.\n\n# Export ‘Full_data’ object as .csv into 'Week 1' folder\nwrite.csv(Full_data, file = \"London_Fire_and_Deprivation_2019.csv\", row.names = FALSE)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe arguments used in merge.csv():\n\nFull_data is an object we are exporting. It is compulsory to specify the object data frame we want to export\nfile = is a mandatory argument. We must give a name to the file we wish to export it as with .csv extension.\nrow.names = this is an annoying argument! It will automatically index the dataset with unique row numbers by default if we do not specify FALSE! Since the data has its own unique identifiers (i.e., LSOA_code) we specify ‘FALSE’ to not perform this action of indexing\n\n\n\nAgain, suppose if you did NOT set the work directory to your folder, you will have to type the whole path location to where you want the data to be exported which could be a hassle:\nFor Windows:\n\nwrite.csv(Full_data, file = \"C:/Users/accountName/Desktop/GEOG0114/Week 1/London_Fire_and_Deprivation_2019.csv\", row.names = FALSE)\n\nFor Mac:\n\nwrite.csv(Full_data, file = \"/Users/accountName/Desktop/GEOG0114/Week 1/London_Fire_and_Deprivation_2019.csv\", row.names = FALSE)\n\nAgain, I do not recommend doing it this way. Just set the work directory with setwd() to make life easier for yourself and to avoid R calling you out for errors.\nNow that we have learned a lot of the basic things in RStudio – the stuff shown in section 1.3 will be used quite a lot in future tutorials. Let us progress to the section 1.4 where will start using RStudio as a GIS software. Here, we will create our first map to display the spatial distribution of socioeconomic deprivation using the Full_data data frame object.",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html#basic-visualisation-of-spatial-data-in-rstudio",
    "href": "03-spatial_analysis_for_data_science.html#basic-visualisation-of-spatial-data-in-rstudio",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "3 Basic visualisation of spatial data in RStudio",
    "text": "3 Basic visualisation of spatial data in RStudio\n\n3.1 Installing packages into RStudio\nSo far, we have been using functions and commands that are by default built-in RStudio. As you will become more and more proficient in RStudio, you will come to realise that there are several functions in RStudio that are in fact not built-in by default which will require external installation.\nFor instance, the sf package which is called Simply Features allows the user to load shapefiles (a type of Vector spatial data) into RStudio’s memory. Another important package is called tmap, this package gives access to various functions that allows the user to write code and emulate RStudio as a GIS software. These are examples of packages with enables mapping of spatial data. They need to be installed as they not built-in programs in RStudio.\nLet us install following packages: tmap and sf using the install.packages() function, and then activate the packages to make them active using the library() function.\nFirst install remotes, tmap and sf packages.:\n\ninstall.packages(\"sf\")\ninstall.packages(\"tmap\")\n\nOnce the installation is complete, you MUST activate the packages using the library() function. Type the following to perform this action:\n\n# Active the sf and tmap packages\nlibrary(\"sf\")\nlibrary(\"tmap\")\n\n\n\n3.2 Adding shapefiles in RStudio\nThe sf package grants the user access to a function called read_sf() to read-in shapefiles into RStudio. A shapefile typically contains the geometry of the spatial features e.g., points, line segment and boundaries of an areal feature etc. The shapefile has the extension of .shp (and it is always accompanied by its other supporting files with extensions .shx, .prj, .dbf and .cpg).\n\n\n\n\n\n\nWarning\n\n\n\nFollowing files must all be stored in the same folder location with .shp file (i.e., .shx, .prj, .dbf and .cpg). If one is missing - the .shp file will not work!\n\n\nWe have two types of shapefiles:\n\nLondon_LSOA_Areas.shp which contains the spatial boundaries of all LSOA postcodes in London.\nLondon_Borough_Shapefile.shp which contains the spatial boundaries for all 33 Boroughs in London.\n\nWe can easily load them in RStudio as Spatial Polygon objects, type into your script:\n\n# Add shapefiles for LSOA and Boroughs\nlondon_lsoa_shapefile &lt;- read_sf(\"London_LSOA_areas.shp\")\nlondon_borough_shapefile &lt;- read_sf(\"London_Boroughs_Shapefile.shp\")\n\nWe interested in visualising the data at an LSOA-level. We would need to merge the non-spatial (aka ordinary) data i.e., Full_data, uniquely in to the Spatial Polygon object we loaded in previously i.e., london_lsoa_shapefile, using the LSOA_code column.\nAgain, we can use the merge() function to perform this task the same way we carried out the joining of the two ordinary data frames in section 1.3.4.2. Consequently, we want the format of the merge code to look something akin to this syntax merge(target_spatial_object, selected_non_spatial_object, by=\"LSOA\").\n\n# Using the merge() function \nSpatial_data &lt;- merge(london_lsoa_shapefile, Full_data, by.x = \"LSOA_code\", by.y = \"LSOA_code\", all.x = TRUE)\n\n# View the datasets\nView(Spatial_data)\n\n\n\n3.3 Mapping with ‘tmap’ functions in RStudio\nThe tmap is the best package for creating maps in RStudio – it’s easy to code and user friendly. Let’s finally start some mapping! Here are some basic ‘tmap’ functions to be very familiar with:\n\n\n\n\n\n\n\ntmap functions\nWhat it does…\n\n\n\n\ntm_shape()\nThis allows the user to add layers to be mapped\n\n\ntm_polygons()\nThis deal with vector type dataset specifically. This allows the user to specify which column in the vector layer to be mapped using the fill = argument. Within the tm_polygon(), it also allows the user to apply the appropriate customisation to the vector layer. Examples of such arguments in 1.) fill_alpha = to modify the transparency; 2.) col_alpha = to modify the transparency of the borders; 3.) col = controls the colour of the line; 4.) fill.scale = controls the appearence and set the colour scheme of the scale that appears inside the legends block. Please note that if your data is continuous, then the appropriate argument to use in the fill.scale = would be either tm_scale_continuous() or tm_scale_interval() for breaking the continuous values into intervals. If they are numerical but discrete (counts) use tm_scale_discrete(). Otherwise, if it is categorical use tm_scale_categorical(); and 4.) fill.legend = tm_legend(...) allows the user to control the presentation of the legend. There are many arguments to experiment with - you can type ?tm_polygons() in console to see them. Please note that the tm_polygons() is immediately followed after specifying the tm_shape() function.\n\n\ntm_layout()\nThis allows the user to make heavy customisations to the main title, legends and other cosmetics to text sizes etc.\n\n\ntm_compass()\nThis allows the user to add a compass visual to the map output\n\n\ntm_scale_bar()\nThis allows the user to add a scale bar to the map output\n\n\n\n\n3.3.1 Visualising the outline(s) of study area\nSuppose you want to visual just the outline of London’s LSOA only:\n\n# Visual outline of London’s LSOA postcodes only\ntm_shape(Spatial_data) + tm_polygons()\n\n# Insert the “Spatial_data” object into the command line of \n# tm_shape(). No customisation has been applied here.\n\n\n\n\n\n\n\n\n\n\nYou can customise the level of transparency for both the area and borders by adding some arguments in the tm_polygon() [i.e., fill_alpha, and col_alpha which only take values between 0 (full transparency) to 1 (100% solid)]. For example:\n\n# Controlling transparencies for borders and areas\n\ntm_shape(Spatial_data) + \n    tm_polygons(fill_alpha = 0.1, col_alpha = 0.4)\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Adding another layer on top of study area\nSuppose you want to add another layer to show the regions (i.e., Borough) of London for which the LSOA areas reside in, you can use another tm_shape() in the code, the coding would be as follows:\n\n# Adding another layer\n\ntm_shape(Spatial_data) + \n    tm_polygons(fill_alpha = 0.1, col_alpha = 0.4) +\ntm_shape(london_borough_shapefile) +\n    tm_polygons(fill_alpha = 0, col_alpha = 1, col = \"black\")\n\n# The background of the added layer (london_borough_shapefile) has been rendered \n# to full transparency with the fill_alpha set to 0 and it borders are fully solid \n# using col_alpha set to 1 and col (colour) set to “black” to appear pronounced.\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Full visualising of data in the maps\nSuppose you want to visual the spatial data in those postcodes, we can use the tm_polygons() function. Let us visual the IMD_Decile column which has DISCRETE (or CATEGORICAL) values classed from 1 to 10 to signify areas that are deprived (with 1 being the poorest) and those that are wealthier (with 10 being the least deprived).\nThe coding would be as follows:\n\n# Generate map IMD_Deciles\n\ntm_shape(Spatial_data) +\n    tm_polygons(\n        fill = \"IMD_Decile\", \n        fill.scale = tm_scale_categorical(values = \"brewer.rd_yl_gn\"),\n        fill.legend = tm_legend(title = \"Deprivation (Deciles)\", frame = FALSE, item.space = -0.2, position = tm_pos_out()),\n        fill_alpha = 1, \n        col_alpha = 0.5, \n        col = \"black\", \n        lwd = 0.1) +\n    tm_shape(london_borough_shapefile) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \n    tm_scalebar(position = c(\"left\", \"bottom\"))\n\nIn the tm_polygons(), we insert the variable of interest we want to visual at the fill = argument. The fill.scale = option allows us to select the appropriate fill scale colour scheme depending on the type of dataset we are using - here it is categorical, so we use tm_scale_categorical(). The fill_legend() allows us to apply the appropriate customisation to the legends.\n\n\n\n\n\n\nNote\n\n\n\nGeography 101 – when creating a map, it is always best to add the North compass and scale bar. This is done with the tm_compass() and tm_scalebar() functions. The tm_layout() allows you to make further customisations to the internal and external plot region of map such as turning of the plot frame or modifying the text size in output. You can experiment with them by checking the help menu – just type: ?tm_layout() in the console.\n\n\n\n\n\n\n\n\n\n\n\nThis concludes this computer lab session. Challenge yourself with the task placed in section 1.6",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html#references",
    "href": "03-spatial_analysis_for_data_science.html#references",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "4 References",
    "text": "4 References\n\n4.1 Recommended reading (see reading list)\n\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & Systems (4th Edition); Chapters 2: The Nature of Geographic Data Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book)\nBook: [Theory] Longley, P. et al (2015) Geographic Information Science & Systems (4th Edition); Chapters 3: Representing Geography Click link (Note: use of your UCL login via institutional organisation (UK Federation) is required to access the digital book)\nBook: [R Programming] Brunsdon, C. et al (2018) An Introduction to R for Spatial Analysis and Mapping; Chapter 3: Basics of Handling Spatial Data in R Click link (Note: Books can be borrowed from UCL’s Library)\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 1: The R Environment Click link (Note: Digital book)\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with tables Click link (Note: Digital book)\nPaper: [R Programming] Tennekes, M. (2018). tmap: Thematic Maps in R. Journal of Statistical Software, 84(6), 1–39. https://doi.org/10.18637/jss.v084.i06 (Download)",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html#data-sources",
    "href": "03-spatial_analysis_for_data_science.html#data-sources",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "5 Data Sources",
    "text": "5 Data Sources\n\nUK Fire statistics incident level dataset (see: “Low-level geography dataset”)[Source: UK Gov] Click Here\nEnglish indices for Multiple Deprivation (IMD) for 2019 [Source: UK Gov] Click Here\nUK Shape files for different geographic units [Source: Office for National Statistics, Open Geography Portal] Click Here",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "03-spatial_analysis_for_data_science.html#exercise",
    "href": "03-spatial_analysis_for_data_science.html#exercise",
    "title": "Week 1: Spatial analysis for data sciences",
    "section": "6 Exercise",
    "text": "6 Exercise\nExploring the burden of fire hazards and vulnerability in London\nAn incidence rate (IR) is typically a measure of frequency used to quantify the occurrence of an event.\nThis basic quantitative measure is often applied in social sciences (e.g., criminology for crime rates, in education on pass rates for grades), epidemiology and disaster sciences as well. The study of fire hazards and safety is an intersection of these three fields.\nObjectives: Use the spatial data you have built throughout the exercise to visualise the distribution of incidence of fires that occurred in residential premises across postcodes in London in 2019.\nHINTS:\n\nThe incidence rate (I) formula for calculating fires hazards across LSOAs in London is the reported number of residential premises in an LSOA that experienced a fire event (E) divided by the total number of residential premises in an LOSA (T). This is shown below expressed per 1,000 residence (per capita). The reported number of residential premises is the column Fires2019. The total number of houses in an LSOA is the column TotalHouses2019 The incidence rate formula is:\n\n\n\n\\(I  = (E/T) × 1000\\)\n\n\n\nExperiment with the tmap functions and arguments to create a map showing the incidence rate of fire hazards across LSOAs in London. The output should look something as follows:\n\n\n\n\n\n\n\n\n\n\n\nUse the tm_shape() functions to add the spatial data frame object with the incidence rates.\nIn the arguments for the tm_polygons() function, inside it, it should include the following codes: 1.) use the argument fill= to specify the name of the column to be mapped; 2.) use the argument fill.scale = tm_scale_continuous() to control how the scale in the legends should appear. It should be continuous as the incidence rates are continuous values. Therefore, within the tm_scale_continuous() argument, use the options values = to control the colour gradient from blue (i.e., low intensity of fire hazards) to red (i.e., high burden) by typing the colour scheme: “-brewer.rd_bu”; 3.) use the argument fill.legend = tm_legend() to control the appearence of the legend block. Inside the tm_legend() specific title = as “Fires per 1,000”, and remove the plot frame around the legend block by stating frame = FALSE; 4.) apply the further cosmetics changes using \"fill_alpha = 1\" (transparency of areas), col_alpha = 0.5 (transparency of lines), col = \"grey\" (color of the line) and lwd = 0.1 (line thickness).\nInclude the following customizations: 1.) a scale bar and a north arrow in the visualisation using the tm_scale_bar() and tm_compass(); and 2.) lastly, use the overlay of the London Borough shapefile add it on top of the London LSOA shapefile by using again, the tm_shape() function, and render it fully transparent. Try using the tm_text() function to include the names of the Boroughs in the visualisation - this column is called ctyua16nm in the Borough’s shapefile.\n\nQuestion: How would you provide a descriptive interpretation of the spatial distribution/incidence of residential fires across LSOAs in London?\n\n\n\n\n\n\nNote\n\n\n\nHave a go at these questions before revealing the solution codes and output below\n\n\n\n\nClick here to see solution code:\n\nLoad the required packages:\n\nlibrary(\"tmap\")\nlibrary(\"sf\")\n\nLoad in the required dataset i.e., London_LSOA_FireHazards_2019.csv using the read.csv() function:\n\n# please be sure set the work directory according to your own PC\n# setwd(\"/Users/anwarmusah/Documents/Websites/GEOG0114/all_datasets/Week 1 - Dataset\")\n\nFire_data &lt;- read.csv(file=\"London_LSOA_FireHazards_2019.csv\", header = TRUE, sep = \",\")\n\nLoad in the required shapefile dataset i.e., London_LSOA_areas.shp and London_Boroughs_Shapefile.shp using the read_sf() function:\n\n# Add shapefiles for LSOA and Boroughs\nlondon_lsoa_shapefile &lt;- read_sf(\"London_LSOA_areas.shp\")\nlondon_borough_shapefile &lt;- read_sf(\"London_Boroughs_Shapefile.shp\")\n\nMerge the fire records to LSOA shapefile using the merge() on common key column (e.g., LSOA_code):\n\nSpatial_data &lt;- merge(london_lsoa_shapefile, Fire_data, by.x = 'LSOA_code', by.y = 'LSOA_code', all.x = TRUE)\n\nCalculate the incidence of fires in London’s LSOA:\n\n# Calculate the incidence rate. \n# This requires creating a new variable in the `Spatial_data` data frame.\n# Let call the new variable 'IncidenceFires'. \n# We are creating the 'IncidenceFires' column based on 'Fires2019' and 'TotalHouses2019'\n# Its a rate per capita so we will divide the two and multiple it by 1000\n\nSpatial_data$IncidenceFires &lt;- (Spatial_data$Fires2019/Spatial_data$TotalHouses2019)*1000\n\nLet’s create the map that shows the burden of fires. Here, we will use a continuous scale for the colour gradient in the legend block:\n\n# Generate a incidence map 'with continuous scale' according to the criteria specified in the hints\ntm_shape(Spatial_data) + \n    tm_polygons(\"IncidenceFires\",\n        fill.scale = tm_scale_continuous(values = \"-brewer.rd_bu\", midpoint = 1.5),\n        fill.legend = tm_legend(title = \"Fires per 1,000\", frame = FALSE),\n        fill_alpha = 1,\n        col_alpha = 0.5,\n        col = \"white\",\n        lwd = 0.1) +\n    tm_shape(london_borough_shapefile) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"ctyua16nm\", size = \"AREA\", fontface = \"bold\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \n    tm_scalebar(position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation: From this static map, it is difficult to discern the spatial patterns in incidence of fire hazards in residential dwellings in LSOAs in London. For instance, Boroughs such as Hackney appears to have a dispersed pattern with some LSOAs reporting high incidence rates. The borough of Bexley has high incidence rates concentrated in the North-East.\n\n\nYou can make this an interactive map by using the tmap_mode(\"view\") code. Let us print this output in an interactive map, so we can zoom into the details and have a clearer picture:\n\n# to map interactive using tmap_mode(\"view\") code, and re-run the map code again\n\n# active interactive mode\ntmap_mode(\"view\")\n# re-run the map code again to show it in an interactive mode\ntm_shape(Spatial_data) + \n    tm_polygons(\"IncidenceFires\",\n        fill.scale = tm_scale_continuous(values = \"-brewer.rd_bu\", midpoint = 1.5),\n        fill.legend = tm_legend(title = \"Fires per 1,000\", frame = FALSE),\n        fill_alpha = 1,\n        col_alpha = 0.5,\n        col = \"white\",\n        lwd = 0.1) +\n    tm_shape(london_borough_shapefile) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_text(\"ctyua16nm\", size = \"AREA\", fontface = \"bold\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) + \n    tm_scalebar(position = c(\"left\", \"bottom\"))\n\n# switch off the interactive mode\ntmap_mode(\"plot\")\n\n\n\n\n\n\n\nNote\n\n\n\nBy running the code: tmap_mode(\"view\"), will cause all map outputs to be generated in interactivity mode. You can switch it off by running the following code: tmap_mode(\"plot\")",
    "crumbs": [
      "Foundation & Theory",
      "Week 1: Spatial analysis for data sciences"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html",
    "href": "07-suitability_mapping_Niche.html",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "Last week, we described how knowledge-driven methods informed by expert opinion (often drawn from experience or literature) can be used to predict locations that are suitable (or favourable) for a specific outcome given a set of environmental factors, which in turn, can be outputted as a thematic raster map. Last week’s approach uses a mixed (i.e., qualitative and quantitative) methodology through the use of Overlay Mapping and Analytical Hierarchy Process (AHP) for suitability mapping, and these approaches are used strictly when there are no point occurrence data.\nThis week, we are using distributional models, often term as either: Ecological (or Environmental) Niche Models, or Environmental or Habitat Suitability Models. These are quantitative methods that use occurrence data in conjunction with environmental data to make a correlative model of the environmental conditions that meet an outcome’s environmental (or ecological) requirements, which in turn, can predict the relative suitability of habitat for that outcome. It has many applications in ecology, epidemiology, disaster risk reduction and social sciences. Today, we are going to use the Maximum Entropy Model (MAXENT) to infer zones for disaster hazards such as wildfires in California given a set of predictor variables (i.e, climate, vegetation, anthropogenic and socioeconomic risk factors which are raster).\n\n\nWe are going to learn the following steps:\n\nHandling of point locations of occurrence data (i.e., points of fires) and preparing it as testing and training data;\nHandling of predictor variables (i.e., raster) and compiling them into a raster stack object, as well as perform extraction of raster values to the points of fire locations;\nGenerating a Niche model using the maxent() from points, and use the stacked raster values to fit a model to estimate probability (or trigger points) of a fire hazard;\nTesting for model validation using ROC and AUC curves, and producing a surface based on threshold to delineate regions for suitability (in context probable trigger points for wildfires).\n\n\n\n\nBefore you begin do make sure to download all data by clicking here. Create a folder on called “Week 5” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 5” folder. Open a new R script and set the work directory to Week 5’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 5\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 5\")\n\n\n\n\nBefore we start anything, we will need to install Java for the appropriate operating system in order to follow this practical. You can download the latest version of Java accordingly:\n\n\n\n\n\n\nWarning\n\n\n\n\nFor Windows or macOS (Intel or M1/2/3) users - please download the latest version of Java [HERE]. For macOS M1/2/3 users - please download Java for macOS ARM64. If you are running on a macOS Intel - please make sure to download the x86_64 version of the JRE.\nFor macOS users, after you have downloaded the appropriate version of Java, please proceed to download the latest version of Azul Zulu Builds of OpenJDK [HERE].\n\n\n\n\n\n\nNext, we will need to load the following packages:\n\nraster: Raster/gridded data analysis and manipulation\nsf: Simple Features\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\nspdep: Access to spatial functions such spsample() needed for generating background points\ntmap: Thematic Mapping\n\nThe above packages raster sf, sp, spdep & tmap should have been installed in the previous session(s). However, we will need to install the following new package(s):\n\ndismo: Provides access to methods for niches distribution modelling in RStudio.\nrJava: Low-level interface for Java. The maxent() function depends on this so it must be installed.\nterra: Provides access to extra functions for processing raster data\nexactextractr: Provides access to extra functions for processing raster data e.g., exact_extract()\n\n\n# Install the packages with install.package():\ninstall.packages(\"dismo\")\ninstall.packages(\"rJava\")\ninstall.packages(\"terra\")\ninstall.packages(\"exactextractr\")\n\n[A] For Windows/macOS (Intel) users:\nUse the following code to load the packages in RStudio:\n\n# Load the packages with library():\nlibrary(\"raster\")\nlibrary(\"dismo\")\nlibrary(\"tmap\")\nlibrary(\"sf\")\nlibrary(\"rJava\")\nlibrary(\"spdep\")\nlibrary(\"terra\")\nlibrary(\"exactextractr\")\n\n[B] For macOS (M1/M2) users:\nPlease use the following code to first configure your macOS system by setting the system’s environment accordingly to this Java home path /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home with the Sys.setenv() function, and then load the packages:\n\n# Set the system environment to this location with Sys.setenv() \nSys.setenv(JAVA_HOME = \"/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home\")\n\n# Load the packages with library():\nlibrary(\"raster\")\nlibrary(\"dismo\")\nlibrary(\"tmap\")\nlibrary(\"sf\")\nlibrary(\"rJava\")\nlibrary(\"spdep\")\nlibrary(\"terra\")\nlibrary(\"exactextractr\")\n\n\n\n\n\n\n\nImportant\n\n\n\nFor macOS M1/M2 users - please use the Sys.setenv() function to configure your macOS system to that Java home path accordingly to prevent RStudio from crashing with that annoying Abort message whenever the maxent() function is executed.\n\n\n\n\n\nWe will be dealing with both point occurrence and raster data for this exercise. The point data are remote-sensed fire detection locations across the State of California during the summer period of 2018. We will be using predictor variables that are climatic (i.e., temperature, precipitation, dryness), environmental (vegetation and elevation) and other social-anthropogenic (socioeconomic deprivation) gridded raster data for California.\nWe will combine them into a MAXENT model in order to quantify the areas that are potential trigger points for wildfires, and whether these variables greatly influencing the risk of a fire hazard.\nLet us begin loading the following list of raster files, each is a variable of interest:\n\nRaster: Temperature named California_temperature_summer_2018.tif\nRaster: Precipitation named California_precipitation_summer_2018.tif\nRaster: Dryness named California_dryness_summer_2018.tif\nRaster: Vegetation (NDVI) named California_vegetation_summer_2018.tif\nRaster: Elevation named California_elevation_summer_2018.tif\nRaster: Deprivation Index named California_socdeprivation_summer_2018.tif\n\n\n# load data raster data\ntemp &lt;- raster(\"California_temperature_summer_2018.tif\")\nprec &lt;- raster(\"California_precipitation_summer_2018.tif\")\ndryn &lt;- raster(\"California_dryness_summer_2018.tif\")\nndvi &lt;- raster(\"California_vegetation_summer_2018.tif\")\nelev &lt;- raster(\"California_elevation_summer_2018.tif\")\nsevi &lt;- raster(\"California_socdeprivation_summer_2018.tif\")\n\nLoad the boundary and county shapefile for California:\n\nShape file: California’s boundary border named .shp\nShape file: California’s County borders named .shp\n\n\n# load shapefile data for california\ncalifornia_border &lt;- read_sf(\"California_boundary.shp\")\ncalifornia_county &lt;- read_sf(\"California_county.shp\")\n\nLoad the point occurrence data for California:\n\n# load occurrence fire data in California\ncalifornia_fires &lt;- read.csv(\"California_Fire_Summer_2018.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nAll shape file and raster data (5km resolution) were projected to the CRS: WGS84 4326.\n\n\nThe occurrence data imported into RStudio is a data frame object. We will need to first convert the occurrence data from data frame to a spatial points object by declaring columns longitude and latitude corresponds to x and y, respectively, as well as specifying that it’s CRS is 4326 (in Decimal Degrees). This can be done with st_as_sf() function.\n\n# Convert to sf object\ncalifornia_fires &lt;- st_as_sf(california_fires, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nLet us visualise the study area to examine the spatial distribution of wildfires.\n\n# map the locations of wildfire points\ntm_shape(california_county) + \n    tm_polygons() + \ntm_shape(california_fires) + \n    tm_dots(fill = \"red\", col = \"black\", size = 0.1)\n\n\n\n\n\n\n\n\n\n\nLet us visualise the six predictor raster variables that will be used for the Niche modelling with MAXENT. Instead of visual each output individually - you can use tmap_arrange() to create a figure panel for the six images.\n\n# map object of temperature stored in m1\nm1 = tm_shape(temp) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Mean Temperature\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) + \n    tm_title(\"A\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of precipitation stored in m2\nm2 = tm_shape(prec) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.blues\"),\n        col.legend = tm_legend(title = \"mm\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) + \n    tm_title(\"B\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of dryness stored in m3\nm3 = tm_shape(dryn) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.spectral\"),\n        col.legend = tm_legend(title = \"mm/0.25yr\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"C\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of ndvi stored in m4\nm4 = tm_shape(ndvi) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.greens\"),\n        col.legend = tm_legend(title = \"Index\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"D\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of elevation stored in m5\nm5 = tm_shape(elev) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.spectral\", midpoint = 1500),\n        col.legend = tm_legend(title = \"Meters\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"E\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of sevi stored in m6\nm6 = tm_shape(sevi) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.reds\"),\n        col.legend = tm_legend(title = \"%\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"F\", position = c(\"left\", \"bottom\"), size = 5)\n\n# stitch the maps together using tmap_arrange() function\ntmap_options(component.autoscale = FALSE)\ntmap_arrange(m1, m2, m3, m4, m5, m6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nA: Temperature (degree Celsius); B: Precipitation (mm); C: Dryness (Evapotranspiration) (mm/0.25 year); D: Vegetation (NDVI); E: Elevation (meters [m]); & F: Socioeconomic vulnerability index (%)\n\n\n\n\n\n\nImportant\n\n\n\nIt is good practice to produce figure outputs of the study area of interest inside the methodology section of an essay, report, dissertation, research paper etc.,. This gives you the opportunity to describe the study area that is under investigation and variables in the methods section.\n\n\n\n\n\n\n\n\nBasically, a band is represented by a single matrix of cell values, and a raster with multiple bands contains multiple spatially coincident matrices of cell values representing the same spatial area. For example, the raster object for temp (i.e., temperature) is a single band raster object. But, if we start to stack raster objects prec, dryn, ndvi and so on top of temp, one over the other, then we have created a multi-band raster object.\nWe need to create this multi-band raster object to enable the following action needed for the analysis:\n\nTo perform the extraction of raster values from all 6 variables on to the occurrence points in one go;\nThe entire multi-band raster object is needed for MAXENT estimation and spatial prediction\n\nWe use the stack() to stack all raster grids into one object and rename the layers within the multi-band object to names that are tenable.\n\nenvCovariates &lt;- stack(temp, prec, dryn, ndvi, elev, sevi)\nnames(envCovariates) &lt;- c(\"Temperature\", \"Precipitation\", \"Dryness\", \"NDVI\", \"Elevation\", \"Deprivation\")\n\n\n\n\nWe need to prepare the background data. What is the background data? With Background data we are not attempting to guess point locations where an event is absent. Here, we are rather trying to characterise the environment of the study region. In this sense, background is the same, irrespective of where the point fire are found or not. Background data establishes the environmental domain of the study, whilst presence data should establish under which conditions a fire is more likely to be present than on average.\nIn essence, we are creating a set of control points which we act as pseudo-absence. These are typically generated at random. There are several ways of performing this action using other functions such as randomPoints(), sampleRandom and many more.\nWe are using the spsample() function because it allows the user to specify the boundary for which the background points (i.e., controls) should be randomly generated within. Twice the number of occurrence points are generated (the choice of twice is up to the user). For reproducibility in the random generation, we have set the set.seed() function to this 20000430.\n\n# set the seed\nset.seed(20000430)\n# we need to coerce 'sf' object california_border into 'sp' object for spsample to work\ncalifornia_border_sp &lt;- as(california_border, Class = \"Spatial\")\n# here, spsample() generates twice number of fire occurrence points randomly within California's border\nbackground_points &lt;- spsample(california_border_sp, n=2*nrow(california_fires), \"random\")\n\n\n\n\nNow, we are going to extract information from our envCovariates raster stack to both the presence and background points. This can be done using the extract() function.\n\n# perform raster extraction from the environmental covariates on to points\ncalifornia_fires_env &lt;- extract(envCovariates, california_fires)\nbackground_points_env &lt;- extract(envCovariates, background_points)\n\n\n\n\n\n\n\nImportant\n\n\n\nAfter the extraction, the objects california_fires_env and background_points_env exist as a large matrix, and not as a data frame.\n\n\nFor all occurrence points (i.e., presence), we need to add an indicator of 1 to signify presence; while for all background points (i.e., absence) - we need to also add an indicator of 0 to signify absence. We do this because we are modelling a probability and such niche models take outcomes that are from a Bernoulli or Binomial distribution.\n\n# convert large matrix objects to data frame objects and add outcome `fire` indicator\ncalifornia_fires_env &lt;-data.frame(california_fires_env,fire=1)\nbackground_points_env &lt;-data.frame(background_points_env,fire=0)\n\n# View one of the data frame\nhead(california_fires_env, n=5)\nhead(background_points_env, n=5)\n\n\n\n\nNow, we need to complete one more step before we construct our wildfire risk model. We have to come up with a way to assess how well our model can actually predict whether we will likely find a trigger point for fires in a particular location. To make this assessment, we will need to perform some ‘cross-validation’ i.e., that is setting aside some of our presence-absence locations, and using them to test the model. In our case, we will randomly withhold 25% of the observations as test data, and retain the other 75% as training data for the prediction.\nThis means that we will be fitting the model multiple times, withholding each fourth of the data separately, then average the results. This is called a k-fold cross-validation (in our case 4-fold).\nHowever, for our purposes of time, we will just fit the model once to demonstrate what is actually happening for you to get the gist. Ideally, we will need to perform a 4-fold cross-validation and in turn average the estimates across the values for AUC and those for the true positives and negative as described in section 5.4.2.\nUse the kfold() function to split the presence data from california_fires_env object into 4 equal parts. This should add an index that makes four random groups of observations. You can hold 25% of the data (i.e., the first portion) by specifying select == 1 as the test data. You can hold the remaining 75% of the data (i.e., the 2nd, 3rd and 4th portion) by specifying select != 1 as the training data.\n\n# set same set.seed() as before\nset.seed(20000430)\n# using k-fold function to split data into 4 equal parts\nselect &lt;- kfold(california_fires_env, 4)\n# 25% of the fire data use for testing the model\ncalifornia_fires_env_test &lt;- california_fires_env[select==1,]\n# 75% of the fire data use for training the model\ncalifornia_fires_env_train &lt;- california_fires_env[select!=1,]\n\nRepeat the above process for the background points:\n\n# set same set.seed() as before\nset.seed(20000430)\n# repeat the process for the background points\nselect &lt;- kfold(background_points_env, 4)\nbackground_points_env_test &lt;- background_points_env[select==1,]\nbackground_points_env_train &lt;- background_points_env[select!=1,]\n\nNow, let us row bind the training and test dataset together using the rbind() function:\n\ntraining_data &lt;- rbind(california_fires_env_train, background_points_env_train)\ntesting_data &lt;- rbind(california_fires_env_test, background_points_env_test)\n\nWe are now in the position to execute the distributional niche models.\n\n\n\n\nNow, we can fit the niche model using the Maximum Entropy (MAXENT) algorithm, which tries to define the combination of environmental risk factors that best predicts the occurrence of the wildfires in California. The maxent() allows the users to implement such algorithm.\nHere are some important notes on it’s usage:\n\nmaxent(): This function uses environmental data for locations of known presence and for a large number of ‘background’ locations. It has three mandatory arguments - x, p, and args.\nx: In this argument, you must specify the columns of the predictor variables in the training data frame. The first columns in the example are the risk factors we are interested.\np: In this argument, you must specify the column containing the presence and absence of fires in the training data frame.\nargs: This allows for additional arguments.\n\n[A] For Windows and macOS (Intel) users, running the maxent() code should look something like:\n\nmodel_training &lt;- maxent(x=training_data[,c(1:6)], p=training_data[,7], args=c(\"responsecurves\"))\n\n[B] For macOS (M1/M2) users, use the tryCatch() on the maxent() code to avoid the Abort warning! The code should look something like:\n\nresult &lt;- tryCatch({\n    model_training &lt;- maxent(x=training_data[,c(1:6)], p=training_data[,7], args=c(\"responsecurves\"))\n    print(\"Model trained successfully!\")\n    model_training\n}, error = function(e) {\n    print(paste(\"Error:\", e$message))\n    NULL\n})\n\n\n\nThe results are stored in the model_training object. We can examine which variable has the biggest contribution to the presence of wildfire presences in California:\n\nplot(model_training, pch=19, xlab = \"Percentage [%]\", cex=1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIMPORTANT NOTES: We can view the contribution estimates for each covariate more precisely by typing in RConsole the following code: model_training@results. Here, we can see the following contribution estimates: NDVI (44.2321%); Elevation (23.5530%); Deprivation (12.0339%); Dryness (9.9266); Temperature (6.8892%); and Precipitation (3.3653%). The contribution estimates should sum up to 100%.\nInterpretation: From this plot, we can see that the model is most sensitive to variation in NDVI, followed with additional contributions from land surface elevation, and from increased levels of socioeconomic deprivation (reporting top three).\n\n\nWe can examine as well as the likelihood of fire occurrence and how it responds to variation in these conditions. To see the shape of the response curves estimated by the model, we can use the response() function:\n\nresponse(model_training)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: In the response plots, we are looking at how the probability of fire occurrence (Y-axes, from zero to one) varies with each the environmental predictors (X-axes). From these plots, we can see that the MAXENT models can include complex environmental responses including plateau, linear, and nonlinear shapes, and some which are utterly unclear. For example, if we look at mean temperature during the summer, we can see that the probability for fire occurrence peaks around 0.60 when temperatures are around 30 degrees Celsius. We can also see that the probability of such outcome increases with more and more vegetation during the summer period. Probability in terms of fires in relation to deprivation is a flat line. For precipitation, dryness and elevation - the patterns are unclear.\n\n\n\n\n\nAn important part is model validation - this involves assessing how well does the model actually predict the occurrence of wildfires. To evaluate the predictive accuracy of the model, we turn back to our test data i.e., testing_data object, and use cross-validation to test the model.\nIn our evaluation - there are two main outputs of interest:\n\nAUC (Area Under the Curve), which is a test of model performance where higher values indicate greater accuracy in our predictions. An AUC value of 0.5 is common cut-off point used for assessing model performance. Note that an AUC value of 0.5 or lower is the same as random guessing of presence/absence, while values towards one mean our predictions are more reliable and accurate.\nmax TPR+TNR, which denotes the probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate. It is generally accepted that this is an optimum value at which to set the threshold for binary classification of the predicted probabilities in our mapping outputs. Anything above value is deemed as a region environmentally suitable for outcome.\n\nWe use the evaluate() function to perform cross-validation analysis.\n\n# model evaluation use the test data on the trained model for validation\ncross_validation &lt;- evaluate(p=testing_data[testing_data$fire==1,], a=testing_data[testing_data$fire==0,], model = model_training)\n\nHere are some important notes on it’s usage:\n\nevaluate(): This function used for model evaluation and validation. It has the following arguments - p, a, and model.\np: In this argument, you must specify the column of outcome and filter on the presence value e.g., testing_data[testing_data$fire==1,].\na: In this argument, you must specify the column of outcome and filter on the absence value e.g., testing_data[testing_data$fire==0,]\nmodel: Specify the full training model object e.g., model_training.\n\nNow call results and plot AUC curve:\n\ncross_validation \n\n\n&gt; cross_validation\nclass          : ModelEvaluation \nn presences    : 14120 \nn absences     : 27609 \nAUC            : 0.9074081 \ncor            : 0.7003824 \nmax TPR+TNR at : 0.4054474\n\n\nplot(cross_validation, 'ROC', cex=1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: On the receiver operator curve, the 1:1 line give an AUC of 0.5. From our curve and the AUC, it is clear that our model appears to do substantially better than random guessing (high AUC value = 0.907 [90.7%]). The optimal probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate is 0.4054474 (40.55%). Hence, we will use predicted probability &gt; 0.4054 to delineate areas of suitability (or trigger points) for wildfires.\n\n\n\n\n\nTo map the predicted probabilities use the predict() function:\n\nprob_wildfire &lt;- predict(model_training, envCovariates)\n\nGenerate a predicted probability map from above prob_wildfire object:\n\n# generate a publication-worthy figure\n# map of probability \ntm_shape(prob_wildfire) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.rd_yl_bu\"),\n        col.legend = tm_legend(title = expression(bold(\"Predicted Probability\")), frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n    tm_scalebar(position=c(\"left\", \"bottom\"), text.size = 1, breaks = c(0, 100, 200, 300)) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nExtract the optimal threshold value from cross validation object cross_validation using the threshold() function and use it to reclassify the raster object i.e., prob_wildfire to a binary raster. Recall, the value was 0.4054474. A probability estimate less than 0.4054474 is classed as 0 and anything above as 1. The predicted probability &gt; 0.4054 are the areas in California suitable (or expected trigger points) for wildfires.\n\n# calculate thresholds of models\nthreshold_value &lt;- threshold(cross_validation, \"spec_sens\")\n# report value\nthreshold_value\n\nReclassifying raster object prob_wildfire with threshold value:\n\n# prepare threshold total map \ncreate_classes_vector &lt;- c(0, threshold_value, 0, threshold_value, 1, 1)\ncreate_clasess_matrix &lt;- matrix(create_classes_vector, ncol = 3, byrow = TRUE)\ncreate_clasess_matrix\n\n\n&gt; create_clasess_matrix\n          [,1]      [,2] [,3]\n[1,] 0.0000000 0.4054474    0\n[2,] 0.4054474 1.0000000    1\n\n\n# create new reclassify raster based on prob_wildfires\nsuitability_wildfires &lt;- reclassify(prob_wildfire, create_clasess_matrix)\n\nGenerate final output which shows regions as trigger points:\n\ntm_shape(suitability_wildfires) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"lightgrey\", \"red\"), labels = c(\"Safe\", \"Trigger Points\")),\n        col.legend = tm_legend(title = expression(bold(\"Threshold\")), frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n    tm_scalebar(position=c(\"left\", \"bottom\"), text.size = 1, breaks = c(0, 100, 200, 300)) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to see code:\n\n\n# split plot panel into 4 segments for 4 AUC plots\npar(mfrow=c(2,2))\n# create a list() object to dump results inside `eMAX`\neMAX&lt;-list()\n\n# use california_fires_env\n# use background_points_env\nfolds &lt;- 4\n\nkfold_pres &lt;- kfold(california_fires_env, folds)\nkfold_back &lt;- kfold(background_points_env, folds)\n\nset.seed(20000430)\n# takes a long time to run 4-fold\nfor (i in 1:folds) {\n    train &lt;- california_fires_env[kfold_pres!= i,]\n    test &lt;- california_fires_env[kfold_pres == i,]\n    backTrain&lt;-background_points_env[kfold_back!=i,]\n    backTest&lt;-background_points_env[kfold_back==i,]\n    dataTrain&lt;-rbind(train,backTrain)\n    dataTest&lt;-rbind(test,backTest)\n    maxnet_eval &lt;- maxent(x=dataTrain[,c(1:6)], p=dataTrain[,7], args=c(\"responsecurves\"))\n    eMAX[[i]] &lt;- evaluate(p=dataTest[dataTest$fire==1,],a=dataTest[dataTest$fire==0,], maxnet_eval)\n    plot(eMAX[[i]],'ROC')\n}\n\naucMAX &lt;- sapply( eMAX, function(x){slot(x, 'auc')} )\n# report 4 of the AUC\naucMAX\n# [1] 0.9052601 0.9086170 0.9049672 0.9083264\n\n# find the mean of AUC (and it must be &gt; 0.50)\nmean(aucMAX)\n# [1] 0.9067927\n\n#Get maxTPR+TNR for the maxnet model\nOpt_MAX&lt;-sapply( eMAX, function(x){ x@t[which.max(x@TPR + x@TNR)] } )\nOpt_MAX\n\n# [1] 0.3703498 0.4434108 0.4275991 0.4465352\n\nMean_OptMAX&lt;-mean(Opt_MAX)\nMean_OptMAX\n\n# use Mean_OptMAX as threshold for mapping suitability\n#Note: that final results is AUC: 0.9067927; threshold: 0.4219737\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to see code:\n\n\n# we want to estimate the fraction of the population at risk being exposed to wildfires in california\n# using the population density raster 100m (data source: WorldPop [www.worldpop.org])\n# loading the raster 'usa_pd_2018_1km.tif'\nusa_popn_dens &lt;- raster(\"usa_pd_2018_1km.tif\")\n\n# we are taking the extent of california from shapefile\ncalifornia_extent &lt;- extent(california_border)\n# we are using the extent cropping the US raster down to California  \ncalifornia_cropped &lt;- crop(usa_popn_dens, california_extent)\n# we are now masking the grids to the boundary of california\ncalifornia_masked &lt;- mask(california_cropped, california_border)\n\ntm_shape(california_masked) + tm_raster(style = \"cont\", title = \"Pop/100m\", palette= \"Blues\")\n\n# we are going to upscale the resolution of California popn grids from the original 1000m to approximately 5000m\n# at the same time - we will aggregate the 1000m population pixel to result in total population in 5000m per pixel\n# note: its not perfect conversion!\n# note that 0.0008333 = 100m\n# note that 0.008333 = 1000m (1km)\n# note that 0.0417 = 5004.2m (~5km)\ncalifornia_popn_5km &lt;- aggregate(california_masked, fact = 0.0417/0.008333333, fun = sum, na.rm =TRUE)\n\n# identify populations at risk from the suitability map and mask out any values that are not in the trigger zone\nhigh_risk_mask &lt;- suitability_wildfires\nhigh_risk_pop &lt;- california_popn_5km * high_risk_mask\nhigh_risk_pop[high_risk_pop == 0] &lt;- NA\n\ntm_shape(high_risk_pop) + tm_raster(style = \"cont\", title = \"Pop/100m\", palette= \"Reds\") +\n    tm_shape(california_county) + tm_polygons(alpha = 0)\n\n# derive total population in a county\ncalifornia_county$total_population &lt;- exact_extract(california_popn_5km, california_county, 'sum')\n\n# derive the total population at risk in a county\noptions(scipen = 3)\ncalifornia_county$risk_population &lt;- exact_extract(high_risk_pop, california_county, 'sum')\n# calculate the percent of the population at risk in a county\ncalifornia_county$AtRisk_percent &lt;- california_county$risk_population/california_county$total_population * 100\n\n# creating the final map\n# create the labels\nRisk_tier &lt;- c(\"&lt;1.0%\", \"1.0-9.9%\", \"10.0-19.9%\", \"20.0-49.9%\", \"50.0% & above\")\n\n# categorizing the AtRisk_percent column\ncalifornia_county$AtRiskCat &lt;- NA\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=0 & california_county$AtRisk_percent&lt;1] &lt;- 1\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=1 & california_county$AtRisk_percent&lt;10] &lt;- 2\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=10 & california_county$AtRisk_percent&lt;20] &lt;- 3\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=20 & california_county$AtRisk_percent&lt;50] &lt;- 4\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=50 & california_county$AtRisk_percent&lt;101] &lt;- 5\n\n# visualisation\ntm_shape(california_county) + \n    tm_polygons(fill = \"AtRiskCat\", fill.scale = tm_scale_categorical(values = c(\"white\", \"#ffffe5\", \"#fed976\", \"#fc4e2a\", \"#bd0026\"), \n        labels = Risk_tier), fill.legend = tm_legend(title = \"Popn at risk [%]\"), col = \"black\") + \n    tm_text(\"NAME\", size = \"AREA\") +\n    tm_scalebar(position=c(\"left\", \"bottom\"), text.size = 1, breaks = c(0, 100, 200, 300)) +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\"), text.size = 0.9) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis week’s practical uses content and inspiration from:\nTaylor, L. 2022. The social side of fire: assessing the inclusion of human social factors in fire prediction models (submitted as a dissertation [for degree in MSc Social & Geographic Data Science] at UCL). Source\n\n\n\n\nBook: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Downloadable)\nBook: [Theory] Stockwell, D. (2019) Niche Modeling: Predictions from Statistical Distributions; Chapter 4: Topology; CRC Press; pages: 45-63.\nOnline: [Tutorials] Hijmans, R.J., & Elith, J. (2021) Species distribution modelling Click link\nOnline: [Tutorials] Kerkhoff, D. (2016) Ecological Responses to Climate Change: Species Distribution Modelling using Maxent Click link\nPaper: [Application] Escobar, L.E., (2020). Ecological Niche Modeling: An Introduction for Veterinarians and Epidemiologists, Frontiers in Veterinary Science Click link\nPaper: [Application] Banks, W.E., (2017). The application of ecological niche modeling methods to archaeological data in order to examine culture-environment relationships and cultural trajectories; Quarternaire Click link\nPaper: [Application] Liao, Y., Lei, Y., Ren, Z., Chen, H., & Li., D., (2017). Predicting the potential risk area of illegal vaccine trade in China; Scientific Reports, Issue 7, 3883. Click link\n\n\n\n\n\nAll shape files [Source: California Open Data Portal] Click Here\nGlobal Wildfires detection points [Source: Fire Information Resource Management System] Click Here\nEnvironmental data for temperature & precipitation [Source: WorldClim] Click Here\nSocioeconomic vulnerability index (requires user login) [Source: Socioeconomic Data and Applications Center (SEDAC)] Click Here\nDigital Elevation Model [Source: SRTM 90m DEM Digital Elevation Database] Click Here\nEvapotranspiration (aridity) 1.0km [Source: NASA MODIS MOD16A2] Click Here\nNormalised Differenced Vegetation Index (NDVI) 250m [Source: NASA MODIS MOD13Q1] Click Here\n2018 Population density (1km) for the United States [Source: Worldpop] Click Here\n\n\n\n\nSuitability mapping of the Aedes mosquito and infestation in Brazil\nMany regions in Brazil were hit hard by the Zika virus infection outbreak in 2015. Zika infection is caused by the arboviruses transmitted by the Aedes mosquitoes which are abundant in Brazil. It is a known fact that increased abundance of the Aedes mosquito is typically associated with standing (or stagnant) water which serves as a reservoir or hotspot for breeding. Apart from the presence of standing (or stagnated) water in human dwellings, it is important to consider other intermediate factors that drive the mosquitoes to increase in population size. These factors are the following:\n\nTemperature\nPrecipitation\nPopulation Density\nNDVI\nLand surface elevation\nNatural lighting\nUrban-rural classification\n\nThe above listed variables are gridded datasets which can be downloaded by clicking [HERE]. Create a map which should the following: 1.) The predicted probability of infestation of the Aedes mosquito in Brazil; and 2.) the suitability map based on the max TPR + TNR threshold to illustrate where the Aedes mosquito will thrive in Brazil.\nAll identified points for mosquito breeding including background points can downloaded from [HERE]. The boundaries for Brazil can be downloaded from [HERE].\nThe expected output should look like:\n\n\n\n\n\n\n\n\n\nNotes:\n\nUse the following set.seed(20000430).\nThe number of pseudo-absences points should be twice the number of presence points.\nUse the following split for training and testing data: 75:25 (i.e., 4 equal parts where 3 is for training and 1 is for testing).\nFor the predicted probability map (A), reclassify the predictions to the following categories - 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8 and 0.8-1.0.\nFor the second suitability map (B), reclassify the probabilities based on the max TPR + TNR threshold i.e., below it as 0 and labelled as ‘None infested areas’ and above as 1 labelled as ‘Infested areas’.\nPerform a 4-fold validation.\n\nThis week’s homework practical was based on the original research paper:\n\nMusah et al (2023). Coalescing disparate data sources for the geospatial prediction of mosquito abundance, using Brazil as a motivating case study. Frontiers in Tropical Diseases. Volume 4. DOI: https://doi.org/10.3389/fitd.2023.1039735\n\n\n\n\n\n\n\nImportant\n\n\n\nHave a go at these questions before using solution codes which can be downloaded from [HERE]",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#introduction",
    "href": "07-suitability_mapping_Niche.html#introduction",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "Last week, we described how knowledge-driven methods informed by expert opinion (often drawn from experience or literature) can be used to predict locations that are suitable (or favourable) for a specific outcome given a set of environmental factors, which in turn, can be outputted as a thematic raster map. Last week’s approach uses a mixed (i.e., qualitative and quantitative) methodology through the use of Overlay Mapping and Analytical Hierarchy Process (AHP) for suitability mapping, and these approaches are used strictly when there are no point occurrence data.\nThis week, we are using distributional models, often term as either: Ecological (or Environmental) Niche Models, or Environmental or Habitat Suitability Models. These are quantitative methods that use occurrence data in conjunction with environmental data to make a correlative model of the environmental conditions that meet an outcome’s environmental (or ecological) requirements, which in turn, can predict the relative suitability of habitat for that outcome. It has many applications in ecology, epidemiology, disaster risk reduction and social sciences. Today, we are going to use the Maximum Entropy Model (MAXENT) to infer zones for disaster hazards such as wildfires in California given a set of predictor variables (i.e, climate, vegetation, anthropogenic and socioeconomic risk factors which are raster).\n\n\nWe are going to learn the following steps:\n\nHandling of point locations of occurrence data (i.e., points of fires) and preparing it as testing and training data;\nHandling of predictor variables (i.e., raster) and compiling them into a raster stack object, as well as perform extraction of raster values to the points of fire locations;\nGenerating a Niche model using the maxent() from points, and use the stacked raster values to fit a model to estimate probability (or trigger points) of a fire hazard;\nTesting for model validation using ROC and AUC curves, and producing a surface based on threshold to delineate regions for suitability (in context probable trigger points for wildfires).\n\n\n\n\nBefore you begin do make sure to download all data by clicking here. Create a folder on called “Week 5” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 5” folder. Open a new R script and set the work directory to Week 5’s folder.\nFor Windows, the work directory will be:\n\nsetwd(\"C:/Users/AccountName/Desktop/GEOG0114/Week 5\")\n\nFor MAC, the work directory will be:\n\nsetwd(\"/Users/AccountName/Desktop/GEOG0114/Week 5\")\n\n\n\n\nBefore we start anything, we will need to install Java for the appropriate operating system in order to follow this practical. You can download the latest version of Java accordingly:\n\n\n\n\n\n\nWarning\n\n\n\n\nFor Windows or macOS (Intel or M1/2/3) users - please download the latest version of Java [HERE]. For macOS M1/2/3 users - please download Java for macOS ARM64. If you are running on a macOS Intel - please make sure to download the x86_64 version of the JRE.\nFor macOS users, after you have downloaded the appropriate version of Java, please proceed to download the latest version of Azul Zulu Builds of OpenJDK [HERE].\n\n\n\n\n\n\nNext, we will need to load the following packages:\n\nraster: Raster/gridded data analysis and manipulation\nsf: Simple Features\nsp: Package for providing classes for spatial data (points, lines, polygons and grids)\nspdep: Access to spatial functions such spsample() needed for generating background points\ntmap: Thematic Mapping\n\nThe above packages raster sf, sp, spdep & tmap should have been installed in the previous session(s). However, we will need to install the following new package(s):\n\ndismo: Provides access to methods for niches distribution modelling in RStudio.\nrJava: Low-level interface for Java. The maxent() function depends on this so it must be installed.\nterra: Provides access to extra functions for processing raster data\nexactextractr: Provides access to extra functions for processing raster data e.g., exact_extract()\n\n\n# Install the packages with install.package():\ninstall.packages(\"dismo\")\ninstall.packages(\"rJava\")\ninstall.packages(\"terra\")\ninstall.packages(\"exactextractr\")\n\n[A] For Windows/macOS (Intel) users:\nUse the following code to load the packages in RStudio:\n\n# Load the packages with library():\nlibrary(\"raster\")\nlibrary(\"dismo\")\nlibrary(\"tmap\")\nlibrary(\"sf\")\nlibrary(\"rJava\")\nlibrary(\"spdep\")\nlibrary(\"terra\")\nlibrary(\"exactextractr\")\n\n[B] For macOS (M1/M2) users:\nPlease use the following code to first configure your macOS system by setting the system’s environment accordingly to this Java home path /Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home with the Sys.setenv() function, and then load the packages:\n\n# Set the system environment to this location with Sys.setenv() \nSys.setenv(JAVA_HOME = \"/Library/Java/JavaVirtualMachines/zulu-8.jdk/Contents/Home\")\n\n# Load the packages with library():\nlibrary(\"raster\")\nlibrary(\"dismo\")\nlibrary(\"tmap\")\nlibrary(\"sf\")\nlibrary(\"rJava\")\nlibrary(\"spdep\")\nlibrary(\"terra\")\nlibrary(\"exactextractr\")\n\n\n\n\n\n\n\nImportant\n\n\n\nFor macOS M1/M2 users - please use the Sys.setenv() function to configure your macOS system to that Java home path accordingly to prevent RStudio from crashing with that annoying Abort message whenever the maxent() function is executed.\n\n\n\n\n\nWe will be dealing with both point occurrence and raster data for this exercise. The point data are remote-sensed fire detection locations across the State of California during the summer period of 2018. We will be using predictor variables that are climatic (i.e., temperature, precipitation, dryness), environmental (vegetation and elevation) and other social-anthropogenic (socioeconomic deprivation) gridded raster data for California.\nWe will combine them into a MAXENT model in order to quantify the areas that are potential trigger points for wildfires, and whether these variables greatly influencing the risk of a fire hazard.\nLet us begin loading the following list of raster files, each is a variable of interest:\n\nRaster: Temperature named California_temperature_summer_2018.tif\nRaster: Precipitation named California_precipitation_summer_2018.tif\nRaster: Dryness named California_dryness_summer_2018.tif\nRaster: Vegetation (NDVI) named California_vegetation_summer_2018.tif\nRaster: Elevation named California_elevation_summer_2018.tif\nRaster: Deprivation Index named California_socdeprivation_summer_2018.tif\n\n\n# load data raster data\ntemp &lt;- raster(\"California_temperature_summer_2018.tif\")\nprec &lt;- raster(\"California_precipitation_summer_2018.tif\")\ndryn &lt;- raster(\"California_dryness_summer_2018.tif\")\nndvi &lt;- raster(\"California_vegetation_summer_2018.tif\")\nelev &lt;- raster(\"California_elevation_summer_2018.tif\")\nsevi &lt;- raster(\"California_socdeprivation_summer_2018.tif\")\n\nLoad the boundary and county shapefile for California:\n\nShape file: California’s boundary border named .shp\nShape file: California’s County borders named .shp\n\n\n# load shapefile data for california\ncalifornia_border &lt;- read_sf(\"California_boundary.shp\")\ncalifornia_county &lt;- read_sf(\"California_county.shp\")\n\nLoad the point occurrence data for California:\n\n# load occurrence fire data in California\ncalifornia_fires &lt;- read.csv(\"California_Fire_Summer_2018.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nAll shape file and raster data (5km resolution) were projected to the CRS: WGS84 4326.\n\n\nThe occurrence data imported into RStudio is a data frame object. We will need to first convert the occurrence data from data frame to a spatial points object by declaring columns longitude and latitude corresponds to x and y, respectively, as well as specifying that it’s CRS is 4326 (in Decimal Degrees). This can be done with st_as_sf() function.\n\n# Convert to sf object\ncalifornia_fires &lt;- st_as_sf(california_fires, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nLet us visualise the study area to examine the spatial distribution of wildfires.\n\n# map the locations of wildfire points\ntm_shape(california_county) + \n    tm_polygons() + \ntm_shape(california_fires) + \n    tm_dots(fill = \"red\", col = \"black\", size = 0.1)\n\n\n\n\n\n\n\n\n\n\nLet us visualise the six predictor raster variables that will be used for the Niche modelling with MAXENT. Instead of visual each output individually - you can use tmap_arrange() to create a figure panel for the six images.\n\n# map object of temperature stored in m1\nm1 = tm_shape(temp) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.spectral\"),\n        col.legend = tm_legend(title = \"Mean Temperature\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) + \n    tm_title(\"A\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of precipitation stored in m2\nm2 = tm_shape(prec) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.blues\"),\n        col.legend = tm_legend(title = \"mm\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) + \n    tm_title(\"B\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of dryness stored in m3\nm3 = tm_shape(dryn) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.spectral\"),\n        col.legend = tm_legend(title = \"mm/0.25yr\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"C\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of ndvi stored in m4\nm4 = tm_shape(ndvi) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.greens\"),\n        col.legend = tm_legend(title = \"Index\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"D\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of elevation stored in m5\nm5 = tm_shape(elev) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.spectral\", midpoint = 1500),\n        col.legend = tm_legend(title = \"Meters\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"E\", position = c(\"left\", \"bottom\"), size = 5)\n\n# map object of sevi stored in m6\nm6 = tm_shape(sevi) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"brewer.reds\"),\n        col.legend = tm_legend(title = \"%\", frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_layout(frame = FALSE) +\n    tm_title(\"F\", position = c(\"left\", \"bottom\"), size = 5)\n\n# stitch the maps together using tmap_arrange() function\ntmap_options(component.autoscale = FALSE)\ntmap_arrange(m1, m2, m3, m4, m5, m6, nrow = 2)\n\n\n\n\n\n\n\n\n\n\nA: Temperature (degree Celsius); B: Precipitation (mm); C: Dryness (Evapotranspiration) (mm/0.25 year); D: Vegetation (NDVI); E: Elevation (meters [m]); & F: Socioeconomic vulnerability index (%)\n\n\n\n\n\n\nImportant\n\n\n\nIt is good practice to produce figure outputs of the study area of interest inside the methodology section of an essay, report, dissertation, research paper etc.,. This gives you the opportunity to describe the study area that is under investigation and variables in the methods section.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#data-preparation-for-the-maxent-analysis",
    "href": "07-suitability_mapping_Niche.html#data-preparation-for-the-maxent-analysis",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "Basically, a band is represented by a single matrix of cell values, and a raster with multiple bands contains multiple spatially coincident matrices of cell values representing the same spatial area. For example, the raster object for temp (i.e., temperature) is a single band raster object. But, if we start to stack raster objects prec, dryn, ndvi and so on top of temp, one over the other, then we have created a multi-band raster object.\nWe need to create this multi-band raster object to enable the following action needed for the analysis:\n\nTo perform the extraction of raster values from all 6 variables on to the occurrence points in one go;\nThe entire multi-band raster object is needed for MAXENT estimation and spatial prediction\n\nWe use the stack() to stack all raster grids into one object and rename the layers within the multi-band object to names that are tenable.\n\nenvCovariates &lt;- stack(temp, prec, dryn, ndvi, elev, sevi)\nnames(envCovariates) &lt;- c(\"Temperature\", \"Precipitation\", \"Dryness\", \"NDVI\", \"Elevation\", \"Deprivation\")\n\n\n\n\nWe need to prepare the background data. What is the background data? With Background data we are not attempting to guess point locations where an event is absent. Here, we are rather trying to characterise the environment of the study region. In this sense, background is the same, irrespective of where the point fire are found or not. Background data establishes the environmental domain of the study, whilst presence data should establish under which conditions a fire is more likely to be present than on average.\nIn essence, we are creating a set of control points which we act as pseudo-absence. These are typically generated at random. There are several ways of performing this action using other functions such as randomPoints(), sampleRandom and many more.\nWe are using the spsample() function because it allows the user to specify the boundary for which the background points (i.e., controls) should be randomly generated within. Twice the number of occurrence points are generated (the choice of twice is up to the user). For reproducibility in the random generation, we have set the set.seed() function to this 20000430.\n\n# set the seed\nset.seed(20000430)\n# we need to coerce 'sf' object california_border into 'sp' object for spsample to work\ncalifornia_border_sp &lt;- as(california_border, Class = \"Spatial\")\n# here, spsample() generates twice number of fire occurrence points randomly within California's border\nbackground_points &lt;- spsample(california_border_sp, n=2*nrow(california_fires), \"random\")\n\n\n\n\nNow, we are going to extract information from our envCovariates raster stack to both the presence and background points. This can be done using the extract() function.\n\n# perform raster extraction from the environmental covariates on to points\ncalifornia_fires_env &lt;- extract(envCovariates, california_fires)\nbackground_points_env &lt;- extract(envCovariates, background_points)\n\n\n\n\n\n\n\nImportant\n\n\n\nAfter the extraction, the objects california_fires_env and background_points_env exist as a large matrix, and not as a data frame.\n\n\nFor all occurrence points (i.e., presence), we need to add an indicator of 1 to signify presence; while for all background points (i.e., absence) - we need to also add an indicator of 0 to signify absence. We do this because we are modelling a probability and such niche models take outcomes that are from a Bernoulli or Binomial distribution.\n\n# convert large matrix objects to data frame objects and add outcome `fire` indicator\ncalifornia_fires_env &lt;-data.frame(california_fires_env,fire=1)\nbackground_points_env &lt;-data.frame(background_points_env,fire=0)\n\n# View one of the data frame\nhead(california_fires_env, n=5)\nhead(background_points_env, n=5)\n\n\n\n\nNow, we need to complete one more step before we construct our wildfire risk model. We have to come up with a way to assess how well our model can actually predict whether we will likely find a trigger point for fires in a particular location. To make this assessment, we will need to perform some ‘cross-validation’ i.e., that is setting aside some of our presence-absence locations, and using them to test the model. In our case, we will randomly withhold 25% of the observations as test data, and retain the other 75% as training data for the prediction.\nThis means that we will be fitting the model multiple times, withholding each fourth of the data separately, then average the results. This is called a k-fold cross-validation (in our case 4-fold).\nHowever, for our purposes of time, we will just fit the model once to demonstrate what is actually happening for you to get the gist. Ideally, we will need to perform a 4-fold cross-validation and in turn average the estimates across the values for AUC and those for the true positives and negative as described in section 5.4.2.\nUse the kfold() function to split the presence data from california_fires_env object into 4 equal parts. This should add an index that makes four random groups of observations. You can hold 25% of the data (i.e., the first portion) by specifying select == 1 as the test data. You can hold the remaining 75% of the data (i.e., the 2nd, 3rd and 4th portion) by specifying select != 1 as the training data.\n\n# set same set.seed() as before\nset.seed(20000430)\n# using k-fold function to split data into 4 equal parts\nselect &lt;- kfold(california_fires_env, 4)\n# 25% of the fire data use for testing the model\ncalifornia_fires_env_test &lt;- california_fires_env[select==1,]\n# 75% of the fire data use for training the model\ncalifornia_fires_env_train &lt;- california_fires_env[select!=1,]\n\nRepeat the above process for the background points:\n\n# set same set.seed() as before\nset.seed(20000430)\n# repeat the process for the background points\nselect &lt;- kfold(background_points_env, 4)\nbackground_points_env_test &lt;- background_points_env[select==1,]\nbackground_points_env_train &lt;- background_points_env[select!=1,]\n\nNow, let us row bind the training and test dataset together using the rbind() function:\n\ntraining_data &lt;- rbind(california_fires_env_train, background_points_env_train)\ntesting_data &lt;- rbind(california_fires_env_test, background_points_env_test)\n\nWe are now in the position to execute the distributional niche models.",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#maxent-analysis",
    "href": "07-suitability_mapping_Niche.html#maxent-analysis",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "Now, we can fit the niche model using the Maximum Entropy (MAXENT) algorithm, which tries to define the combination of environmental risk factors that best predicts the occurrence of the wildfires in California. The maxent() allows the users to implement such algorithm.\nHere are some important notes on it’s usage:\n\nmaxent(): This function uses environmental data for locations of known presence and for a large number of ‘background’ locations. It has three mandatory arguments - x, p, and args.\nx: In this argument, you must specify the columns of the predictor variables in the training data frame. The first columns in the example are the risk factors we are interested.\np: In this argument, you must specify the column containing the presence and absence of fires in the training data frame.\nargs: This allows for additional arguments.\n\n[A] For Windows and macOS (Intel) users, running the maxent() code should look something like:\n\nmodel_training &lt;- maxent(x=training_data[,c(1:6)], p=training_data[,7], args=c(\"responsecurves\"))\n\n[B] For macOS (M1/M2) users, use the tryCatch() on the maxent() code to avoid the Abort warning! The code should look something like:\n\nresult &lt;- tryCatch({\n    model_training &lt;- maxent(x=training_data[,c(1:6)], p=training_data[,7], args=c(\"responsecurves\"))\n    print(\"Model trained successfully!\")\n    model_training\n}, error = function(e) {\n    print(paste(\"Error:\", e$message))\n    NULL\n})\n\n\n\nThe results are stored in the model_training object. We can examine which variable has the biggest contribution to the presence of wildfire presences in California:\n\nplot(model_training, pch=19, xlab = \"Percentage [%]\", cex=1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIMPORTANT NOTES: We can view the contribution estimates for each covariate more precisely by typing in RConsole the following code: model_training@results. Here, we can see the following contribution estimates: NDVI (44.2321%); Elevation (23.5530%); Deprivation (12.0339%); Dryness (9.9266); Temperature (6.8892%); and Precipitation (3.3653%). The contribution estimates should sum up to 100%.\nInterpretation: From this plot, we can see that the model is most sensitive to variation in NDVI, followed with additional contributions from land surface elevation, and from increased levels of socioeconomic deprivation (reporting top three).\n\n\nWe can examine as well as the likelihood of fire occurrence and how it responds to variation in these conditions. To see the shape of the response curves estimated by the model, we can use the response() function:\n\nresponse(model_training)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: In the response plots, we are looking at how the probability of fire occurrence (Y-axes, from zero to one) varies with each the environmental predictors (X-axes). From these plots, we can see that the MAXENT models can include complex environmental responses including plateau, linear, and nonlinear shapes, and some which are utterly unclear. For example, if we look at mean temperature during the summer, we can see that the probability for fire occurrence peaks around 0.60 when temperatures are around 30 degrees Celsius. We can also see that the probability of such outcome increases with more and more vegetation during the summer period. Probability in terms of fires in relation to deprivation is a flat line. For precipitation, dryness and elevation - the patterns are unclear.\n\n\n\n\n\nAn important part is model validation - this involves assessing how well does the model actually predict the occurrence of wildfires. To evaluate the predictive accuracy of the model, we turn back to our test data i.e., testing_data object, and use cross-validation to test the model.\nIn our evaluation - there are two main outputs of interest:\n\nAUC (Area Under the Curve), which is a test of model performance where higher values indicate greater accuracy in our predictions. An AUC value of 0.5 is common cut-off point used for assessing model performance. Note that an AUC value of 0.5 or lower is the same as random guessing of presence/absence, while values towards one mean our predictions are more reliable and accurate.\nmax TPR+TNR, which denotes the probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate. It is generally accepted that this is an optimum value at which to set the threshold for binary classification of the predicted probabilities in our mapping outputs. Anything above value is deemed as a region environmentally suitable for outcome.\n\nWe use the evaluate() function to perform cross-validation analysis.\n\n# model evaluation use the test data on the trained model for validation\ncross_validation &lt;- evaluate(p=testing_data[testing_data$fire==1,], a=testing_data[testing_data$fire==0,], model = model_training)\n\nHere are some important notes on it’s usage:\n\nevaluate(): This function used for model evaluation and validation. It has the following arguments - p, a, and model.\np: In this argument, you must specify the column of outcome and filter on the presence value e.g., testing_data[testing_data$fire==1,].\na: In this argument, you must specify the column of outcome and filter on the absence value e.g., testing_data[testing_data$fire==0,]\nmodel: Specify the full training model object e.g., model_training.\n\nNow call results and plot AUC curve:\n\ncross_validation \n\n\n&gt; cross_validation\nclass          : ModelEvaluation \nn presences    : 14120 \nn absences     : 27609 \nAUC            : 0.9074081 \ncor            : 0.7003824 \nmax TPR+TNR at : 0.4054474\n\n\nplot(cross_validation, 'ROC', cex=1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nInterpretation: On the receiver operator curve, the 1:1 line give an AUC of 0.5. From our curve and the AUC, it is clear that our model appears to do substantially better than random guessing (high AUC value = 0.907 [90.7%]). The optimal probability threshold at which our model maximizes the True Positive Rate and the True Negative Rate is 0.4054474 (40.55%). Hence, we will use predicted probability &gt; 0.4054 to delineate areas of suitability (or trigger points) for wildfires.\n\n\n\n\n\nTo map the predicted probabilities use the predict() function:\n\nprob_wildfire &lt;- predict(model_training, envCovariates)\n\nGenerate a predicted probability map from above prob_wildfire object:\n\n# generate a publication-worthy figure\n# map of probability \ntm_shape(prob_wildfire) + \n    tm_raster(col.scale = tm_scale_continuous(values = \"-brewer.rd_yl_bu\"),\n        col.legend = tm_legend(title = expression(bold(\"Predicted Probability\")), frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n    tm_scalebar(position=c(\"left\", \"bottom\"), text.size = 1, breaks = c(0, 100, 200, 300)) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\nExtract the optimal threshold value from cross validation object cross_validation using the threshold() function and use it to reclassify the raster object i.e., prob_wildfire to a binary raster. Recall, the value was 0.4054474. A probability estimate less than 0.4054474 is classed as 0 and anything above as 1. The predicted probability &gt; 0.4054 are the areas in California suitable (or expected trigger points) for wildfires.\n\n# calculate thresholds of models\nthreshold_value &lt;- threshold(cross_validation, \"spec_sens\")\n# report value\nthreshold_value\n\nReclassifying raster object prob_wildfire with threshold value:\n\n# prepare threshold total map \ncreate_classes_vector &lt;- c(0, threshold_value, 0, threshold_value, 1, 1)\ncreate_clasess_matrix &lt;- matrix(create_classes_vector, ncol = 3, byrow = TRUE)\ncreate_clasess_matrix\n\n\n&gt; create_clasess_matrix\n          [,1]      [,2] [,3]\n[1,] 0.0000000 0.4054474    0\n[2,] 0.4054474 1.0000000    1\n\n\n# create new reclassify raster based on prob_wildfires\nsuitability_wildfires &lt;- reclassify(prob_wildfire, create_clasess_matrix)\n\nGenerate final output which shows regions as trigger points:\n\ntm_shape(suitability_wildfires) + \n    tm_raster(col.scale = tm_scale_categorical(values = c(\"lightgrey\", \"red\"), labels = c(\"Safe\", \"Trigger Points\")),\n        col.legend = tm_legend(title = expression(bold(\"Threshold\")), frame = FALSE)) +\n    tm_shape(california_county) + \n    tm_polygons(fill_alpha = 0, col = \"black\") +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\")) +\n    tm_scalebar(position=c(\"left\", \"bottom\"), text.size = 1, breaks = c(0, 100, 200, 300)) +\n    tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to see code:\n\n\n# split plot panel into 4 segments for 4 AUC plots\npar(mfrow=c(2,2))\n# create a list() object to dump results inside `eMAX`\neMAX&lt;-list()\n\n# use california_fires_env\n# use background_points_env\nfolds &lt;- 4\n\nkfold_pres &lt;- kfold(california_fires_env, folds)\nkfold_back &lt;- kfold(background_points_env, folds)\n\nset.seed(20000430)\n# takes a long time to run 4-fold\nfor (i in 1:folds) {\n    train &lt;- california_fires_env[kfold_pres!= i,]\n    test &lt;- california_fires_env[kfold_pres == i,]\n    backTrain&lt;-background_points_env[kfold_back!=i,]\n    backTest&lt;-background_points_env[kfold_back==i,]\n    dataTrain&lt;-rbind(train,backTrain)\n    dataTest&lt;-rbind(test,backTest)\n    maxnet_eval &lt;- maxent(x=dataTrain[,c(1:6)], p=dataTrain[,7], args=c(\"responsecurves\"))\n    eMAX[[i]] &lt;- evaluate(p=dataTest[dataTest$fire==1,],a=dataTest[dataTest$fire==0,], maxnet_eval)\n    plot(eMAX[[i]],'ROC')\n}\n\naucMAX &lt;- sapply( eMAX, function(x){slot(x, 'auc')} )\n# report 4 of the AUC\naucMAX\n# [1] 0.9052601 0.9086170 0.9049672 0.9083264\n\n# find the mean of AUC (and it must be &gt; 0.50)\nmean(aucMAX)\n# [1] 0.9067927\n\n#Get maxTPR+TNR for the maxnet model\nOpt_MAX&lt;-sapply( eMAX, function(x){ x@t[which.max(x@TPR + x@TNR)] } )\nOpt_MAX\n\n# [1] 0.3703498 0.4434108 0.4275991 0.4465352\n\nMean_OptMAX&lt;-mean(Opt_MAX)\nMean_OptMAX\n\n# use Mean_OptMAX as threshold for mapping suitability\n#Note: that final results is AUC: 0.9067927; threshold: 0.4219737\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here to see code:\n\n\n# we want to estimate the fraction of the population at risk being exposed to wildfires in california\n# using the population density raster 100m (data source: WorldPop [www.worldpop.org])\n# loading the raster 'usa_pd_2018_1km.tif'\nusa_popn_dens &lt;- raster(\"usa_pd_2018_1km.tif\")\n\n# we are taking the extent of california from shapefile\ncalifornia_extent &lt;- extent(california_border)\n# we are using the extent cropping the US raster down to California  \ncalifornia_cropped &lt;- crop(usa_popn_dens, california_extent)\n# we are now masking the grids to the boundary of california\ncalifornia_masked &lt;- mask(california_cropped, california_border)\n\ntm_shape(california_masked) + tm_raster(style = \"cont\", title = \"Pop/100m\", palette= \"Blues\")\n\n# we are going to upscale the resolution of California popn grids from the original 1000m to approximately 5000m\n# at the same time - we will aggregate the 1000m population pixel to result in total population in 5000m per pixel\n# note: its not perfect conversion!\n# note that 0.0008333 = 100m\n# note that 0.008333 = 1000m (1km)\n# note that 0.0417 = 5004.2m (~5km)\ncalifornia_popn_5km &lt;- aggregate(california_masked, fact = 0.0417/0.008333333, fun = sum, na.rm =TRUE)\n\n# identify populations at risk from the suitability map and mask out any values that are not in the trigger zone\nhigh_risk_mask &lt;- suitability_wildfires\nhigh_risk_pop &lt;- california_popn_5km * high_risk_mask\nhigh_risk_pop[high_risk_pop == 0] &lt;- NA\n\ntm_shape(high_risk_pop) + tm_raster(style = \"cont\", title = \"Pop/100m\", palette= \"Reds\") +\n    tm_shape(california_county) + tm_polygons(alpha = 0)\n\n# derive total population in a county\ncalifornia_county$total_population &lt;- exact_extract(california_popn_5km, california_county, 'sum')\n\n# derive the total population at risk in a county\noptions(scipen = 3)\ncalifornia_county$risk_population &lt;- exact_extract(high_risk_pop, california_county, 'sum')\n# calculate the percent of the population at risk in a county\ncalifornia_county$AtRisk_percent &lt;- california_county$risk_population/california_county$total_population * 100\n\n# creating the final map\n# create the labels\nRisk_tier &lt;- c(\"&lt;1.0%\", \"1.0-9.9%\", \"10.0-19.9%\", \"20.0-49.9%\", \"50.0% & above\")\n\n# categorizing the AtRisk_percent column\ncalifornia_county$AtRiskCat &lt;- NA\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=0 & california_county$AtRisk_percent&lt;1] &lt;- 1\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=1 & california_county$AtRisk_percent&lt;10] &lt;- 2\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=10 & california_county$AtRisk_percent&lt;20] &lt;- 3\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=20 & california_county$AtRisk_percent&lt;50] &lt;- 4\ncalifornia_county$AtRiskCat[california_county$AtRisk_percent&gt;=50 & california_county$AtRisk_percent&lt;101] &lt;- 5\n\n# visualisation\ntm_shape(california_county) + \n    tm_polygons(fill = \"AtRiskCat\", fill.scale = tm_scale_categorical(values = c(\"white\", \"#ffffe5\", \"#fed976\", \"#fc4e2a\", \"#bd0026\"), \n        labels = Risk_tier), fill.legend = tm_legend(title = \"Popn at risk [%]\"), col = \"black\") + \n    tm_text(\"NAME\", size = \"AREA\") +\n    tm_scalebar(position=c(\"left\", \"bottom\"), text.size = 1, breaks = c(0, 100, 200, 300)) +\n    tm_compass(type = \"arrow\", position = c(\"right\", \"top\"), text.size = 0.9) +\n    tm_layout(frame = FALSE)",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#attributions",
    "href": "07-suitability_mapping_Niche.html#attributions",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "This week’s practical uses content and inspiration from:\nTaylor, L. 2022. The social side of fire: assessing the inclusion of human social factors in fire prediction models (submitted as a dissertation [for degree in MSc Social & Geographic Data Science] at UCL). Source",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#references-see-reading-list",
    "href": "07-suitability_mapping_Niche.html#references-see-reading-list",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "Book: [R Programming] Dorman, M. (2014) Learning R for Geospatial Analysis; Chapter 3: Working with Rasters Click link (Note: Downloadable)\nBook: [Theory] Stockwell, D. (2019) Niche Modeling: Predictions from Statistical Distributions; Chapter 4: Topology; CRC Press; pages: 45-63.\nOnline: [Tutorials] Hijmans, R.J., & Elith, J. (2021) Species distribution modelling Click link\nOnline: [Tutorials] Kerkhoff, D. (2016) Ecological Responses to Climate Change: Species Distribution Modelling using Maxent Click link\nPaper: [Application] Escobar, L.E., (2020). Ecological Niche Modeling: An Introduction for Veterinarians and Epidemiologists, Frontiers in Veterinary Science Click link\nPaper: [Application] Banks, W.E., (2017). The application of ecological niche modeling methods to archaeological data in order to examine culture-environment relationships and cultural trajectories; Quarternaire Click link\nPaper: [Application] Liao, Y., Lei, Y., Ren, Z., Chen, H., & Li., D., (2017). Predicting the potential risk area of illegal vaccine trade in China; Scientific Reports, Issue 7, 3883. Click link",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#data-sources",
    "href": "07-suitability_mapping_Niche.html#data-sources",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "All shape files [Source: California Open Data Portal] Click Here\nGlobal Wildfires detection points [Source: Fire Information Resource Management System] Click Here\nEnvironmental data for temperature & precipitation [Source: WorldClim] Click Here\nSocioeconomic vulnerability index (requires user login) [Source: Socioeconomic Data and Applications Center (SEDAC)] Click Here\nDigital Elevation Model [Source: SRTM 90m DEM Digital Elevation Database] Click Here\nEvapotranspiration (aridity) 1.0km [Source: NASA MODIS MOD16A2] Click Here\nNormalised Differenced Vegetation Index (NDVI) 250m [Source: NASA MODIS MOD13Q1] Click Here\n2018 Population density (1km) for the United States [Source: Worldpop] Click Here",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "07-suitability_mapping_Niche.html#exercise",
    "href": "07-suitability_mapping_Niche.html#exercise",
    "title": "Week 5: Ecological Niche Models",
    "section": "",
    "text": "Suitability mapping of the Aedes mosquito and infestation in Brazil\nMany regions in Brazil were hit hard by the Zika virus infection outbreak in 2015. Zika infection is caused by the arboviruses transmitted by the Aedes mosquitoes which are abundant in Brazil. It is a known fact that increased abundance of the Aedes mosquito is typically associated with standing (or stagnant) water which serves as a reservoir or hotspot for breeding. Apart from the presence of standing (or stagnated) water in human dwellings, it is important to consider other intermediate factors that drive the mosquitoes to increase in population size. These factors are the following:\n\nTemperature\nPrecipitation\nPopulation Density\nNDVI\nLand surface elevation\nNatural lighting\nUrban-rural classification\n\nThe above listed variables are gridded datasets which can be downloaded by clicking [HERE]. Create a map which should the following: 1.) The predicted probability of infestation of the Aedes mosquito in Brazil; and 2.) the suitability map based on the max TPR + TNR threshold to illustrate where the Aedes mosquito will thrive in Brazil.\nAll identified points for mosquito breeding including background points can downloaded from [HERE]. The boundaries for Brazil can be downloaded from [HERE].\nThe expected output should look like:\n\n\n\n\n\n\n\n\n\nNotes:\n\nUse the following set.seed(20000430).\nThe number of pseudo-absences points should be twice the number of presence points.\nUse the following split for training and testing data: 75:25 (i.e., 4 equal parts where 3 is for training and 1 is for testing).\nFor the predicted probability map (A), reclassify the predictions to the following categories - 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8 and 0.8-1.0.\nFor the second suitability map (B), reclassify the probabilities based on the max TPR + TNR threshold i.e., below it as 0 and labelled as ‘None infested areas’ and above as 1 labelled as ‘Infested areas’.\nPerform a 4-fold validation.\n\nThis week’s homework practical was based on the original research paper:\n\nMusah et al (2023). Coalescing disparate data sources for the geospatial prediction of mosquito abundance, using Brazil as a motivating case study. Frontiers in Tropical Diseases. Volume 4. DOI: https://doi.org/10.3389/fitd.2023.1039735\n\n\n\n\n\n\n\nImportant\n\n\n\nHave a go at these questions before using solution codes which can be downloaded from [HERE]",
    "crumbs": [
      "Raster-Based Analysis",
      "Week 5: Ecological Niche Models"
    ]
  },
  {
    "objectID": "12-video_achives.html",
    "href": "12-video_achives.html",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 01:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 01:59:49)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video (Length: 43:13)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 01:59:48)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video (Length: 01:06:42)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video: Not Available\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video (Length: 01:06:42)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video (Length: 01:06:17)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video (Length: 00:41:47)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\n\n\n\nLecture video: Not Available\nComputer practical video (Length: 01:57:53)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-1",
    "href": "12-video_achives.html#week-1",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 01:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 01:59:49)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-2",
    "href": "12-video_achives.html#week-2",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 43:13)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 01:59:48)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-3",
    "href": "12-video_achives.html#week-3",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 01:06:42)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-4",
    "href": "12-video_achives.html#week-4",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video: Not Available\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-5",
    "href": "12-video_achives.html#week-5",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 01:06:42)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-6",
    "href": "12-video_achives.html#week-6",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 01:06:17)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-9",
    "href": "12-video_achives.html#week-9",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video (Length: 00:41:47)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]\nComputer practical video (Length: 02:00:01)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  },
  {
    "objectID": "12-video_achives.html#week-10",
    "href": "12-video_achives.html#week-10",
    "title": "2023-2024/25",
    "section": "",
    "text": "Lecture video: Not Available\nComputer practical video (Length: 01:57:53)\n\n\n\n\n\n\n\n\n\n[Watch on YouTube]",
    "crumbs": [
      "Video Archive",
      "2023-2024/25"
    ]
  }
]