[["index.html", "GEOG0114: Principles of Spatial Analysis Welcome Structure Moodle Office hours for GEOG0114", " GEOG0114: Principles of Spatial Analysis Welcome Welcome to GEOG0114: Principles of Spatial Analysis, one of the core 1st term modules for this MSc programme (Social and Geographic Data Science). This module has been designed as an introduction to the core tenets and principles of spatial analysis. Here, you will learn the key concepts and spatial analytical methods, which are applicable to the domains of social science and geography. In the first three weeks, we will cover Spatial analysis for data science, Graphical representation of spatial data and Spatial autocorrelation. Afterwards, in week 4, 5 and 6, we will cover a variety of basic geostatistical techniques that require analysis of point and raster data - these include suitability mapping such as Analytical Hierarchical Process (AHP) and Ecological Niche Modelling; and then Geostatistical analysis using Kriging. In week 7, 8, 9 and 10, we will learn specialised tools for performing spatial analysis on vector data (e.g., point, line and areal structures)- these include Network Analysis, Geodemographics, as well as spatial regression models approaches such as Geographical Weighted Regression (GWR) and Spatial Lag &amp; Error models. All these tutorials will be covered in RStudio. Structure All lectures and computer practicals will be delivered in-person. All Lectures are held on Monday from 02:00pm to 03:00pm at the North West Wing (Room GO7). All computer lab seminars are delivered on Monday (after lectures) from 03:00pm to 05:00pm in the same room i.e., North West Wing (Room GO7). Technical support workshops are held on Thursday from 03:00pm to 05:00pm at the Institute of Education (IOE) (Bedford Way [20])(Room 604). IMPORTANT NOTE: Please bring your own laptops with you to the computer practicals and technical support sessions on Monday and Thursday, respectively Moodle Moodle is the central point of your learning experience for GEOG0114. Please use it on a regular basis to check for updates concerning the schedule for weekly topics, access to the practical materials and assessment. All lecture notes, videos, practical materials including reading lists and downloadable data sets will be hosted on this webpage. Office hours for GEOG0114 Feel free to drop-by in-person at our offices for additional support Office hours with Dr Anwar Musah (TBC) Office hours with Dr Justin van Dijk (TBC) Office hours with Shun-Chan (Dan) Tsai (TBC) Our contact information and office details are: Dr Anwar Musah UCL Department of Geography Room 115, North West Wing Building, WC1E 6BT Email: a.musah@ucl.ac.uk Dr Justin van Dijk UCL Department of Geography Room 117, North West Wing Building, WC1E 6BT Email: j.t.vandijk@ucl.ac.uk Shun-Chan Tsai UCL Department of Geography Email: shun-chan.tsai.14@ucl.ac.uk "],["suitability-mapping-part-1.html", "1 Suitability Mapping: Part 1 1.1 Introduction 1.2 Suitability Mapping using Binary Classification OR Ratings 1.3 Finding areas of suitability by multiplying the binary maps 1.4 Finding areas of suitability by summing the binary maps to create ratings 1.5 Standardizing the raster variables to the same scale (from 1 to 10) 1.6 Determining the weight of each variable using Saaty’s AHP 1.7 Weighted Linear Combination (WLC) and derivation of Suitability Maps based from AHP analysis 1.8 References", " 1 Suitability Mapping: Part 1 1.1 Introduction Now, with the proliferation of open spatial dataset, risk models derived from environmental, sociodemographic and topological factors are becoming increasingly available for open research. Such models have broadly shown to be useful in delineating geographical areas of risk or suitability for a certain outcomes. Today, we are going to explore this qualitatively using a knowledge-driven approach. 1.1.1 Learning outcomes To provide an introductory overview to the applicability of knowledge-driven methods, in particular, we are going to learn the Multi-Criteria Decision Approach (MCDA) which is a method that use decision rules from existing knowledge to identify areas potential suitability for an outcome. It is especially useful in data-sparse situations, or when for the first time exploring the potential geographical limits of certain outcome. For instance, using modest number of raster layers such as population density, urbanisation, approximating to street segments, house prices and deprivation; it is possible to combine such information so to determine regions for which crime events such as burglaries are likely to occur, or suitable in that matter. This approach has been widely used in a number of disciplines over the last decades, and has gained prominence in public health related fields such as vector-borne disease prevention, and disaster sciences such as landslides. We will learn how to apply these methods to the two context. 1.1.2 Datasets &amp; setting up the work directory Before you begin do make sure to download all data by clicking here. Create a folder on called “Week 4” within your “GEOG0114” folder stored in the desktop of your personal computer. Make sure to extract all data from the zip folder and store it into “Week 4” folder. Open a new R script and set the work directory to Week 4’s folder. For Windows, the work directory will be: setwd(&quot;C:/Users/AccountName/Desktop/GEOG0114/Week 4&quot;) For MAC, the work directory will be: setwd(&quot;/Users/AccountName/Desktop/GEOG0114/Week 4&quot;) 1.1.3 Loading and installing packages We will need to load the following packages: sf: Simple Features tmap: Thematic Mapping raster: Raster/gridded data analysis and manipulation sp: Package for providing classes for spatial data (points, lines, polygons and grids) The above packages sf, tmap, raster &amp; sp should have been installed in the previous session(s). We will need to install a new package: spatialEco - Spatial Analysis and Modelling Utilities package, which provides the user further functions such as raster.invert() which we will need in the third section. BAMMtools - It provides the user access to an important function getJenksBreaks() for the AHP analysis in the third section. # Install the packages: spatialEco using the install.package() install.packages(&quot;spatialEco&quot;) install.packages(&quot;BAMMtools&quot;) # Load the packages with library() library(&quot;spatialEco&quot;) library(&quot;BAMMtools&quot;) 1.1.4 Loading datasets We will be dealing with raster data only for this exercise. These will be a series of climate, environmental and other social-anthropogenic gridded data from Kenya. We are going to combine them in order to determine areas that are suitable for disease transmission of Lymphatic Filariasis, a neglected tropical disease, caused by a range of mosquitoes that spread the parasitic worms called the Wuchereria bancrofti through a mosquito bite. There is evidence that indicates that precipitation, temperature, population density, aridity, dryness, land surface elevation and vegetation are risk factors for mosquito infestation, and in turn, greatly influencing the way mosquitoes breed and feeding behaviour within human dwellings thus maintaining disease transmission. We will use the following techniques for mapping areas of suitability: Binary Classification &amp; Ranking Suitability Mapping using Weighted Overlay Analysis MCDA Mapping Lets begin loading the following list of raster files, each is a variable of interest: Raster: Mean Temperature in Kenya (1000m resolution) named Kenya Mean Teperature.tif Raster: Normalized Difference Vegetation Index in Kenya (1000m resolution) named Kenya NDVI.tif Raster: Precipitation in Kenya (1000m resolution) named Kenya Precipitation.tif Raster: Population Density in Kenya (1000m resolution) named Kenya Population Density.tif Raster: Land Surface Elevation in Kenya (1000m resolution) named Kenya Elevation.tif Raster: Levels of Dryness in Kenya (1000m resolution) named Kenya Aridity Index.tif temp &lt;- raster(&quot;Kenya Mean Teperature.tif&quot;) nvdi &lt;- raster(&quot;Kenya NDVI.tif&quot;) prec &lt;- raster(&quot;Kenya Precipitation.tif&quot;) popl &lt;- raster(&quot;Kenya Population Density.tif&quot;) elev &lt;- raster(&quot;Kenya Elevation.tif&quot;) arid &lt;- raster(&quot;Kenya Aridity Index.tif&quot;) # ignore those stupid warning messages as these are annoying as hell and are related to same migration/updates done on PROJ4 to PROJ6 - which I can&#39;t for the life of me figure out how to implement this bloody update. Load the shapefile nation and state borders for Kenya: Shape file: Kenya’s National border named Kenya_Border_3857.shp Shape file: Kenya’s State border named Kenya_States_3857.shp # load the shapefiles kenya_border &lt;- st_read(&quot;Kenya_Border_3857.shp&quot;) ## Reading layer `Kenya_Border_3857&#39; from data source `/Volumes/Anwar-HHD/GEOG0114/Github/2022_2023/datasets/week-4/Kenya_Border_3857.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 1 feature and 2 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 3774798 ymin: -526069.9 xmax: 4667205 ymax: 564140.5 ## Projected CRS: WGS 84 / Pseudo-Mercator kenya_states &lt;- st_read(&quot;Kenya_States_3857.shp&quot;) ## Reading layer `Kenya_States_3857&#39; from data source `/Volumes/Anwar-HHD/GEOG0114/Github/2022_2023/datasets/week-4/Kenya_States_3857.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 47 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 3774798 ymin: -526069.9 xmax: 4667205 ymax: 564140.5 ## Projected CRS: WGS 84 / Pseudo-Mercator IMPORTANT NOTES: All shape file and raster data were in projected to the CRS: Spherical mercator 3857 You can inspect each raster to know its dimension, extent, resolution and minimum and maximum values. Since are going to stack all the rasters together, you definitely want the: dimension, extent and resolution to be the same. If there’s a slight difference the stack won’t work. # for instance temp and aridity temp ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Mean Teperature.tif ## names : Kenya_Mean_Teperature ## values : 1.2, 29.6 (min, max) arid ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Aridity Index.tif ## names : Kenya_Aridity_Index ## values : 0.0835, 2.7548 (min, max) To visualize raster data - you can write the following code: # For instance take the mean temperature for Kenya # Spectral colours are useful for diverging scales &quot;Spectral&quot; is Rd-Or-Yl-Gr-Bu. &quot;-Spectral&quot; reverses the order tm_shape(temp) + tm_raster(style = &quot;cont&quot;, title = &quot;Mean Temperature&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) 1.2 Suitability Mapping using Binary Classification OR Ratings One thing to know about this approach - the mappings are purely knowledge-driven and not analytical. For instance, according to previous studies (see example of papers: 1 &amp; 2), we have learnt a bit about the limits or thresholds for these predictors from or below which we can consider an area being suitable for the occurrence of Lymphatic Filariasis (LF). We have summarized these limits or thresholds in a table as follows: Risk factor Threshold for suitability Temperature &gt; 15.0 (degree Celsius) Precipitation &gt; 350 (mm of rainfall) Aridity &gt; 0.20 (i.e., semi-arid (&gt; 0.5) dry sub-humid) Elevation &lt; 1200m (low land) Population Density &gt; 0 (Inhabitants must exist source for blood meal) NDVI &gt; 0.5 (mild levels of vegetation) We should use the aforementioned thresholds to produce binary or Boolean maps using the above criteria. This means that the pixel values of each raster layer will be equal to 0 indicating that its an unsuitable condition for LF transmission, and 1 for suitable conditions for LF transmission. Let us reclassify each layer according to the above criteria, starting with temperature: # reclassify temperature as a binary or Boolean layer temp ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Mean Teperature.tif ## names : Kenya_Mean_Teperature ## values : 1.2, 29.6 (min, max) # lowest value = 1.2 # highest value = 29.6 # reclassify anything below 15 as 0 and above as 1 # take the minimum value and minus 1 to it to ensure that lowest # ---value is included in the categorization for 0 # take the maximum value and add 1 to it to also ensure that the highest # ---value is included in the categorization for 1 # c(from lowest value, to threshold, class as 0, from thresholds, to maximum value, class as 1) temp_cl &lt;- c(temp@data@min-1, 15, 0, 15, temp@data@max+1, 1) # convert into a matrix format temp_cl_mat &lt;- matrix(temp_cl, ncol = 3, byrow = TRUE) # see matrix temp_cl_mat ## [,1] [,2] [,3] ## [1,] 0.2 15.0 0 ## [2,] 15.0 30.6 1 # apply matrix to reclassify() function to categorize the raster accordingly temp_recl &lt;- reclassify(temp, temp_cl_mat) When you reclassify the raster for temp to temp_recl. This is what the output should look like: tm_shape(temp_recl) + tm_raster(style = &quot;cat&quot;, title = &quot;Temperature&quot;, palette= c(&quot;grey&quot;, &quot;#F1948A&quot;), labels = c(&quot;Unsuitable (&lt;15.0)&quot;, &quot;Suitable (15 &amp; above)&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Lets repeat the same steps for elevation to reclassify it’s layer according to the given criteria: # reclassify elevation as a binary or Boolean layer elev ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Elevation.tif ## names : Kenya_Elevation ## values : -11, 4865 (min, max) ## attributes : ## ID OBJECTID COUNT ## from: -11 410 1 ## to : 4865 5286 1 # lowest value = -11m (below sea level) # highest value = 4865m (above sea level) # reclassify anything below 1200m as 1 and above as 0 # again, take the minimum value and minus 1 to it to # ---ensure that lowest value is included in the categorization for 0 # again, take the maximum value and add 1 to it to also ensure that # ---the highest value is included in the categorization for 1 # c(from lowest value, to threshold, class as 1, from thresholds, to maximum value, class as 0) elev_cl &lt;- c(elev@data@min-1, 1200, 1, 1200, elev@data@max+1, 0) # convert into a matrix format elev_cl_mat &lt;- matrix(elev_cl, ncol = 3, byrow = TRUE) # see matrix elev_cl_mat ## [,1] [,2] [,3] ## [1,] -12 1200 1 ## [2,] 1200 4866 0 # apply matrix to reclassify() function to categorize the raster accordingly elev_recl &lt;- reclassify(elev, elev_cl_mat) The elevation output should look something like: tm_shape(elev_recl) + tm_raster(style = &quot;cat&quot;, title = &quot;Elevation&quot;, palette= c(&quot;grey&quot;, &quot;orange&quot;), labels = c(&quot;Unsuitable (&gt;1200m)&quot;, &quot;Suitable (1200m &amp; below)&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) Try reclassifying the remaining raster arid, prec, popl and ndvi. The solutions are provided in the code chunks below. Click here Solutions # nvdi nvdi ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya NDVI.tif ## names : Kenya_NDVI ## values : -0.1161125, 0.8743375 (min, max) nvdi_cl &lt;- c(nvdi@data@min-1, 0.5, 0, 0.5, nvdi@data@max+1, 1) nvdi_cl_mat &lt;- matrix(nvdi_cl, ncol = 3, byrow = TRUE); nvdi_cl_mat ## [,1] [,2] [,3] ## [1,] -1.116113 0.500000 0 ## [2,] 0.500000 1.874337 1 nvdi_recl &lt;- reclassify(nvdi, nvdi_cl_mat) # prec prec ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Precipitation.tif ## names : Kenya_Precipitation ## values : 172, 2625 (min, max) ## attributes : ## ID OBJECTID Value Count ## from: 0 1 172 8 ## to : 2206 2207 2625 1 prec_cl &lt;- c(prec@data@min-1, 350, 0, 350, prec@data@max+1, 1) prec_cl_mat &lt;- matrix(prec_cl, ncol = 3, byrow = TRUE); prec_cl_mat ## [,1] [,2] [,3] ## [1,] 171 350 0 ## [2,] 350 2626 1 prec_recl &lt;- reclassify(prec, prec_cl_mat) # popl popl ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Population Density.tif ## names : Kenya_Population_Density ## values : 0, 125260 (min, max) popl_cl &lt;- c(popl@data@min-1, 0, 0, 0, popl@data@max+1, 1) popl_cl_mat &lt;- matrix(popl_cl, ncol = 3, byrow = TRUE) popl_cl_mat ## [,1] [,2] [,3] ## [1,] -1 0 0 ## [2,] 0 125261 1 popl_recl &lt;- reclassify(popl , popl_cl_mat) # arid arid ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : Kenya Aridity Index.tif ## names : Kenya_Aridity_Index ## values : 0.0835, 2.7548 (min, max) arid_cl &lt;- c(arid@data@min-1, 0.20, 0, 0.20, arid@data@max+1, 1) arid_cl_mat &lt;- matrix(arid_cl, ncol = 3, byrow = TRUE) arid_cl_mat ## [,1] [,2] [,3] ## [1,] -0.9165 0.2000 0 ## [2,] 0.2000 3.7548 1 arid_recl &lt;- reclassify(arid, arid_cl_mat) 1.3 Finding areas of suitability by multiplying the binary maps We can identify the environmentally suitable areas for occurrence of LF by simply multiplying the binary maps. Therefore, only the cells or areas with the value of 1 will be kept in the output raster layer. You can do this by writing the following formula: Suitable_LF_Binary &lt;- temp_recl*nvdi_recl*prec_recl*elev_recl*popl_recl*arid_recl Visualizing the output: tm_shape(Suitable_LF_Binary) + tm_raster(style = &quot;cat&quot;, title = &quot;&quot;, palette=c(&quot;#f0f0f0&quot;, &quot;red&quot;), labels=c(&quot;Zone: Not Suitable&quot;, &quot;Zone: Highly Suitable&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) IMPORTANT NOTES: Red region corresponds to areas that are highly suitable for LF. South East of Kenya (i.e., Kwale, Kilifi and Lamu) are environmental suitable based on binary classification 1.4 Finding areas of suitability by summing the binary maps to create ratings We can identify the environmentally suitable areas for occurrence of LF by simply summing the binary maps generate ratings for each pixel to show the intensity of suitability for LF. A pixel with a summed value would mean the following: 0 would mean that none of the six factors are present at pixel. 1 would mean at least one out of the six factors are present at that pixel. 2 would mean at least two out of the six risk factors are present at that pixel (and so on…) 6 is the maximum meaning that all factors are present there. Therefore, pixel is rated as the highest levels of suitability for LF. To sum up the raster, we would first need to use the stack() function before using the calc() for the summation. RasterStack &lt;- stack(temp_recl, nvdi_recl, prec_recl, elev_recl, popl_recl, arid_recl) Suitable_LF_Summed &lt;- calc(RasterStack, sum) # check for minimum and maximum Suitable_LF_Summed@data@min ## [1] 2 Suitable_LF_Summed@data@max ## [1] 6 # minimum = 2 # maximum = 6 Visualizing the output: tm_shape(Suitable_LF_Summed) + tm_raster(style = &quot;cat&quot;, title = &quot;Suitability score&quot;, palette=c(&quot;#FDFEFE&quot;, &quot;#FADBD8&quot;, &quot;#F5B7B1&quot;, &quot;#F1948A&quot;, &quot;#E74C3C&quot;), labels=c(&quot;Low (2)&quot;, &quot;Modest (3)&quot;, &quot;Medium (4)&quot;, &quot;High (5)&quot;, &quot;Highest (6)&quot;)) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_text(&quot;NAME_1&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) IMPORTANT NOTES: Using this approach allows the user to see of the fly the intensity for LF suitability. States such as Mandera, Wajir, Isiolo and Turkana have areas that have largely low or modest ratings for LF suitability. Whereas, again the highly suitable areas for LF are the South East states (i.e., Kwale, Kilifi and Lamu). The rating approach is much better because you can see the intensity for LF suitability. Both approaches highly accessible especially for data and public health program managers in sub-Saharan Africa. These kinds of maps are particularly useful to optimize resources (especially within a low-resource setting) for identifying high-risk areas through knowledge-driven approach (and averting to survey potential unsuitable areas to not wasting limited resources). This approach is great for descriptive knowledge-based decision analysis; however, it does not take into account the factor that certain variables have more importance than others. For instance, precipitation is significantly more important than NDVI because elevated levels of rainfall has a positive impact on a mosquitoes’ survival lifespan and it provides an reservoir for female mosquitoes to breed in standing water. Higher rainfall within a densely populated environment provides not only a breeding ground, but a location closer to their food source (i.e., human and animal blood). Thus, population density is important an factor above NDVI but not as important as precipitation (i.e., high importance: precipitation &gt; population density &gt; NDVI :less importance). We can take this into account by applying weights for each variable determined by importance for the MCDA process - this approach is referred to as the Saaty’s Analytical Hierarchy Process (AHP). Let us see how we apply this methodology. 1.5 Standardizing the raster variables to the same scale (from 1 to 10) We are going to use Saaty’s Analytical Hierarchy Process (AHP). We will need to standardize our raster factors in order to make comparisons and combination possible, as all of them contain different measures: temp (degree Celsius), prec (mm), elev (meters), popl (counts/sqkm); while nvdi and arid are derived indices without any units. Before deriving the weights and applying to the equation that is a linear combination of the above variables to estimate the suitability index for LF, we can standardize them using the Jenks Natural Breaks algorithm. IMPORTANT NOTES: Jenks Natural Breaks algorithm is the preferred approach because it calculates the optimum breakpoints while seeking to minimize the variance within categories, and at the same time maximizing the variance between categories. Let’s begin to standardize the first variable temp. There is a bit of data cleaning involved - here is the code for to calculate the breaks using Jenks algorithm to get the raster scales from 1 to 10. # cleaning for temp # Extract values from Raster tempValues &lt;- values(temp) # Change the values from vector object to data.frame object tempDF &lt;- as.data.frame(tempValues) # Remove missing values and reapply column name tempDF &lt;- as.data.frame(tempDF[!is.na(tempDF$tempValues),]) colnames(tempDF) &lt;- &quot;tempValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels #---at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. tempJenks &lt;- getJenksBreaks(tempDF$tempValues, 10, subset = nrow(tempDF)*0.10) # See value in vector tempJenks ## [1] 2.8 12.1 15.6 18.1 20.3 22.4 24.4 26.1 27.5 29.6 # shows first element tempJenks[1] ## [1] 2.8 # shows second element tempJenks[2] ## [1] 12.1 # so on and so further... # Create categorisation by using the Jenks values in the vector temp_jenks_cl &lt;- c(temp@data@min-1, tempJenks[1], 1, tempJenks[1], tempJenks[2], 2, tempJenks[2], tempJenks[3], 3, tempJenks[3], tempJenks[4], 4, tempJenks[4], tempJenks[5], 5, tempJenks[5], tempJenks[6], 6, tempJenks[6], tempJenks[7], 7, tempJenks[7], tempJenks[8], 8, tempJenks[8], tempJenks[9], 9, tempJenks[9], temp@data@max+1, 10) # create matrix temp_jenks_cl_mat &lt;- matrix(temp_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix temp_jenks_cl_mat ## [,1] [,2] [,3] ## [1,] 0.2 2.8 1 ## [2,] 2.8 12.1 2 ## [3,] 12.1 15.6 3 ## [4,] 15.6 18.1 4 ## [5,] 18.1 20.3 5 ## [6,] 20.3 22.4 6 ## [7,] 22.4 24.4 7 ## [8,] 24.4 26.1 8 ## [9,] 26.1 27.5 9 ## [10,] 27.5 30.6 10 # reclassify original raster using the jenks classifications temp_jenks_recl &lt;- reclassify(temp, temp_jenks_cl_mat) Visualize the output with the scale from 1 to 10: tm_shape(temp_jenks_recl) + tm_raster(style = &quot;cont&quot;, title = &quot;Temp (on Jenks scale)&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) We need to repeat this process of reclassification and standardization using natural breaks for the remaining raster grids for prec, arid, popl and ndvi. The solutions are provided in the hidden code chunks below. For elev, we will treat this differently. Click here Solutions # 2 prec # Extract values from Raster precValues &lt;- values(prec) # Change the values from vector object to data.frame object precDF &lt;- as.data.frame(precValues) # Remove missing values and reapply column name precDF &lt;- as.data.frame(precDF[!is.na(precDF$precValues),]) colnames(precDF) &lt;- &quot;precValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. precJenks &lt;- getJenksBreaks(precDF$precValues, 10, subset = nrow(precDF)*0.10) # See value in vector precJenks ## [1] 172 299 416 561 706 870 1080 1326 1618 2505 # shows first element precJenks[1] ## [1] 172 # shows second element precJenks[2] ## [1] 299 # so on and so further... # Create categorisation by using the Jenks values in the vector prec_jenks_cl &lt;- c(prec@data@min-1, precJenks[1], 1, precJenks[1], precJenks[2], 2, precJenks[2], precJenks[3], 3, precJenks[3], precJenks[4], 4, precJenks[4], precJenks[5], 5, precJenks[5], precJenks[6], 6, precJenks[6], precJenks[7], 7, precJenks[7], precJenks[8], 8, precJenks[8], precJenks[9], 9, precJenks[9], prec@data@max+1, 10) # create matrix prec_jenks_cl_mat &lt;- matrix(prec_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix prec_jenks_cl_mat ## [,1] [,2] [,3] ## [1,] 171 172 1 ## [2,] 172 299 2 ## [3,] 299 416 3 ## [4,] 416 561 4 ## [5,] 561 706 5 ## [6,] 706 870 6 ## [7,] 870 1080 7 ## [8,] 1080 1326 8 ## [9,] 1326 1618 9 ## [10,] 1618 2626 10 # reclassify original raster using the jenks classifications prec_jenks_recl &lt;- reclassify(prec, prec_jenks_cl_mat) # 3. popl # Extract values from Raster poplValues &lt;- values(popl) # Change the values from vector object to data.frame object poplDF &lt;- as.data.frame(poplValues) # Remove missing values and reapply column name poplDF &lt;- as.data.frame(poplDF[!is.na(poplDF$poplValues),]) colnames(poplDF) &lt;- &quot;poplValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. poplJenks &lt;- getJenksBreaks(poplDF$poplValues, 10, subset = nrow(poplDF)*0.10) # See value in vector poplJenks ## [1] 0.0000 151.9751 512.5206 1234.2362 3349.1699 7239.2158 13791.5898 23811.1758 33522.6758 56472.0664 # shows first element poplJenks[1] ## [1] 0 # shows second element poplJenks[2] ## [1] 151.9751 # so on and so further... # Create categorisation by using the Jenks values in the vector popl_jenks_cl &lt;- c(popl@data@min-1, poplJenks[1], 1, poplJenks[1], poplJenks[2], 2, poplJenks[2], poplJenks[3], 3, poplJenks[3], poplJenks[4], 4, poplJenks[4], poplJenks[5], 5, poplJenks[5], poplJenks[6], 6, poplJenks[6], poplJenks[7], 7, poplJenks[7], poplJenks[8], 8, poplJenks[8], poplJenks[9], 9, poplJenks[9], popl@data@max+1, 10) # create matrix popl_jenks_cl_mat &lt;- matrix(popl_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix popl_jenks_cl_mat ## [,1] [,2] [,3] ## [1,] -1.0000 0.0000 1 ## [2,] 0.0000 151.9751 2 ## [3,] 151.9751 512.5206 3 ## [4,] 512.5206 1234.2362 4 ## [5,] 1234.2362 3349.1699 5 ## [6,] 3349.1699 7239.2158 6 ## [7,] 7239.2158 13791.5898 7 ## [8,] 13791.5898 23811.1758 8 ## [9,] 23811.1758 33522.6758 9 ## [10,] 33522.6758 125261.0156 10 # reclassify original raster using the jenks classifications popl_jenks_recl &lt;- reclassify(popl, popl_jenks_cl_mat) # 4 nvdi # Extract values from Raster nvdiValues &lt;- values(nvdi) # Change the values from vector object to data.frame object nvdiDF &lt;- as.data.frame(nvdiValues) # Remove missing values and reapply column name nvdiDF &lt;- as.data.frame(nvdiDF[!is.na(nvdiDF$nvdiValues),]) colnames(nvdiDF) &lt;- &quot;nvdiValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. # EXTRA NOTE: The values for nvdi are very close to each other and so the algorithm splits it to just two cateogries nvdiJenks &lt;- getJenksBreaks(nvdiDF$tempValues, 2, subset = nrow(nvdiDF)*0.10) # See value in vector nvdiJenks ## [1] 6.94333e-310 # shows first element nvdiJenks[1] ## [1] 6.94333e-310 # shows second element nvdiJenks[2] ## [1] NA # so on and so further... # Create categorisation by using the Jenks values in the vector nvdi_jenks_cl &lt;- c(nvdi@data@min-1, nvdiJenks[1], 1, nvdiJenks[1], nvdi@data@max+1, 2) # create matrix nvdi_jenks_cl_mat &lt;- matrix(nvdi_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix nvdi_jenks_cl_mat ## [,1] [,2] [,3] ## [1,] -1.116113e+00 6.943330e-310 1 ## [2,] 6.943330e-310 1.874337e+00 2 # reclassify original raster using the jenks classifications nvdi_jenks_recl &lt;- reclassify(nvdi, nvdi_jenks_cl_mat) # 5. arid # Extract values from Raster aridValues &lt;- values(arid) # Change the values from vector object to data.frame object aridDF &lt;- as.data.frame(aridValues) # Remove missing values and reapply column name aridDF &lt;- as.data.frame(aridDF[!is.na(aridDF$aridValues),]) colnames(aridDF) &lt;- &quot;aridValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. # EXTRA NOTE: The values for aridity are very close to each other and so the algorithm splits it to just two cateogries aridJenks &lt;- getJenksBreaks(aridDF$aridValues, 2, subset = nrow(aridDF)*0.10) # See value in vector aridJenks ## [1] 0.08372869 2.58311462 # shows first element aridJenks[1] ## [1] 0.08372869 # shows second element aridJenks[2] ## [1] 2.583115 # so on and so further... # Create categorisation by using the Jenks values in the vector arid_jenks_cl &lt;- c(arid@data@min-1, aridJenks[1], 1, aridJenks[1], arid@data@max+1, 2) # create matrix arid_jenks_cl_mat &lt;- matrix(arid_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix arid_jenks_cl_mat ## [,1] [,2] [,3] ## [1,] -0.91650000 0.08372869 1 ## [2,] 0.08372869 3.75479984 2 # reclassify original raster using the jenks classifications arid_jenks_recl &lt;- reclassify(arid, arid_jenks_cl_mat) For elevation, the risk of LF.decreases with higher values for elevation. Therefore, after applying the Jenks intervals, we need to flip the raster values accordingly. # 6. elev # Extract values from Raster elevValues &lt;- values(elev) # Change the values from vector object to data.frame object elevDF &lt;- as.data.frame(elevValues) # Remove missing values and reapply column name elevDF &lt;- as.data.frame(elevDF[!is.na(elevDF$elevValues),]) colnames(elevDF) &lt;- &quot;elevValues&quot; # Use the getJenksBreaks() function. Sample 0.10 (10%) of the pixels at random and base the categorisation on this. # NOTE: Doing this on the full data will take forever - so use the subset argument. elevJenks &lt;- getJenksBreaks(elevDF$elevValues, 10, subset = nrow(elevDF)*0.10) # See value in vector elevJenks ## [1] -1 254 480 715 1011 1363 1729 2155 2748 4258 # shows first element elevJenks[1] ## [1] -1 # shows second element elevJenks[2] ## [1] 254 # so on and so further... # Create categorisation by using the Jenks values in the vector elev_jenks_cl &lt;- c(elev@data@min-1, elevJenks[1], 1, elevJenks[1], elevJenks[2], 2, elevJenks[2], elevJenks[3], 3, elevJenks[3], elevJenks[4], 4, elevJenks[4], elevJenks[5], 5, elevJenks[5], elevJenks[6], 6, elevJenks[6], elevJenks[7], 7, elevJenks[7], elevJenks[8], 8, elevJenks[8], elevJenks[9], 9, elevJenks[9], elev@data@max+1, 10) # create matrix elev_jenks_cl_mat &lt;- matrix(elev_jenks_cl, ncol = 3, byrow = TRUE) # view categorisation in matrix elev_jenks_cl_mat ## [,1] [,2] [,3] ## [1,] -12 -1 1 ## [2,] -1 254 2 ## [3,] 254 480 3 ## [4,] 480 715 4 ## [5,] 715 1011 5 ## [6,] 1011 1363 6 ## [7,] 1363 1729 7 ## [8,] 1729 2155 8 ## [9,] 2155 2748 9 ## [10,] 2748 4866 10 # reclassify original raster using the jenks classifications elev_jenks_recl &lt;- reclassify(elev, elev_jenks_cl_mat) # Now flip the values using raster.invert() function rev_elev_jenks_recl &lt;- raster.invert(elev_jenks_recl) Visualize the inverted output (NOTE: Blue: High elevation, Red: low elevation): tm_shape(rev_elev_jenks_recl) + tm_raster(style = &quot;cont&quot;, title = &quot;Inverted Elev (on Jenks scale)&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE) 1.6 Determining the weight of each variable using Saaty’s AHP To estimate the weights, a helpful tool has been developed and provided in an Excel Worksheet. It has been created for you so that you can see how the calculations are carryout step-by-step to derive the weights. You can interact with the cells to see the formulas and explanations are provided at each step of the way. Open the spreadsheet Guidance - AHP Calculator Tool.xlsx and read &amp; follow through the steps carefully. These are steps summarized here: Pairwise comparison &amp; Judgement values: Carry out a pairwise comparison of the factors by constructing a matrix as seen in step 1 in the spreadsheet. Use the criteria and make a “Judgement” on the pairwise comparison (it could be based on expert opinion or from literature) asserting a variable’s importance over the other. Matrices: Create a square matrix with the judgement values inserted accordingly (see step 2) Reciprocals: Take the reciprocals in the matrix (see step 3) Take column sums: Sum each of the columns accordingly (see step 4) Normalization of matrix: For each element in a column in that square matrix, divide it by it’s corresponding column sum. Repeat this step for all other elements in that matrix. Once completed, sum-up the elements from the division for each row and finally divide it by the number of variables to obtain the priority vector or weights. You know that the calculations are correct if the sum of the weights are equal to 1 (see step 5). Validation of whether judgement values are reasonable: We need to calculate a Consistency Ratio (CR), which is derived from the Consistency Index (CI) divided by the Random Index (RI). For the CI, we estimate an eigenvalue which is derived from the summed products between the summed column multplied by the weights (see step 6 and click on the cell C75 to view formula). Use the eigenvalue and estimate the CI (see formula in the notes for step 6). Next, use the Random Index table (developed by Saaty, 1980) to determine the RI based on the dimension of the matrix (in this case, it is 6). Finally, calculate the CR by dividing the CI/RI (see step 6 and click on the cell E91 to view the formula). If CI &lt; 1., the judgement values assigned in step 1 are fine. If CR is bigger than 1.0, then judgement values in pairwise comparison in step 1 were unreasonable (and thus you’ll have to repeat the step with different values again!) 1.7 Weighted Linear Combination (WLC) and derivation of Suitability Maps based from AHP analysis Our model uses the Weighted Linear Combination (WLC) approach as the decision rule. The formula to estimate the suitability of LF is as follows: Suitability (LF) = \\(w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5 + w_6x_6\\) The \\(w_i\\) and \\(x_i\\) represent the weights deprived from AHP analysis and raster variables. The weights are multiplied to it corresponding raster to obtain a raster with values weighted but ultimately scaled with an upper limit of 10 (but slightly adjusted for NDVI and Aridity because their scaled to 2). This is the derived formula to use in calculating the suitability regions for LF in RStudio: # use the rescaled columns in the formula (not the originals!) suitablemap_WLC &lt;- 0.3324*prec_jenks_recl + 0.2775*temp_jenks_recl + 0.1571*popl_jenks_recl + 0.0901*nvdi_jenks_recl + 0.0767*arid_jenks_recl + 0.0659*rev_elev_jenks_recl suitablemap_WLC ## class : RasterLayer ## dimensions : 1090, 892, 972280 (nrow, ncol, ncell) ## resolution : 1000, 1000 (x, y) ## extent : 3774798, 4666798, -526069.9, 563930.1 (xmin, xmax, ymin, ymax) ## crs : +proj=merc +a=6378137 +b=6378137 +lat_ts=0 +lon_0=0 +x_0=0 +y_0=0 +k=1 +units=m +nadgrids=@null +wktext +no_defs ## source : memory ## names : layer ## values : 3.2741, 7.4973 (min, max) Finally, visualize the output: tm_shape(suitablemap_WLC) + tm_raster(style = &quot;cont&quot;, title = &quot;LF Suitability (AHP WLC)&quot;, palette= &quot;-Spectral&quot;) + tm_shape(kenya_states) + tm_polygons(alpha = 0, border.col = &quot;black&quot;) + tm_text(&quot;NAME_1&quot;, size = &quot;AREA&quot;) + tm_layout(frame = FALSE, legend.outside = TRUE, legend.title.size = 0.5, legend.text.size = 0.5) + tm_scale_bar(position = c(&quot;left&quot;,&quot;bottom&quot;)) + tm_compass(position = c(&quot;right&quot;, &quot;top&quot;)) IMPORTANT NOTES: The suitability ranges are estimated to be from 3.27 to 7.49 (weighted on a scale with a upper limit of 10). The highest suitability for LF are regions with values closest to 7.49 and vice versa. 1.8 References 1.8.1 Recommended reading Paper: R.W. Saaty (1987). The Analytical Hierarchy Process - What it is and how it is used, Mathl Modelling, Volume 9(3-5), pages 161 to 176 LINK Technical Document: IDRISI Kilimanjaro: Guide to GIS and Image Processing. Chapter 12: Decision Support - Decision Strategy Analysis, Section on ‘A Typology of Decisions’, pages 151 to 154. DOWNLOAD Paper: A.A. Ali, A. Ahmad (2019). Spatial susceptibility analysis of vector-borne diseases in KMC using geospatial technique and MCDM approach, Modeling Earth Systems and Environment, Issue 5, pages 1135–1159 LINK Paper: X. Zhao et al., (2020). Malaria Risk Map Using Spatial Multi-Criteria Decision Analysis along Yunnan Border During the Pre-elimination Period, Am J Trop Med Hyg, Volume 103 (2), pages 793–809 LINK Paper: B. Ahmed (2015). Landslide susceptibility mapping using multi-criteria evaluation techniques in Chittagong Metropolitan Area, Bangladesh. Landslides, Issue 12, pages 1077–1095 LINK 1.8.2 Data Sources The population density 2015 data for Kenya was obtained from Worldpop and resampled at 1km Click Here Raster for annual precipitation was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here Raster for annual temperature was created from bio1_27 and bio1_37, averaged and clipped to Kenya. Obtained for WorldClim Click Here Aridity Index data was obtained for the Global Aridity and PET Database and clipped to Kenya Click Here Normalized Differenced Vegetation Index was obtained from NASA MODIS Click Here (Registration required). Elevation was obtained from the SRTM CSI CGIAR Project, and cropped to Kenya Click Here "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
